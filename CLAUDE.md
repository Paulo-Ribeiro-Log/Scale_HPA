# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**IMPORTANTE**: Responda sempre em portuguÃªs brasileiro (pt-br).
**IMPORTANTE**: Mensagens de commit (git commit) devem ser sempre em portuguÃªs brasileiro.
**IMPORTANTE**: Mantenha o foco na filosofia KISS.
**IMPORTANTE**: Sempre compile o build em ./build/ - usar `./build/k8s-hpa-manager` para executar a aplicaÃ§Ã£o.
**IMPORTANTE**: Interface **totalmente responsiva** - adapta-se a qualquer tamanho de terminal (recomendado: 80x24 ou maior).

---

## ğŸ“‘ Ãndice / Table of Contents

1. [Quick Start](#-quick-start-para-novos-chats)
2. [Development Commands](#-development-commands)
3. [Architecture Overview](#-architecture-overview)
4. [Interface Web](#-interface-web-reacttypescript)
5. [Common Pitfalls](#%EF%B8%8F-common-pitfalls--gotchas)
6. [Testing Strategy](#-testing-strategy)
7. [Troubleshooting](#-troubleshooting)
8. [Continuing Development](#-continuing-development)
9. [HistÃ³rico de CorreÃ§Ãµes](#-histÃ³rico-de-correÃ§Ãµes-principais)

---

## ğŸš€ Quick Start Para Novos Chats

### Project Summary
**Terminal-based Kubernetes HPA and Azure AKS Node Pool management tool** built with Go and Bubble Tea TUI framework. Features async rollout progress tracking with Rich Python-style progress bars, integrated status panel, session management, and unified HPA/node pool operations.

**NOVO (Outubro 2025)**: Interface web completa (React/TypeScript) com compatibilidade 100% TUI para sessÃµes.

### Estado Atual (Novembro 2025)

**VersÃ£o Atual:** v1.3.9 (Release: 03 de novembro de 2025)
**GitHub Release:** https://github.com/Paulo-Ribeiro-Log/Scale_HPA/releases/tag/v1.3.9

**TUI (Terminal Interface):**
- âœ… Interface responsiva (adapta-se ao tamanho real do terminal - mÃ­nimo 80x24)
- âœ… ExecuÃ§Ã£o sequencial de node pools para stress tests (F12)
- âœ… Rollouts detalhados de HPA (Deployment/DaemonSet/StatefulSet)
- âœ… CronJob management (F9) e Prometheus Stack (F8)
- âœ… Status container compacto (80x10) com progress bars Rich Python
- âœ… Auto-descoberta de clusters via `k8s-hpa-manager autodiscover`
- âœ… ValidaÃ§Ã£o VPN on-demand (verifica conectividade K8s antes de operaÃ§Ãµes crÃ­ticas)
- âœ… Modais de confirmaÃ§Ã£o (Ctrl+D/Ctrl+U exigem confirmaÃ§Ã£o)
- âœ… Log detalhado de alteraÃ§Ãµes (antes â†’ depois) no StatusContainer
- âœ… Sistema de Logs completo (F3) - visualizador com scroll, copiar, limpar
- âœ… Race condition corrigida (Mutex RWLock para testes paralelos de cluster)
- âœ… **Sistema de updates automÃ¡tico** - DetecÃ§Ã£o 1x por dia com notificaÃ§Ã£o

**Web Interface:**
- âœ… Interface web completa (99% funcional)
- âœ… HPAs, Node Pools, CronJobs e Prometheus Stack implementados
- âœ… Dashboard com **gauge de dois anÃ©is** mostrando Capacity vs Allocatable - v1.3.3
- âœ… **MÃ©tricas precisas** idÃªnticas ao K9s (uso de Allocatable ao invÃ©s de Capacity) - v1.3.3
- âœ… Sistema de sessÃµes completo (save/load/rename/delete/edit)
- âœ… Staging area com preview de alteraÃ§Ãµes
- âœ… Snapshot de cluster para rollback
- âœ… Sistema de heartbeat e auto-shutdown (20min inatividade)
- âœ… ApplyAllModal com progress tracking e rollout simulation
- âœ… **Rollout individual para Prometheus Stack** (Deployment/StatefulSet/DaemonSet) - Outubro 2025
- âœ… **Aplicar Agora para Node Pools** - AplicaÃ§Ã£o individual sem staging - Outubro 2025
- âœ… **Campo de busca inteligente** - HPAs (nome/namespace) e Node Pools (nome/cluster) - v1.2.1
- âœ… **Modal de ediÃ§Ã£o inline** - EdiÃ§Ã£o completa de HPAs no ApplyAllModal - v1.2.1
- âœ… **Sistema de eventos** - Refetch sem reload para estabilidade - v1.2.1
- âœ… **Sistema de Log Viewer** - Modal com captura em tempo real, auto-refresh, exportar CSV - v1.2.1
- âœ… **Toggle de Namespaces de Sistema** - Exibe/oculta namespaces de sistema (kube-system, monitoring, etc.) - Outubro 2025
- âœ… **Combobox de Cluster no Header** - Busca integrada com filtro em tempo real, keyboard navigation - v1.3.2
- âœ… **Redesign CronJobs e Prometheus Pages** - SplitView layout, auto-refresh, controles compactos - v1.3.4
- âœ… **Redesign Staging Page** - SplitView layout (2/5 + 3/5), busca integrada, editor inline - v1.3.7
- âœ… **Load Session Modal Simplificado** - Removido "Apply Directly", scroll independente por painel - v1.3.8
- âœ… **EdiÃ§Ã£o Inline de Node Pools no ApplyAllModal** - Menu â‹® com opÃ§Ãµes "Editar ConteÃºdo" e "Remover da Lista" - v1.3.9
- âœ… **Editor nÃ£o fecha apÃ³s salvar** - CorreÃ§Ã£o em StagingPanel para HPAs e Node Pools - v1.3.9
- âœ… **PÃ¡gina de Monitoring HPA-Watchdog** - Sidebar retrÃ¡til, integraÃ§Ã£o com engine de monitoramento, mÃ©tricas em tempo real - Novembro 2025
- âœ… **RefatoraÃ§Ã£o RotatingCollector** - Sistema de monitoramento simplificado, reduÃ§Ã£o de 850 â†’ 450 linhas, baseline automÃ¡tico de 3 dias - 07 nov 2025
- âœ… **Aba ConfigMaps (Monaco Editor)** - Listagem completa com filtro por namespace, ediÃ§Ã£o YAML com monaco-yaml, diff, dry-run e apply direto via backend Go; cards de estatÃ­sticas sÃ£o ocultados apenas nesta aba para maximizar o espaÃ§o Ãºtil - Nov 2025
- âœ… **Diff visual com Diff2HTML** - Modal dedicado (side-by-side) usando tema VS Code dark, nomes reais de arquivos e mesma paleta do Monaco; backend gera unified diff via `difflib` - Nov 2025
- âœ… **Melhorias de UX na aba ConfigMaps** - Toggle de Labels, botÃ£o para recolher o painel de ConfigMaps e botÃµes â€œXâ€ de limpeza em todos os campos de busca (HPAs, Node Pools, etc.) para liberar espaÃ§o no editor - Nov 2025

### Tech Stack
- **Language**: Go 1.23+ (toolchain 1.24.7)
- **TUI Framework**: Bubble Tea v0.24.2 + Lipgloss v1.1.0
- **K8s Client**: client-go v0.31.4 (official)
- **Azure SDK**: azcore v1.19.1, azidentity v1.12.0
- **Web Frontend**: React 18.3 + TypeScript 5.8 + Vite 5.4
- **Web UI**: shadcn/ui (Radix UI) + Tailwind CSS 3.4
- **Architecture**: MVC pattern com state-driven UI

---

## ğŸ”§ Development Commands

### Terminal Requirements (TUI)

**âœ… Interface Totalmente Responsiva**

A aplicaÃ§Ã£o usa **EXATAMENTE o tamanho do seu terminal** - sem forÃ§ar dimensÃµes artificiais:

- **Adapta-se ao terminal**: Usa suas dimensÃµes reais (ex: 80x24, 120x30, etc)
- **Texto legÃ­vel**: NÃ£o precisa zoom out - mantenha Ctrl+0 (tamanho normal)
- **Otimizada para produÃ§Ã£o**: Layout compacto, operaÃ§Ã£o segura sem erros visuais
- **Sem limites artificiais**: Removido forÃ§amento de 188x45 que causava texto minÃºsculo

**Como funciona:**
1. AplicaÃ§Ã£o detecta tamanho real do terminal
2. Ajusta painÃ©is automaticamente (60x12 base)
3. Status panel compacto (80x10)
4. Context box inline (cluster | sessÃ£o)
5. Scroll quando necessÃ¡rio

**ValidaÃ§Ã£o VPN e Azure:**
- **VPN Check**: Usa `kubectl cluster-info` para validar conectividade K8s real
- **ValidaÃ§Ã£o on-demand**: Testa VPN em inÃ­cio, namespaces, HPAs e timeouts
- **Azure timeout**: 5 segundos para evitar travamentos DNS
- **Mensagens claras**: Exibidas no StatusContainer com soluÃ§Ãµes (F5 para retry)

### Installation and Updates

```bash
# InstalaÃ§Ã£o completa em 1 comando (clone + build + install)
curl -fsSL https://raw.githubusercontent.com/Paulo-Ribeiro-Log/Scale_HPA/main/install-from-github.sh | bash

# O que faz:
# - Clona repositÃ³rio
# - Compila com injeÃ§Ã£o de versÃ£o
# - Instala em /usr/local/bin/
# - Copia scripts utilitÃ¡rios para ~/.k8s-hpa-manager/scripts/
# - Cria atalho k8s-hpa-web

# Sistema de updates automÃ¡tico
k8s-hpa-manager version       # Verificar versÃ£o e updates disponÃ­veis
~/.k8s-hpa-manager/scripts/auto-update.sh             # Auto-update interativo
~/.k8s-hpa-manager/scripts/auto-update.sh --yes       # Auto-update sem confirmaÃ§Ã£o
~/.k8s-hpa-manager/scripts/auto-update.sh --check     # Apenas verificar
~/.k8s-hpa-manager/scripts/auto-update.sh --dry-run   # Simular

# Scripts utilitÃ¡rios instalados
k8s-hpa-web start/stop/status/logs/restart            # Gerenciar servidor web
~/.k8s-hpa-manager/scripts/uninstall.sh              # Desinstalar
~/.k8s-hpa-manager/scripts/backup.sh                 # Backup (dev)
~/.k8s-hpa-manager/scripts/restore.sh                # Restore (dev)
```

ğŸ“š **DocumentaÃ§Ã£o:**
- `INSTALL_GUIDE.md` - Guia completo de instalaÃ§Ã£o
- `UPDATE_BEHAVIOR.md` - Como funciona o sistema de updates
- `AUTO_UPDATE_EXAMPLES.md` - Exemplos de uso do auto-update

### Building and Running (TUI)

```bash
# Build TUI
make build                    # Build to ./build/k8s-hpa-manager (version auto-detected)
make build-all                # Build for multiple platforms (Linux, macOS, Windows)
make run                      # Build and run
make run-dev                  # Run with debug logging (go run . --debug)
make version                  # Show detected version from git tags
make release                  # Build for all platforms (Linux, macOS amd64/arm64, Windows)
```

### Building and Running (Web Interface)

```bash
# Frontend development
make web-install              # Install frontend dependencies (npm install)
make web-dev                  # Start Vite dev server (port 5173)
                              # Backend: ./build/k8s-hpa-manager web --port 8080 (terminal 2)

# Production build
make web-build                # Build frontend â†’ internal/web/static/
make build-web                # Build completo (frontend + Go binary com embed)

# Run web server
./build/k8s-hpa-manager web              # Background mode (default)
./build/k8s-hpa-manager web -f           # Foreground mode
./build/k8s-hpa-manager web --port 8080  # Custom port

# IMPORTANTE: Rebuild obrigatÃ³rio
./rebuild-web.sh -b           # Script recomendado (evita cache issues)
```

### Testing

```bash
make test                     # Run all tests with verbose output
make test-coverage            # Run tests with coverage (generates coverage.html)
```

### Safe Deploy (Deploy Seguro)

**Script automatizado para deploy seguro de dev2 â†’ main com validaÃ§Ãµes completas:**

```bash
./safe-deploy.sh              # Deploy completo (interativo com confirmaÃ§Ãµes)
./safe-deploy.sh --dry-run    # Simular deploy sem executar (teste)
./safe-deploy.sh --yes        # Deploy automÃ¡tico sem confirmaÃ§Ãµes
./safe-deploy.sh --skip-tests # Pular execuÃ§Ã£o de testes (nÃ£o recomendado)
./safe-deploy.sh --skip-build # Pular build (nÃ£o recomendado)
./safe-deploy.sh --help       # Ver todas as opÃ§Ãµes
```

**O que o script faz:**
1. âœ… **ValidaÃ§Ãµes iniciais**: Working tree limpo, branches existem
2. âœ… **Testes**: Executa `make test` (pode pular com --skip-tests)
3. âœ… **Build**: Compila TUI e Web (pode pular com --skip-build)
4. âœ… **Backup**: Cria branch de backup automÃ¡tico (backup-TIMESTAMP-pre-deploy)
5. âœ… **Merge**: dev2 â†’ main com detecÃ§Ã£o de conflitos
6. âœ… **Sync**: Rebase com origin/main
7. âœ… **Tags**: OpÃ§Ã£o de atualizar tags (ex: v1.2.0)
8. âœ… **Push**: Branch main e tags para GitHub
9. âœ… **Sync dev2**: OpÃ§Ã£o de sincronizar dev2 com main apÃ³s deploy

**Workflow recomendado:**
```bash
# 1. Testar primeiro (dry-run)
./safe-deploy.sh --dry-run

# 2. Deploy real apÃ³s validar
./safe-deploy.sh

# 3. Ou deploy automÃ¡tico (CI/CD)
./safe-deploy.sh --yes
```

**Vantagens:**
- ğŸ›¡ï¸ Previne quebra da branch main
- ğŸ”„ Backup automÃ¡tico antes de qualquer alteraÃ§Ã£o
- âœ… ValidaÃ§Ãµes completas (testes, build, working tree)
- ğŸ“Š Resumo claro do que serÃ¡ feito
- ğŸ¯ Modo dry-run para testes seguros

**Nota:** O script `safe-deploy.sh` estÃ¡ no `.gitignore` e nÃ£o Ã© versionado (uso local apenas).

### Installation

```bash
./install.sh                  # Automated installer â†’ /usr/local/bin/
./uninstall.sh                # Uninstaller (optionally removes session data)

# After installation
k8s-hpa-manager               # Run TUI from anywhere
k8s-hpa-manager web           # Run web interface
k8s-hpa-manager --debug       # Debug mode
k8s-hpa-manager --help        # Show help
```

### Cluster Auto-Discovery

```bash
k8s-hpa-manager autodiscover  # Auto-descobre clusters do kubeconfig
```
- Extrai resource groups do campo `user` (formato: `clusterAdmin_{RG}_{CLUSTER}`)
- Descobre subscriptions via Azure CLI
- Gera/atualiza `~/.k8s-hpa-manager/clusters-config.json`
- EscalÃ¡vel para 26, 70+ clusters

**Workflow:**
1. `az aks get-credentials --name CLUSTER --resource-group RG`
2. `k8s-hpa-manager autodiscover`
3. Node Pools prontos para uso (TUI e Web)

### Backup and Restore

```bash
./backup.sh "descriÃ§Ã£o"       # Criar backup antes de modificaÃ§Ãµes
./restore.sh                  # Listar backups disponÃ­veis
./restore.sh backup_name      # Restaurar backup especÃ­fico
```
- MantÃ©m os 10 backups mais recentes automaticamente
- Metadados inclusos (git commit, data, usuÃ¡rio)

---

## ğŸ—ï¸ Architecture Overview

### Estrutura de DiretÃ³rios

```
k8s-hpa-manager/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ root.go                    # CLI entry point & commands (Cobra)
â”‚   â”œâ”€â”€ web.go                     # Web server command
â”‚   â”œâ”€â”€ version.go                 # Version command
â”‚   â”œâ”€â”€ autodiscover.go            # Cluster auto-discovery
â”‚   â””â”€â”€ k8s-teste/                 # Layout test command
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ tui/                       # Terminal UI (Bubble Tea)
â”‚   â”‚   â”œâ”€â”€ app.go                 # Main orchestrator + centralized text methods
â”‚   â”‚   â”œâ”€â”€ handlers.go            # Event handlers
â”‚   â”‚   â”œâ”€â”€ views.go               # UI rendering & layout
â”‚   â”‚   â”œâ”€â”€ message.go             # Bubble Tea messages
â”‚   â”‚   â”œâ”€â”€ text_input.go          # Centralized text input with intelligent cursor
â”‚   â”‚   â”œâ”€â”€ resource_*.go          # HPA/Node Pool resource management
â”‚   â”‚   â”œâ”€â”€ cronjob_*.go           # CronJob management (F9)
â”‚   â”‚   â”œâ”€â”€ components/            # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ status_container.go
â”‚   â”‚   â”‚   â””â”€â”€ unified_container.go
â”‚   â”‚   â””â”€â”€ layout/                # Layout managers
â”‚   â”‚       â”œâ”€â”€ manager.go
â”‚   â”‚       â”œâ”€â”€ screen.go
â”‚   â”‚       â”œâ”€â”€ panels.go
â”‚   â”‚       â””â”€â”€ constants.go
â”‚   â”œâ”€â”€ web/                       # Web Interface (React/TypeScript)
â”‚   â”‚   â”œâ”€â”€ frontend/              # React SPA
â”‚   â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ components/    # UI components (shadcn/ui)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ contexts/      # StagingContext, TabContext
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hooks/         # useHeartbeat, custom hooks
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lib/           # API client, utilities
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ pages/         # Index, CronJobs, Prometheus
â”‚   â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â”‚   â””â”€â”€ vite.config.ts
â”‚   â”‚   â”œâ”€â”€ handlers/              # Go REST API handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ hpas.go           # HPA CRUD
â”‚   â”‚   â”‚   â”œâ”€â”€ nodepools.go      # Node Pool management
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.go       # Session save/load/rename/delete/edit
â”‚   â”‚   â”‚   â”œâ”€â”€ cronjobs.go       # CronJob management
â”‚   â”‚   â”‚   â””â”€â”€ prometheus.go     # Prometheus Stack
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â””â”€â”€ auth.go           # Bearer token auth
â”‚   â”‚   â”œâ”€â”€ static/               # Build output (embedado no Go binary)
â”‚   â”‚   â””â”€â”€ server.go             # Gin HTTP server com heartbeat/auto-shutdown
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ types.go               # All data structures & app state
â”‚   â”œâ”€â”€ session/
â”‚   â”‚   â””â”€â”€ manager.go             # Session persistence (template naming)
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â””â”€â”€ client.go              # K8s API wrapper (client-go)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ kubeconfig.go          # Cluster discovery
â”‚   â”œâ”€â”€ azure/
â”‚   â”‚   â””â”€â”€ auth.go                # Azure SDK authentication
â”‚   â”œâ”€â”€ updater/                   # Versioning system
â”‚   â”‚   â”œâ”€â”€ version.go
â”‚   â”‚   â”œâ”€â”€ github.go
â”‚   â”‚   â””â”€â”€ checker.go
â”‚   â””â”€â”€ ui/                        # UI utilities
â”‚       â”œâ”€â”€ progress.go
â”‚       â”œâ”€â”€ logs.go
â”‚       â””â”€â”€ status_panel.go
â”œâ”€â”€ build/                         # Build artifacts
â”œâ”€â”€ backups/                       # Code backups (via backup.sh)
â”œâ”€â”€ Docs/                          # Documentation (web POC, plans, fixes)
â”œâ”€â”€ go.mod & go.sum
â”œâ”€â”€ makefile
â”œâ”€â”€ rebuild-web.sh                 # Web rebuild script (recomendado)
â””â”€â”€ *.sh scripts                   # Install, uninstall, backup, restore
```

### Core Components

**TUI Layer** (`internal/tui/`):
- `app.go` - Main Bubble Tea app with centralized text editing
- `handlers.go` - User input and event handling
- `views.go` - UI rendering with intelligent cursor display
- `text_input.go` - Centralized text input module with cursor overlay
- `resource_*.go` - HPA and node pool resource management
- `cronjob_*.go` - CronJob management (F9)
- `components/` - Reusable UI components (status, containers)
- `layout/` - Layout management system

**Web Layer** (`internal/web/`):
- `server.go` - Gin HTTP server com heartbeat e auto-shutdown
- `handlers/` - REST API endpoints (HPAs, Node Pools, Sessions, CronJobs, Prometheus)
- `middleware/auth.go` - Bearer token authentication
- `frontend/` - React/TypeScript SPA com shadcn/ui

**Business Logic** (`internal/`):
- `kubernetes/client.go` - K8s client wrapper with per-cluster management
- `config/kubeconfig.go` - Kubeconfig discovery (akspriv-* pattern) **+ Mutex RWLock**
- `session/manager.go` - Session persistence with template naming (compatÃ­vel TUI â†” Web)
- `models/types.go` - Complete domain model and app state (AppModel)
- `azure/auth.go` - Azure SDK auth with browser/device code fallback

**Entry Points**:
- `main.go` - Application bootstrap
- `cmd/root.go` - Cobra CLI commands and flags (TUI)
- `cmd/web.go` - Web server command (background/foreground modes)

### Data Flow

1. **State-Driven Architecture**: `AppModel` in `models/types.go` maintains complete app state
2. **State Transitions**: `AppState` enum manages flow:
   - Cluster Selection â†’ Session Selection â†’ Namespace Selection â†’ HPA/Node Pool Management â†’ Editing â†’ Help
3. **Multi-Selection Flow**: One Cluster â†’ Multiple Namespaces â†’ Multiple HPAs/Node Pools â†’ Individual Editing
4. **Bubble Tea Messages**: Coordinate between UI interactions and business logic (TUI)
5. **React Query + Context**: State management na web interface
6. **Client Management**: Per-cluster Kubernetes client instances (thread-safe via RWLock)
7. **Session System**: Preserves state for review/editing before application (TUI e Web compartilham formato)

### Dependencies

**Core Framework**:
- Bubble Tea v0.24.2 - TUI framework
- Lipgloss v1.1.0 - Styling and layout
- Cobra v1.10.1 - CLI commands
- Gin v1.11.0 - HTTP server (web)

**Kubernetes**:
- client-go v0.31.4 - Official K8s Go client

**Azure**:
- azcore v1.19.1 - Core SDK
- azidentity v1.12.0 - Authentication
- Azure CLI - External dependency for node pool operations

**Web Frontend**:
- React 18.3 + TypeScript 5.8
- Vite 5.4 - Build tool com HMR
- shadcn/ui - UI components (Radix UI primitives)
- Tailwind CSS 3.4 - Styling
- React Query (TanStack) - Server state management
- React Router DOM - Client-side routing

---

## ğŸŒ Interface Web (React/TypeScript)

### Quick Start Web

```bash
# Development (2 terminais)
make web-install                              # Terminal 1: Install dependencies
make web-dev                                  # Terminal 1: Vite dev server (port 5173)
./build/k8s-hpa-manager web --port 8080       # Terminal 2: Backend API

# Production Build
make build-web                                # Build frontend + Go binary (embeds static/)
./build/k8s-hpa-manager web                   # Run integrated server (background mode)

# Background vs Foreground
./build/k8s-hpa-manager web                   # Background (default) - daemon mode
./build/k8s-hpa-manager web -f                # Foreground - logs no terminal
# Auto-shutdown: 20 min apÃ³s Ãºltima pÃ¡gina fechar (sistema de heartbeat)
```

### Tech Stack Frontend

| Tecnologia | VersÃ£o | Uso |
|------------|--------|-----|
| **React** | 18.3 | UI framework |
| **TypeScript** | 5.8 | Type safety |
| **Vite** | 5.4 | Build tool (HMR rÃ¡pido) |
| **shadcn/ui** | Latest | UI components (Radix UI) |
| **Tailwind CSS** | 3.4 | Styling |
| **React Query** | TanStack | Server state management |
| **React Router** | DOM | Client-side routing |
| **Lucide React** | Latest | Icons |
| **Recharts** | Latest | Charts (Dashboard) |

### Sistema de Heartbeat e Auto-Shutdown

**Problema resolvido:** Servidor web rodando em background consome recursos indefinidamente mesmo sem uso.

**âš ï¸ CORREÃ‡ÃƒO CRÃTICA (Outubro 2025):** Dois bugs crÃ­ticos foram corrigidos no sistema de heartbeat que causavam shutdown prematuro. Ver detalhes completos na seÃ§Ã£o [HistÃ³rico de CorreÃ§Ãµes](#correÃ§Ã£o-crÃ­tica-sistema-de-heartbeatauto-shutdown-outubro-2025-).

**SoluÃ§Ã£o:**
- **Frontend**: Hook `useHeartbeat` envia POST `/heartbeat` a cada 5 minutos
- **Backend**: Reseta timer de 20 minutos (ou 30min inicial) ao receber heartbeat
- **Auto-shutdown**: Servidor desliga automaticamente se nenhuma pÃ¡gina conectada por 20min
- **Thread-safe**: `sync.RWMutex` protege timestamp de heartbeat + `sync.Mutex` protege timer (corrigido em Oct/2025)

**ImplementaÃ§Ã£o:**

```typescript
// Frontend: hooks/useHeartbeat.ts
useEffect(() => {
  const sendHeartbeat = async () => {
    await fetch('/heartbeat', { method: 'POST' });
  };

  sendHeartbeat(); // Imediato ao montar
  const interval = setInterval(sendHeartbeat, 5 * 60 * 1000); // 5 min

  return () => clearInterval(interval);
}, []);
```

```go
// Backend: internal/web/server.go
func (s *Server) startInactivityMonitor() {
    s.shutdownTimer = time.AfterFunc(20*time.Minute, s.autoShutdown)
}

func (s *Server) handleHeartbeat(c *gin.Context) {
    s.heartbeatMutex.Lock()
    s.lastHeartbeat = time.Now()
    s.heartbeatMutex.Unlock()

    if s.shutdownTimer != nil {
        s.shutdownTimer.Stop()
    }
    s.shutdownTimer = time.AfterFunc(20*time.Minute, s.autoShutdown)
}
```

### Features Implementadas Web

| Feature | Status | DescriÃ§Ã£o |
|---------|--------|-----------|
| **HPAs** | âœ… 100% | CRUD completo com ediÃ§Ã£o de recursos (CPU/Memory Request/Limit) + Aplicar Agora |
| **Node Pools** | âœ… 100% | Editor funcional (autoscaling, node count, min/max) + **BotÃ£o "Aplicar Agora"** |
| **CronJobs** | âœ… 100% | Suspend/Resume |
| **Prometheus Stack** | âœ… 100% | Resource management + **Rollout individual (Deployment/StatefulSet/DaemonSet)** |
| **Sessions** | âœ… 100% | Save/Load/Rename/Delete/Edit (compatÃ­vel TUI) |
| **Staging Area** | âœ… 100% | Preview de alteraÃ§Ãµes antes de aplicar |
| **ApplyAllModal** | âœ… 100% | Progress tracking com rollout simulation |
| **Dashboard** | âœ… 100% | Grid 2x2 com mÃ©tricas reais (CPU/Memory allocation) |
| **Snapshot Cluster** | âœ… 100% | Captura estado atual para rollback |
| **Heartbeat System** | âœ… 100% | Auto-shutdown em 20min inatividade |
| **Log Viewer** | âœ… 100% | Modal com logs em tempo real (app + servidor), auto-refresh, copiar, exportar CSV, limpar |
| **System Namespaces Toggle** | âœ… 100% | Filtro de namespaces de sistema (kube-*, monitoring, etc.) com botÃ£o toggle |

### Workflow Session Management (Web)

```
1. Editar HPAs/Node Pools â†’ Staging Area (mudanÃ§as pendentes em memÃ³ria)
2. "Save Session" â†’ Modal com folders (HPA-Upscale/Downscale/Node-Upscale/Downscale)
3. Templates de nomenclatura: {action}_{cluster}_{timestamp}_{env}
4. "Load Session" â†’ Grid de sessÃµes com dropdown menu (â‹®)
5. Dropdown actions:
   - Load: Carrega para Staging Area
   - Rename: Altera nome da sessÃ£o
   - Edit Content: EditSessionModal (edita HPAs/Node Pools salvos)
   - Delete: Remove sessÃ£o (com confirmaÃ§Ã£o)
6. "Apply Changes" â†’ ApplyAllModal com preview before/after
7. Progress tracking: Rollout simulation com progress bars
```

### Snapshot de Cluster para Rollback

**Feature NOVA (Outubro 2025):**
- Captura estado atual do cluster (TODOS os HPAs + Node Pools)
- Salva como sessÃ£o sem modificaÃ§Ãµes (original_values = new_values)
- Permite rollback completo em caso de incident

**Workflow:**
```
1. Selecionar cluster
2. "Save Session" â†’ Detecta staging vazio
3. Modal oferece "Capturar Snapshot do Cluster"
4. Backend busca dados FRESCOS via API K8s/Azure (nÃ£o usa cache)
5. Salva em folder "Rollback" ou custom
6. Para restaurar: Load session â†’ Apply
```

### Toggle de Namespaces de Sistema

**Feature NOVA (Outubro 2025):**
- Filtro inteligente de namespaces de sistema (kube-system, kube-public, monitoring, etc.)
- BotÃ£o toggle na mesma linha do tÃ­tulo "Available HPAs"
- Estados visuais distintos: ON (azul/primary) e OFF (cinza/muted)
- Default: desabilitado (namespaces de sistema ocultos)

**ImplementaÃ§Ã£o:**
- **Backend**: Query parameter `showSystem=true` em `/api/v1/hpas`
- **Frontend**: Estado React com Ã­cones Eye/EyeOff
- **Filtro**: Lista de 53+ namespaces de sistema em `internal/kubernetes/client.go`
- **Posicionamento**: Propriedade `titleAction` no componente `SplitView`

**Workflow:**
```
1. UsuÃ¡rio acessa pÃ¡gina de HPAs
2. Por padrÃ£o, namespaces de sistema estÃ£o ocultos (botÃ£o OFF - cinza)
3. Clicar no botÃ£o toggle:
   - ON (Eye + azul): Mostra namespaces de sistema
   - OFF (EyeOff + cinza): Oculta namespaces de sistema
4. Backend filtra usando isSystemNamespace()
5. Lista de HPAs atualizada automaticamente via useEffect
```

**Namespaces de sistema filtrados:**
- Kubernetes core: `kube-system`, `kube-public`, `kube-node-lease`, `default`
- Monitoring: `monitoring`, `prometheus`, `grafana`, `kube-prometheus-stack`
- Networking: `calico-system`, `tigera-operator`, `istio-system`, `linkerd`
- Storage: `rook-ceph`, `longhorn-system`, `openebs`
- CI/CD: `argocd`, `flux-system`, `tekton-pipelines`
- Logging: `logging`, `elastic-system`, `loki`
- Security: `cert-manager`, `vault`, `gatekeeper-system`
- E mais 30+ namespaces...

**Arquivos modificados:**
- `internal/web/handlers/hpas.go` - Parse query parameter `showSystem`
- `internal/web/frontend/src/lib/api/client.ts` - ParÃ¢metro `showSystem` em `getHPAs()`
- `internal/web/frontend/src/hooks/useAPI.ts` - Hook `useHPAs` com `showSystem`
- `internal/web/frontend/src/components/SplitView.tsx` - Suporte a `titleAction`
- `internal/web/frontend/src/pages/Index.tsx` - Estado e botÃ£o toggle

### Rebuild Web ObrigatÃ³rio

**IMPORTANTE**: Sempre use o script recomendado para rebuilds web:

```bash
./rebuild-web.sh -b           # Build completo (frontend + backend)
```

**Por que nÃ£o usar `make build` direto:**
- Cache do Vite pode causar stale files
- Static files podem nÃ£o embedar corretamente
- Frontend e backend precisam sincronizar versÃµes

**ApÃ³s rebuild:**
1. Hard refresh no browser: `Ctrl+Shift+R`
2. Verificar logs: `/tmp/k8s-hpa-manager-web-*.log` (modo background)

### API Endpoints

**Base URL**: `http://localhost:8080/api/v1`

**AutenticaÃ§Ã£o**: Bearer token no header `Authorization: Bearer poc-token-123`

| Endpoint | Method | DescriÃ§Ã£o |
|----------|--------|-----------|
| `/clusters` | GET | Lista clusters disponÃ­veis |
| `/namespaces?cluster=X` | GET | Lista namespaces do cluster |
| `/hpas?cluster=X&namespace=Y` | GET | Lista HPAs |
| `/hpas/:cluster/:namespace/:name` | PUT | Atualiza HPA |
| `/nodepools?cluster=X` | GET | Lista node pools |
| `/nodepools/:cluster/:rg/:name` | PUT | Atualiza node pool |
| `/sessions` | GET | Lista sessÃµes salvas |
| `/sessions` | POST | Salva nova sessÃ£o |
| `/sessions/:name` | DELETE | Remove sessÃ£o |
| `/sessions/:name/rename` | PUT | Renomeia sessÃ£o |
| `/sessions/:name` | PUT | Atualiza conteÃºdo da sessÃ£o |
| `/cronjobs?cluster=X&namespace=Y` | GET | Lista CronJobs |
| `/prometheus?cluster=X` | GET | Lista recursos Prometheus |
| `/prometheus/:cluster/:namespace/:type/:name/rollout` | POST | **Rollout de recurso Prometheus (deployment/statefulset/daemonset)** |
| `/logs` | GET | Retorna logs da aplicaÃ§Ã£o e servidor (buffer + arquivos) |
| `/logs` | DELETE | Limpa buffer de logs da aplicaÃ§Ã£o |
| `/heartbeat` | POST | Heartbeat (mantÃ©m servidor vivo) |

---

## âš ï¸ Common Pitfalls / Gotchas

### Web Development

1. **SEMPRE usar `./rebuild-web.sh -b`** para builds web
   - âŒ NÃƒO: `npm run build && make build` (pode causar cache issues)
   - âœ… SIM: `./rebuild-web.sh -b`

2. **Hard refresh obrigatÃ³rio** apÃ³s rebuild
   - `Ctrl+Shift+R` no browser para limpar cache JavaScript

3. **TabProvider obrigatÃ³rio** no App.tsx
   - Deve envolver `StagingProvider` e outros contexts
   - Erro sem TabProvider: "useTabManager must be used within a TabProvider"

4. **Cluster name suffix mismatch**
   - Sessions salvam sem `-admin` (ex: `akspriv-prod`)
   - Kubeconfig contexts tÃªm `-admin` (ex: `akspriv-prod-admin`)
   - **Fix**: `StagingContext.loadFromSession()` adiciona `-admin` automaticamente
   - **Fix**: `findClusterInConfig()` remove `-admin` para matching

5. **Staging context patterns**
   - âŒ NÃƒO existe: `staging.add()`, `staging.getNodePool()`
   - âœ… Usar: `staging.addHPAToStaging()`, `staging.stagedNodePools.find()`

6. **Background mode logs**
   - Logs salvos em `/tmp/k8s-hpa-manager-web-*.log`
   - Use `tail -f /tmp/k8s-hpa-manager-web-*.log` para debug

### TUI Development

1. **Sempre usar `[]rune` para texto** (Unicode-safe)
   ```go
   // âŒ ERRADO
   text := "Hello"
   text[0] = 'h' // NÃ£o funciona com emojis

   // âœ… CORRETO
   runes := []rune("Hello ğŸ‘‹")
   runes[0] = 'h'
   text = string(runes)
   ```

2. **ESC deve preservar contexto**
   - Usar `handleEscape()` centralizado em `handlers.go`
   - NUNCA fazer `return tea.Quit` direto no ESC
   - Exemplo: F9 (CronJobs) â†’ ESC â†’ volta para Namespaces (preserva seleÃ§Ãµes)

3. **Estado sempre em AppModel**
   - `internal/models/types.go` Ã© a ÃšNICA fonte de verdade
   - NUNCA criar estado local em handlers ou views
   - Bubble Tea messages para comunicaÃ§Ã£o assÃ­ncrona

4. **Bubble Tea messages para async**
   - NUNCA usar goroutines diretas para operaÃ§Ãµes K8s/Azure
   - Sempre retornar `tea.Cmd` que envia mensagem quando completo
   ```go
   // âŒ ERRADO
   go func() {
       applyHPA() // Race condition!
   }()

   // âœ… CORRETO
   return func() tea.Msg {
       err := applyHPA()
       return HPAAppliedMsg{err: err}
   }
   ```

5. **Mutex para concorrÃªncia**
   - `clientMutex` em `getClient()` - protege criaÃ§Ã£o de K8s clients
   - `heartbeatMutex` em web server - protege timestamp
   - Double-check locking pattern para performance

### Azure CLI

1. **Warnings nÃ£o sÃ£o erros**
   - `pkg_resources deprecated` â†’ ignorar
   - `isOnlyWarnings()` em `executeAzureCommand()` separa stderr real de warnings

2. **Scale com autoscaling habilitado**
   - Azure CLI rejeita `scale` se autoscaling enabled
   - **Ordem correta**: Disable autoscaling â†’ Scale â†’ Enable autoscaling
   - Ver `buildNodePoolCommands()` em `app.go` para lÃ³gica de 4 cenÃ¡rios

3. **Timeout de 5 segundos**
   - ValidaÃ§Ã£o Azure com timeout evita travamento em problemas de rede/DNS
   - Ver `configurateSubscription()` em `cmd/root.go`

### Session System

1. **Folders obrigatÃ³rios**
   - Save/Load/Delete/Rename requerem `folder` parameter (query string na API)
   - Folders: `HPA-Upscale`, `HPA-Downscale`, `Node-Upscale`, `Node-Downscale`, `Mixed`

2. **Metadata auto-calculada**
   - NÃƒO editar manualmente campos `clusters_affected`, `namespaces_affected`
   - Backend recalcula automaticamente ao salvar/atualizar sessÃ£o

3. **Compatibilidade TUI â†” Web**
   - Mesmo formato JSON
   - Mesma estrutura de diretÃ³rios (`~/.k8s-hpa-manager/sessions/`)
   - `SessionManager` Go compartilhado por ambos

### Race Conditions Conhecidas (RESOLVIDAS)

1. **getClient() race condition** âœ… RESOLVIDO
   - MÃºltiplos goroutines criavam clients simultaneamente
   - **Fix**: `sync.RWMutex` com double-check locking
   - Ver `internal/config/kubeconfig.go`

2. **testClusterConnections() race** âœ… RESOLVIDO
   - `tea.Batch()` iniciava todos testes em paralelo
   - **Fix**: Mutex protege criaÃ§Ã£o de clients (read lock para leituras, write lock para criaÃ§Ã£o)

---

## ğŸ§ª Testing Strategy

### Unit Tests

```bash
make test                     # Run all tests
make test-coverage            # Coverage report â†’ coverage.html
```

### Manual Testing Web

**Pre-requisitos:**
1. Build obrigatÃ³rio: `./rebuild-web.sh -b`
2. Hard refresh no browser: `Ctrl+Shift+R`

**Checklist:**
- [ ] HPAs: Load, Edit (min/max replicas, targets, resources), Apply
- [ ] Node Pools: Load, Edit (count, autoscaling, min/max), Apply
- [ ] Sessions: Save, Load, Rename, Delete, Edit Content
- [ ] Staging Area: Add items, Clear, Apply, Cancel
- [ ] ApplyAllModal: Preview changes, Apply, Progress tracking
- [ ] Heartbeat: Abrir tab â†’ fechar â†’ servidor desliga em 20min
- [ ] Snapshot: Capturar estado do cluster para rollback
- [ ] Dashboard: MÃ©tricas reais (CPU/Memory allocation)
- [ ] **Recovery Mode**: SeleÃ§Ã£o granular de itens, validaÃ§Ã£o de cluster, progress tracking, resumo final

**Logs:**
```bash
# Modo background
tail -f /tmp/k8s-hpa-manager-web-*.log

# Modo foreground
./build/k8s-hpa-manager web -f --debug
```

### Manual Testing TUI

```bash
make run-dev                              # Debug mode
./build/k8s-hpa-manager --demo            # Demo mode (sem executar)
./build/k8s-hpa-manager --debug           # Debug logging
```

**Checklist:**
- [ ] Cluster discovery e conexÃ£o (F5 para reload)
- [ ] Multi-namespace selection (Space para selecionar mÃºltiplos)
- [ ] HPA batch operations (Ctrl+U para aplicar todos)
- [ ] Node Pool sequential execution (F12 para marcar *1 e *2)
- [ ] Session save/load (Ctrl+S/Ctrl+L)
- [ ] VPN validation (mensagens em operaÃ§Ãµes crÃ­ticas)
- [ ] CronJob management (F9)
- [ ] Prometheus Stack (F8)
- [ ] Log viewer (F3)
- [ ] Modais de confirmaÃ§Ã£o (Ctrl+D/Ctrl+U)

### Testing VPN Validation

**Simular VPN desconectada:**
```bash
# Desconectar VPN
sudo ifconfig <vpn-interface> down

# Iniciar aplicaÃ§Ã£o
./build/k8s-hpa-manager

# Esperado:
# ğŸ” Validando conectividade VPN...
# âŒ VPN desconectada - Kubernetes inacessÃ­vel
# ğŸ’¡ SOLUÃ‡ÃƒO: Conecte-se Ã  VPN e tente novamente (F5)
```

### Testing Auto-Shutdown (Web)

```bash
# Iniciar servidor em foreground para ver logs
./build/k8s-hpa-manager web -f --debug

# Abrir browser em http://localhost:8080
# Fechar todas as abas
# Aguardar 20 minutos

# Esperado no terminal:
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘             AUTO-SHUTDOWN POR INATIVIDADE                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â° Ãšltimo heartbeat: 14:35:22 (hÃ¡ 20 minutos)
# ğŸ›‘ Nenhuma pÃ¡gina web conectada por mais de 20 minutos
# âœ… Servidor sendo encerrado...
```

### Testing Update System

**Teste 1: DetecÃ§Ã£o de Updates**
```bash
./build/k8s-hpa-manager version

# Esperado (se houver update disponÃ­vel):
# k8s-hpa-manager versÃ£o 1.1.0
# ğŸ” Verificando updates...
# ğŸ†• Nova versÃ£o disponÃ­vel: 1.1.0 â†’ 1.2.0
# ğŸ“¦ Download: https://github.com/Paulo-Ribeiro-Log/Scale_HPA/releases/tag/v1.2.0
# ğŸ“ Release Notes (preview): ...
```

**Teste 2: Auto-Update Check**
```bash
~/.k8s-hpa-manager/scripts/auto-update.sh --check

# Esperado:
# Status da InstalaÃ§Ã£o
# â„¹ï¸  VersÃ£o atual: 1.1.0
# â„¹ï¸  LocalizaÃ§Ã£o: /usr/local/bin/k8s-hpa-manager
# âš ï¸  Nova versÃ£o disponÃ­vel: 1.1.0 â†’ 1.2.0
```

**Teste 3: Auto-Update Dry-Run**
```bash
~/.k8s-hpa-manager/scripts/auto-update.sh --dry-run --yes

# Esperado:
# âš ï¸  MODO DRY RUN - Nenhuma alteraÃ§Ã£o serÃ¡ feita
# â„¹ï¸  Auto-confirmaÃ§Ã£o ativada (--yes)
# [DRY RUN] Simulando download e instalaÃ§Ã£o...
# âœ… SimulaÃ§Ã£o concluÃ­da! (modo dry-run)
```

**Teste 4: Cache de VerificaÃ§Ã£o**
```bash
# Verificar cache
ls -lh ~/.k8s-hpa-manager/.update-check
cat ~/.k8s-hpa-manager/.update-check

# ForÃ§ar nova verificaÃ§Ã£o
rm ~/.k8s-hpa-manager/.update-check
./build/k8s-hpa-manager version
```

**Teste 5: InstalaÃ§Ã£o do Zero**
```bash
# Em mÃ¡quina limpa ou container
curl -fsSL https://raw.githubusercontent.com/Paulo-Ribeiro-Log/Scale_HPA/main/install-from-github.sh | bash

# Esperado:
# âœ… InstalaÃ§Ã£o concluÃ­da com sucesso!
# VersÃ£o instalada: 1.2.0
# BinÃ¡rio: /usr/local/bin/k8s-hpa-manager
```

---

## ğŸ”§ Troubleshooting

### Problemas Comuns Web

| Problema | SoluÃ§Ã£o |
|----------|---------|
| **Tela branca apÃ³s rebuild** | Hard refresh: `Ctrl+Shift+R` |
| **"TabProvider not found"** | Adicionar `<TabProvider>` em App.tsx |
| **Sessions nÃ£o carregam** | Verificar `~/.k8s-hpa-manager/sessions/` existe |
| **Cluster not found** | Executar `k8s-hpa-manager autodiscover` |
| **401 Unauthorized** | Token incorreto - usar `poc-token-123` (default) |
| **Servidor nÃ£o desliga** | Verificar heartbeat no console do browser (POST /heartbeat a cada 5min) |

### Problemas Comuns TUI

| Problema | SoluÃ§Ã£o |
|----------|---------|
| **Cluster offline** | `kubectl cluster-info --context=<cluster>` |
| **VPN desconectada** | Conectar VPN e pressionar F5 para reload |
| **HPAs nÃ£o carregam** | Verificar RBAC e toggle namespaces sistema (tecla `S`) |
| **Azure timeout** | Validar `az login` e subscription ativa |
| **Race condition** | Atualizar para versÃ£o com mutex fix (v1.6.0+) |
| **Node pools nÃ£o carregam** | Executar `k8s-hpa-manager autodiscover` |

### Problemas Comuns - Sistema de Updates

| Problema | SoluÃ§Ã£o |
|----------|---------|
| **Updates nÃ£o detectados** | Remover cache: `rm ~/.k8s-hpa-manager/.update-check` e executar `k8s-hpa-manager version` |
| **GitHub API rate limit** | Configurar token: `export GITHUB_TOKEN=ghp_...` antes de executar |
| **VersÃ£o mostra "dev"** | Recompilar com `make build` (injeta versÃ£o via git tags) |
| **Cache nÃ£o expira** | TTL de 24h - forÃ§ar com `rm ~/.k8s-hpa-manager/.update-check` |
| **Auto-update falha** | Verificar conexÃ£o, permissÃµes sudo e requisitos (Go, Git, kubectl) |
| **Scripts nÃ£o instalados** | Executar `curl ... install-from-github.sh | bash` novamente |

### Debug Mode

```bash
# TUI
k8s-hpa-manager --debug

# Web
./build/k8s-hpa-manager web -f --debug

# Logs exibidos:
#   - Estado da aplicaÃ§Ã£o (AppState transitions)
#   - Mensagens Bubble Tea
#   - OperaÃ§Ãµes Kubernetes (API calls)
#   - Azure authentication flow
#   - HTTP requests/responses (web)
```

### Backup e Restore

```bash
# Criar backup antes de modificaÃ§Ãµes
./backup.sh "descriÃ§Ã£o do backup"

# Listar backups disponÃ­veis
./restore.sh

# Restaurar backup especÃ­fico
./restore.sh backup_20251001_122526
```

- MantÃ©m 10 backups mais recentes
- Metadados inclusos (git commit, data, usuÃ¡rio)

---

## ğŸš€ Continuing Development

### Context for Next Claude Sessions

**Quick Context Template:**
```
Projeto: Terminal-based Kubernetes HPA + Azure AKS Node Pool management tool

VersÃ£o Atual: v1.2.0 (Outubro 2025)
Release: https://github.com/Paulo-Ribeiro-Log/Scale_HPA/releases/tag/v1.2.0

Tech Stack:
- Go 1.23+ (toolchain 1.24.7)
- TUI: Bubble Tea + Lipgloss
- Web: React 18.3 + TypeScript 5.8 + Vite 5.4 + shadcn/ui
- K8s: client-go v0.31.4
- Azure: azcore v1.19.1, azidentity v1.12.0

Estado Atual (Outubro 2025):
âœ… TUI completo com execuÃ§Ã£o sequencial, validaÃ§Ã£o VPN, modais
âœ… Web interface 99% funcional (HPAs, Node Pools, Sessions, Dashboard)
âœ… Sistema de heartbeat e auto-shutdown (20min inatividade)
âœ… Snapshot de cluster para rollback
âœ… Race condition corrigida (mutex RWLock)
âœ… Compatibilidade TUI â†” Web para sessÃµes
âœ… Sistema completo de instalaÃ§Ã£o e updates (v1.2.0)

Build TUI: make build
Build Web: ./rebuild-web.sh -b
Binary: ./build/k8s-hpa-manager
InstalaÃ§Ã£o: curl -fsSL https://raw.githubusercontent.com/Paulo-Ribeiro-Log/Scale_HPA/main/install-from-github.sh | bash
```

### File Structure Quick Reference

```
internal/
â”œâ”€â”€ tui/                       # Terminal UI (Bubble Tea)
â”‚   â”œâ”€â”€ app.go                 # Main orchestrator + text methods
â”‚   â”œâ”€â”€ handlers.go            # Event handling
â”‚   â”œâ”€â”€ views.go               # UI rendering
â”‚   â”œâ”€â”€ resource_*.go          # Resource management
â”‚   â””â”€â”€ components/            # UI components
â”œâ”€â”€ web/                       # Web Interface
â”‚   â”œâ”€â”€ frontend/src/          # React/TypeScript SPA
â”‚   â”‚   â”œâ”€â”€ components/        # shadcn/ui components
â”‚   â”‚   â”œâ”€â”€ contexts/          # StagingContext, TabContext
â”‚   â”‚   â””â”€â”€ pages/             # Index, CronJobs, Prometheus
â”‚   â”œâ”€â”€ handlers/              # Go REST API
â”‚   â”‚   â”œâ”€â”€ hpas.go
â”‚   â”‚   â”œâ”€â”€ nodepools.go
â”‚   â”‚   â””â”€â”€ sessions.go
â”‚   â””â”€â”€ server.go              # Gin HTTP server
â”œâ”€â”€ models/types.go            # App state (AppModel)
â”œâ”€â”€ session/manager.go         # Session persistence
â”œâ”€â”€ kubernetes/client.go       # K8s wrapper (com mutex)
â”œâ”€â”€ config/kubeconfig.go       # Cluster discovery (com mutex)
â””â”€â”€ azure/auth.go              # Azure auth
```

### Development Commands Quick Reference

```bash
# TUI
make build                    # â†’ ./build/k8s-hpa-manager
make run-dev                  # Debug mode

# Web
./rebuild-web.sh -b           # Build completo (recomendado)
make web-dev                  # Vite dev server
./build/k8s-hpa-manager web   # Run server

# Testing
make test                     # Unit tests
make test-coverage            # Coverage report

# Installation & Updates
curl -fsSL https://raw.githubusercontent.com/Paulo-Ribeiro-Log/Scale_HPA/main/install-from-github.sh | bash
k8s-hpa-manager version       # Check version and updates
~/.k8s-hpa-manager/scripts/auto-update.sh              # Interactive update
~/.k8s-hpa-manager/scripts/auto-update.sh --yes        # Auto-confirm (for cron)
~/.k8s-hpa-manager/scripts/auto-update.sh --check      # Check status
~/.k8s-hpa-manager/scripts/auto-update.sh --dry-run    # Simulate

# Cluster setup
k8s-hpa-manager autodiscover  # Auto-descobre clusters

# Backup
./backup.sh "desc"            # Create backup
./restore.sh                  # List/restore backups
```

### Best Practices

**When Adding Features:**
1. Follow MVC pattern: Views in `views.go`, logic in `handlers.go`, state in `models/types.go`
2. Use Bubble Tea commands for async operations (NUNCA goroutines diretas)
3. Update help in `renderHelp()` function (TUI)
4. Run `make build` (TUI) ou `./rebuild-web.sh -b` (Web) after changes
5. Update this CLAUDE.md

**Code Style:**
- **Error handling**: Proper propagation, no panics
- **State management**: All UI state in `AppModel` (TUI) ou Context API (Web)
- **Async operations**: Bubble Tea commands (TUI) ou React Query (Web)
- **Unicode safety**: Always use `[]rune` para texto
- **Logging**: Use `a.debugLog()` method (TUI) ou console (Web)
- **Concurrency**: Mutex quando necessÃ¡rio (ex: `clientMutex` em `getClient()`)

**Common Gotchas:**
- Function closures: Check for missing `}`
- Bubble Tea returns: Always return `tea.Model` and `tea.Cmd`
- Text editing: Initialize `CursorPosition` when starting
- Session persistence: Use folder-aware functions
- Azure auth: Handle token expiration gracefully
- Web rebuild: SEMPRE usar `./rebuild-web.sh -b`
- Hard refresh: `Ctrl+Shift+R` apÃ³s rebuild web

### Known Technical Debt

**Code Quality:**
- Some async operations need better error propagation
- Unit test coverage could be expanded (especialmente web handlers)
- Inline documentation for complex functions
- Large cluster lists could benefit from virtualization

**UI/UX:**
- Better handling of very small terminals (TUI)
- Support for color themes/accessibility (both)
- More intuitive keyboard shortcuts (TUI)
- More detailed progress indicators (both)

### Potential Next Features

**High Priority - TODAS IMPLEMENTADAS! âœ…**
1. âœ… Field validation (CPU/memory formats, replica ranges)
2. âœ… Undo/Redo functionality (via staging + menu de ediÃ§Ã£o)
3. âœ… Search/Filter within HPA/namespace lists (campos de busca implementados)
4. âœ… Export sessions to YAML/JSON (save/load session)

**Medium Priority:**
5. â³ User-configurable templates (parcial - folders existem)
6. â³ **Metrics integration (current usage alongside targets)** - [Ver plano detalhado](./Docs/METRICS_INTEGRATION_PLAN.md)
7. âœ… History tracking with timestamps
8. â³ Plugin system for custom validation

**Advanced:**
9. â³ Git integration for config tracking
10. â³ **Alertmanager integration (proactive alerts + recommendations)** - [Ver plano detalhado](./ALERTMANAGER_INTEGRATION_PLAN.md)
11. âœ… RESTful API for external tools
12. â³ Prometheus/Grafana integration (parcial - apenas Prometheus Stack management)

---

## ğŸ“œ HistÃ³rico de CorreÃ§Ãµes (Principais)

### Nova Arquitetura: SimpleCollector (Novembro 2025) âœ…

**Data:** 08 de novembro de 2025

**MotivaÃ§Ã£o:** Sistema de rotaÃ§Ã£o de portas (RotatingCollector) era complexo e nÃ£o estava funcionando corretamente. Port-forwards nÃ£o eram criados para todos os clusters e baseline nÃ£o era carregado do SQLite.

**Problema anterior:**
- RotaÃ§Ã£o de portas (55551-55556) entre mÃºltiplos clusters nÃ£o escalava
- Port-forward temporÃ¡rio durante scans (criado e destruÃ­do rapidamente)
- Baseline era recriado toda vez ao invÃ©s de carregar do SQLite
- Sistema complexo com slots e duraÃ§Ã£o calculada dinamicamente

**SoluÃ§Ã£o: SimpleCollector - Arquitetura Simplificada**

**Novo modelo:**
1. **Scans normais**: 1 porta por cluster, port-forward criado durante scan e destruÃ­do apÃ³s
2. **Baseline**: Porta dedicada (55557) separada dos scans
3. **LÃ³gica inteligente de baseline**:
   - Verifica primeiro se baseline existe no SQLite via `IsBaselineReady()`
   - SÃ³ coleta baseline se nÃ£o existir ou estiver desatualizado
   - Porta 55557 criada sob demanda, destruÃ­da apÃ³s coleta

**Componentes implementados:**

**1ï¸âƒ£ SimpleCollector** (`internal/monitoring/collector/simple_collector.go`):
```go
type SimpleCollector struct {
    targets       map[string]*SimpleTarget // Cluster â†’ Target mapping
    scanPorts     []int                    // [55551-55556] para scans normais
    baselinePort  int                      // 55557 para baseline
    baselineQueue chan BaselineRequest     // Fila de baselines pendentes
}
```

**2ï¸âƒ£ Fluxos principais:**

**Scan normal (30s interval):**
```
1. executeScan() â†’ scanCluster(cluster)
2. Criar port-forward temporÃ¡rio
3. Aguardar 2s para port-forward estar pronto
4. Coletar mÃ©tricas via Prometheus (CPU, Memory, Replicas)
5. Enriquecer snapshot com K8s API (se disponÃ­vel)
6. Salvar snapshots no SQLite (batch)
7. Destruir port-forward
```

**Baseline (sob demanda):**
```
1. AddTarget() â†’ requestBaselineIfNeeded()
2. Verificar se baseline existe: persistence.IsBaselineReady()
3. Se nÃ£o existe ou desatualizado â†’ addToBaselineQueue()
4. baselineWorker() processa fila
5. Criar port-forward na porta 55557
6. collectHistoricalData() busca 3 dias via QueryRange
7. Salvar ~4320 snapshots no SQLite
8. Marcar baseline como pronto: MarkBaselineReady()
9. Destruir port-forward (libera porta)
```

**3ï¸âƒ£ VerificaÃ§Ã£o de baseline:**
```go
func (c *SimpleCollector) requestBaselineIfNeeded(cluster, namespace, hpaName string) {
    // Verifica se baseline jÃ¡ existe e estÃ¡ atualizado
    ready, err := c.persistence.IsBaselineReady(cluster, namespace, hpaName)

    if ready {
        log.Debug().Msg("Baseline jÃ¡ existe e estÃ¡ atualizado")
        return
    }

    // Baseline nÃ£o existe ou estÃ¡ desatualizado, adiciona Ã  fila
    c.addToBaselineQueue(cluster, namespace, hpaName)
}
```

**BenefÃ­cios:**
- âœ… **Simplicidade**: 1 arquivo ao invÃ©s de sistema complexo de rotaÃ§Ã£o
- âœ… **Escalabilidade**: Suporta N clusters (scan sequencial)
- âœ… **SeparaÃ§Ã£o de responsabilidades**: Scans e baseline nÃ£o interferem entre si
- âœ… **Baseline inteligente**: Carrega do SQLite primeiro, sÃ³ recria se necessÃ¡rio
- âœ… **Port-forward eficiente**: Criado sob demanda, destruÃ­do apÃ³s uso
- âœ… **Fila de baseline**: Processa HPAs sequencialmente sem sobrecarga

**Arquivos criados:**
- `internal/monitoring/collector/simple_collector.go` (NOVO - ~665 linhas)

**PrÃ³ximo passo:**
- Integrar SimpleCollector no `internal/monitoring/engine/engine.go` (substituir RotatingCollector)

---

### CorreÃ§Ã£o: Linhas de ReferÃªncia nos GrÃ¡ficos de MÃ©tricas (Novembro 2025) âœ…

**Data:** 08 de novembro de 2025

**Problema:** Linhas tracejadas de CPU Request e CPU Limit nÃ£o apareciam no grÃ¡fico de CPU da pÃ¡gina de Monitoring, apesar de funcionarem corretamente no grÃ¡fico de Memory.

**Root Cause:** O eixo Y do grÃ¡fico de CPU estava com escala automÃ¡tica baseada apenas nos valores de uso (0.8% a 3.6%), mas as ReferenceLine estavam posicionadas em 75% (Request) e 100% (Limit), ficando **fora da escala visÃ­vel do grÃ¡fico**.

**SoluÃ§Ã£o implementada:**
1. **Domain fixo no YAxis**: ForÃ§ado `domain={[0, 150]}` para garantir que linhas atÃ© 100% sejam sempre visÃ­veis
2. **Label completo no Target**: Adicionado valor percentual no label da linha verde (`Target: 60%`)
3. **Aplicado em ambos os grÃ¡ficos**: CPU e Memory agora tÃªm comportamento consistente

**Arquivos modificados:**
- `internal/web/frontend/src/components/MetricsPanel.tsx`:
  - Linha 522: `domain={[0, 150]}` no YAxis de CPU
  - Linha 530: Label `Target: ${cpuTarget}%` com cor verde
  - Linha 686: `domain={[0, 150]}` no YAxis de Memory
  - Linha 694: Label `Target: ${memoryTarget}%` com cor verde

**Resultado:**
- âœ… Linhas tracejadas de Request (laranja) e Limit (vermelha) agora aparecem corretamente
- âœ… Linha Target (verde) com label descritivo
- âœ… Escala do grÃ¡fico vai atÃ© 150% para acomodar picos acima do limit
- âœ… ConsistÃªncia visual entre grÃ¡ficos de CPU e Memory

---

### RefatoraÃ§Ã£o Completa: Sistema de Monitoramento RotatingCollector (Novembro 2025) âœ…

**Data:** 07 de novembro de 2025

**MotivaÃ§Ã£o:** Sistema de monitoramento anterior (TimeSlotManager + BaselineWorkers + Queue + Scheduler) tinha 800+ linhas de cÃ³digo complexo, violando princÃ­pio KISS e causando over-engineering.

**SoluÃ§Ã£o:** RefatoraÃ§Ã£o completa em 3 fases, reduzindo para ~450 linhas com arquitetura simplificada.

---

#### **FASE 1: Limpeza de CÃ³digo Legado** âœ…

**Arquivos deletados:**
- âŒ `internal/monitoring/timeslot/timeslot.go` (~300 linhas)
- âŒ `internal/monitoring/baseline/worker.go` (~200 linhas)
- âŒ `internal/monitoring/baseline/queue.go` (~150 linhas)
- âŒ `internal/monitoring/baseline/scheduler.go` (~200 linhas)
- âŒ `monitoring-targets.json` (persistÃªncia duplicada)

**Arquivos limpos:**
- `internal/monitoring/engine/engine.go` - Removidos imports e referÃªncias aos componentes deletados

**Resultado:** -850 linhas de cÃ³digo complexo removidas

---

#### **FASE 2: RotatingCollector - Sistema Simplificado** âœ…

**Arquivo criado:** `internal/monitoring/collector/rotating.go` (~450 linhas)

**Arquitetura:**

```go
type RotatingCollector struct {
    clusters     []string                    // Lista de clusters ativos
    targets      map[string]*ClusterTarget   // Cluster â†’ Target mapping
    ports        []int                       // [55551, 55552, 55553, 55554, 55555, 55556]
    slotDuration time.Duration               // Calculado: 60s / totalSlots
    currentSlot  int
    totalSlots   int                         // ceil(len(clusters) / 6)

    persistence  *storage.Persistence
    pfManager    *portforward.PortForwardManager
    kubeManager  *config.KubeConfigManager

    running      bool
    stopCh       chan struct{}
    mu           sync.RWMutex
    wg           sync.WaitGroup
    ctx          context.Context
    cancel       context.CancelFunc
}
```

**Funcionalidades:**

**1ï¸âƒ£ RotaÃ§Ã£o DinÃ¢mica de Portas:**
- 6 portas fixas (55551-55556)
- RotaÃ§Ã£o inteligente: `totalSlots = ceil(numClusters / 6)`
- DuraÃ§Ã£o de slot adaptativa: `slotDuration = 60s / totalSlots`
- Exemplo: 11 clusters â†’ 2 slots de 30s cada

**2ï¸âƒ£ MÃ©todos Principais:**
```go
func NewRotatingCollector(...) *RotatingCollector
func (c *RotatingCollector) Start() error
func (c *RotatingCollector) Stop()
func (c *RotatingCollector) AddTarget(target scanner.ScanTarget)
func (c *RotatingCollector) RemoveTarget(cluster string)
func (c *RotatingCollector) rotationLoop()              // Loop principal
func (c *RotatingCollector) collectSlot(slotIndex int)  // Coleta 1 slot (6 clusters paralelos)
func (c *RotatingCollector) collectCluster(cluster, port) error
```

**3ï¸âƒ£ Coleta de MÃ©tricas:**
```go
// Dentro de collectCluster():
promEndpoint := fmt.Sprintf("http://localhost:%d", port)
promClient := prometheus.NewClient(cluster, promEndpoint)

for _, ns := range target.Namespaces {
    for _, hpaName := range target.HPAs {
        snapshot := &models.HPASnapshot{
            Cluster: cluster, Namespace: ns, Name: hpaName, Timestamp: now,
        }
        promClient.EnrichSnapshot(ctx, snapshot) // Coleta CPU, Memory, Replicas
        snapshots = append(snapshots, snapshot)
    }
}

persistence.SaveSnapshots(snapshots) // Batch insert no SQLite
```

**4ï¸âƒ£ RecÃ¡lculo DinÃ¢mico:**
```go
func (c *RotatingCollector) recalculateSlots() {
    numClusters := len(c.clusters)
    numPorts := len(c.ports)

    c.totalSlots = (numClusters + numPorts - 1) / numPorts  // Ceiling division
    c.slotDuration = 60 * time.Second / time.Duration(c.totalSlots)
}
```

**IntegraÃ§Ã£o no Engine:**
```go
// engine.go: InicializaÃ§Ã£o
kubeManager, _ := config.NewKubeConfigManager(kubeconfigPath)
rotatingCollector := collector.NewRotatingCollector(persistence, pfManager, kubeManager)

// Start()
if err := rotatingCollector.Start(); err != nil {
    return err
}

// AddTarget()
if e.running && e.rotatingCollector != nil {
    e.rotatingCollector.AddTarget(target)
}

// Stop()
if e.rotatingCollector != nil {
    e.rotatingCollector.Stop()
}
```

**Testes:**
- âœ… CompilaÃ§Ã£o sem erros
- âœ… 11 clusters carregados
- âœ… Slots recalculados dinamicamente (1 slot â†’ 2 slots)
- âœ… Graceful shutdown funcionando

---

#### **FASE 3: Baseline Inteligente** âœ…

**Feature:** Coleta histÃ³rica de 3 dias (72h) de mÃ©tricas do Prometheus para novos HPAs.

**ImplementaÃ§Ã£o:**

```go
func (c *RotatingCollector) CollectBaseline(cluster, namespace, hpaName string) {
    c.wg.Add(1)
    go func() {
        defer c.wg.Done()

        // 1. Port-forward temporÃ¡rio
        c.pfManager.Start(cluster)
        defer c.pfManager.Stop(cluster)

        // 2. Cliente Prometheus
        promClient, _ := prometheus.NewClient(cluster, "http://localhost:55551")

        // 3. Range de 3 dias
        end := time.Now()
        start := end.Add(-72 * time.Hour)
        step := 1 * time.Minute

        // 4. Query range para histÃ³rico
        replicasResult, _ := promClient.QueryRange(ctx, replicasQuery, start, end, step)
        cpuResult, _ := promClient.QueryRange(ctx, cpuQuery, start, end, step)
        memoryResult, _ := promClient.QueryRange(ctx, memoryQuery, start, end, step)

        // 5. Converter para snapshots (~4320 pontos)
        snapshots := parseResults(replicasResult, cpuResult, memoryResult)

        // 6. Batch insert no SQLite
        c.persistence.SaveSnapshots(snapshots)
        c.persistence.MarkBaselineReady(cluster, namespace, hpaName)
    }()
}
```

**Trigger AutomÃ¡tico:**
```go
// engine.go: AddTarget()
if e.running && e.rotatingCollector != nil {
    for _, ns := range target.Namespaces {
        for _, hpaName := range target.HPAs {
            e.rotatingCollector.CollectBaseline(target.Cluster, ns, hpaName)
        }
    }
}
```

**Queries Prometheus:**
```go
// RÃ©plicas
kube_horizontalpodautoscaler_status_current_replicas{namespace="X",horizontalpodautoscaler="Y"}

// CPU
sum(rate(container_cpu_usage_seconds_total{namespace="X",pod=~"Y.*"}[1m])) /
sum(kube_pod_container_resource_requests{namespace="X",pod=~"Y.*",resource="cpu"}) * 100

// MemÃ³ria
sum(container_memory_working_set_bytes{namespace="X",pod=~"Y.*"}) /
sum(kube_pod_container_resource_requests{namespace="X",pod=~"Y.*",resource="memory"}) * 100
```

**CorrelaÃ§Ã£o de Timestamps:**
```go
// Usa rÃ©plicas como base, busca CPU/Memory com Â±30s de tolerÃ¢ncia
for _, sample := range replicasMatrix {
    for _, value := range sample.Values {
        timestamp := time.Unix(int64(value.Timestamp)/1000, 0)
        snapshot := &models.HPASnapshot{Timestamp: timestamp, ...}

        // Busca CPU correspondente
        for _, cpuSample := range cpuMatrix[0].Values {
            cpuTimestamp := time.Unix(int64(cpuSample.Timestamp)/1000, 0)
            if cpuTimestamp.Equal(timestamp) || cpuTimestamp.Sub(timestamp).Abs() < 30*time.Second {
                snapshot.CPUCurrent = float64(cpuSample.Value)
                break
            }
        }
        // ... mesmo para memÃ³ria
    }
}
```

**Testes:**
- âœ… CollectBaseline() chamado ao adicionar HPA
- âœ… Port-forward criado (porta 55551)
- âœ… Query range executado (3 dias)
- âœ… Batch insert no SQLite
- âœ… Flag `baseline_ready` marcada
- âœ… Testes unitÃ¡rios atualizados (4 PASS, 3 SKIP)

---

**Arquivos modificados:**
- `internal/monitoring/collector/rotating.go` (NOVO - 602 linhas)
- `internal/monitoring/engine/engine.go` (+40 linhas)
- `internal/monitoring/engine/engine_baseline_test.go` (2 testes desabilitados com documentaÃ§Ã£o)

**BenefÃ­cios:**
- âœ… **ReduÃ§Ã£o de cÃ³digo**: 850 linhas â†’ 450 linhas (~53% menor)
- âœ… **Simplicidade**: 1 arquivo ao invÃ©s de 4+ componentes
- âœ… **KISS**: RotaÃ§Ã£o simples com slots dinÃ¢micos
- âœ… **Escalabilidade**: Suporta N clusters com apenas 6 portas
- âœ… **Baseline automÃ¡tico**: Coleta histÃ³rica de 3 dias para novos HPAs
- âœ… **Manutenibilidade**: CÃ³digo fÃ¡cil de entender e debugar

**Problemas conhecidos resolvidos:**
- âœ… Over-engineering eliminado
- âœ… Port-forwards gerenciados corretamente (temporÃ¡rios por scan)
- âœ… Graceful shutdown implementado
- âœ… Thread-safe (RWMutex)
- âœ… Testes atualizados para nova arquitetura

---

### PÃ¡gina de Monitoring + IntegraÃ§Ã£o HPA-Watchdog (Novembro 2025) âœ…

**Data:** 05 de novembro de 2025

**Feature implementada:** PÃ¡gina de monitoramento em tempo real integrada com o HPA-Watchdog engine, com sidebar retrÃ¡til e coleta automÃ¡tica de mÃ©tricas via Prometheus.

**Componentes implementados:**

**1ï¸âƒ£ MonitoringPage com Sidebar RetrÃ¡til**
- Sidebar 320px com lista de HPAs monitorados (agrupados por cluster)
- BotÃ£o toggle para esconder/mostrar sidebar (maximiza Ã¡rea de grÃ¡ficos)
- AnimaÃ§Ã£o suave de transiÃ§Ã£o (300ms)
- Badge de status do engine (ğŸŸ¢ Ativo / âš« Parado) com atualizaÃ§Ã£o a cada 10s

**2ï¸âƒ£ IntegraÃ§Ã£o Backend - Monitoring Engine**
- Handler `AddHPA` com normalizaÃ§Ã£o automÃ¡tica de cluster name (remove `-admin`)
- Sistema de persistÃªncia automÃ¡tica de targets em `~/.k8s-hpa-manager/monitoring-targets.json`
- Port-forward automÃ¡tico por scan (start â†’ scan â†’ stop) para cada cluster
- Compatibilidade com mÃºltiplos clusters simultÃ¢neos

**3ï¸âƒ£ CorreÃ§Ã£o CrÃ­tica: NormalizaÃ§Ã£o de Cluster Name**
- **Problema**: Frontend enviava `akspriv-prod-admin`, mas port-forward precisava de `akspriv-prod`
- **SoluÃ§Ã£o**: Handler `AddHPA` remove sufixo `-admin` automaticamente (linha 485)
```go
clusterName := strings.TrimSuffix(req.Cluster, "-admin")
```

**4ï¸âƒ£ API Client - Novos MÃ©todos**
```typescript
addHPAToMonitoring(cluster, namespace, hpa)  // POST /monitoring/hpa
getMonitoringStatus()                         // GET /monitoring/status
startMonitoring()                             // POST /monitoring/start
```

**5ï¸âƒ£ Workflow Completo**
1. UsuÃ¡rio seleciona HPA e clica "Monitorar"
2. Frontend chama `addHPAToMonitoring()` com cluster normalizado
3. Backend adiciona target ao engine (sem `-admin`)
4. Engine inicia automaticamente se parado
5. Port-forward Ã© criado por scan: `kubectl port-forward svc/prometheus-k8s -n monitoring --context akspriv-prod-admin`
6. MÃ©tricas coletadas via Prometheus e salvas no cache
7. Frontend exibe mÃ©tricas em tempo real na sidebar

**Arquivos modificados:**
- `internal/web/frontend/src/pages/MonitoringPage.tsx` - Sidebar retrÃ¡til + badge status
- `internal/web/frontend/src/lib/api/client.ts` - MÃ©todos de monitoring (removida duplicata)
- `internal/web/handlers/monitoring.go` - NormalizaÃ§Ã£o de cluster + logs detalhados
- `internal/monitoring/engine/engine.go` - Port-forward por scan (jÃ¡ existia)
- `internal/web/frontend/src/pages/Index.tsx` - Handler onMonitor com auto-start

**Problemas Identificados e SoluÃ§Ãµes:**
- âŒ **Targets antigos com `-admin`**: Salvos antes da correÃ§Ã£o, quebravam port-forward
  - âœ… SoluÃ§Ã£o: Remover via API ou limpar arquivo `monitoring-targets.json`
- âŒ **localStorage com HPAs antigos**: Dados obsoletos no browser
  - âœ… SoluÃ§Ã£o: `localStorage.removeItem("monitored_hpas")` + reload

**BenefÃ­cios:**
- âœ… Monitoramento em tempo real de mÃºltiplos clusters
- âœ… Sidebar retrÃ¡til maximiza Ã¡rea de grÃ¡ficos
- âœ… Auto-start do engine quando HPA Ã© adicionado
- âœ… PersistÃªncia de targets entre reinicializaÃ§Ãµes
- âœ… Port-forward automÃ¡tico e isolado por scan

**âš ï¸ PROBLEMA IDENTIFICADO (Novembro 2025):**

ApÃ³s anÃ¡lise detalhada do fluxo de monitoramento, foi identificado que a **implementaÃ§Ã£o atual estÃ¡ ERRADA**:

**Problemas crÃ­ticos:**
1. **Port-forward efÃªmero**: Porta Ã© criada e destruÃ­da a cada scan (engine.go:373-389)
2. **Sem baseline histÃ³rica**: Monitoring inicia sem dados de comparaÃ§Ã£o
3. **Sem fila de portas**: NÃ£o hÃ¡ gerenciamento de duas portas simultÃ¢neas
4. **Cleanup inadequado**: Portas podem ficar Ã³rfÃ£s se servidor crashar

**Fluxo CORRETO (conforme explicado pelo usuÃ¡rio):**

> "o fluxo deve iniciar com o portfoward do prometheus no namespace 'monitoring' na porta 9090, e seguir com a coleta historica dos dados do prometheus dos ultimos 3 dias do hpa selecionado, isso feito os dados serÃ£o salvos no sqlite e a partir dai o hpa comeÃ§a a ser monitorado de fato, pois jÃ¡ temos a base para iniciar a comparaÃ§Ã£o e analise. isso Ã© extremamente importante pois sem essa parte nada temos como comparativo."

**Arquitetura correta:**
1. **Port-forward persistente**: Vive durante toda execuÃ§Ã£o do servidor (nÃ£o por scan)
2. **Coleta histÃ³rica PRIMEIRO**: 3 dias de dados via Prometheus range queries â†’ SQLite
3. **Baseline obrigatÃ³ria**: SÃ³ inicia monitoring apÃ³s coletar histÃ³rico
4. **Duas portas simultÃ¢neas**: 55553 e 55554 abertas ao mesmo tempo
5. **Fila alternada**: Leitura alternada entre portas (load balancing)
6. **Cleanup garantido**: DestruiÃ§Ã£o apenas no shutdown do servidor

**Documento de refatoraÃ§Ã£o criado:**
- `/home/paulo/Scripts/Scripts GO/Scale_HPA/Scale_HPA/MONITORING_IMPLEMENTATION_TODO.md`
- ContÃ©m 4 fases de implementaÃ§Ã£o detalhadas
- Inclui cÃ³digo de exemplo e planos de teste

**âœ… IMPLEMENTAÃ‡ÃƒO CONCLUÃDA (06 nov 2025) - Fases 1-4 REFATORADAS:**

### RefatoraÃ§Ã£o Completa: Time-Slot Based Scanning âœ…

**Problema original:** Port-forwards persistentes (1 por cluster) nÃ£o escalavam para >2 clusters (sÃ³ 2 portas disponÃ­veis: 55553, 55554).

**SoluÃ§Ã£o final:** Sistema de rotaÃ§Ã£o temporal com time slots para scanning paralelo.

### Fase 1: Port-Forward Manager (Dual Port) âœ…
- âœ… PortForwardManager com 2 portas simultÃ¢neas (55553, 55554)
- âœ… Sistema de ocupaÃ§Ã£o (oddBusy/evenBusy flags)
- âœ… Auto-descoberta de Prometheus service (5 nomes comuns)
- âœ… Release de porta ao parar port-forward

### Fase 2: Baseline Collection System âœ…
- âœ… 3 dias (72h) de coleta histÃ³rica via Prometheus
- âœ… 16 mÃ©tricas coletadas (CPU, Memory, P95/P99, Throttling, OOM, etc.)
- âœ… ValidaÃ§Ã£o de cobertura (mÃ­nimo 70% de dados)
- âœ… SQLite persistence com `metrics_json` field
- âœ… Flag `baseline_ready` controla inÃ­cio do monitoring
- âœ… Coleta durante scan (usa port-forward ativo do TimeSlotManager)

### Fase 3: TimeSlotManager + Port Queue âœ…

**Arquitetura de Time Slots:**
```go
// internal/monitoring/engine/timeslot.go (NOVO)
type TimeSlotManager struct {
    clusters []string
    totalSlots int // (len(clusters) + 1) / 2
    slotDuration time.Duration // 30s (2 clusters), 20s (4), 15s (6+)
    currentSlot int
    slotStart time.Time
}

// Exemplo: 4 clusters â†’ 2 slots de 20s cada
// Slot 0 (0-20s):  cluster[0] (55553) + cluster[1] (55554)
// Slot 1 (20-40s): cluster[2] (55553) + cluster[3] (55554)
// Slot 0 (40-60s): repete...
```

**CorreÃ§Ã£o aplicada em `engine.go`:**
- âŒ **Removido**: Port-forwards persistentes no `Start()` (1 por cluster)
- âŒ **Removido**: scanLoop() que gerenciava scans sequenciais
- âŒ **Removido**: runScan() com cÃ³digo duplicado
- âœ… **Novo**: TimeSlotManager com rotaÃ§Ã£o circular
- âœ… **Novo**: timeSlotScanLoop() - Verifica slot atual a cada 2s
- âœ… **Novo**: executeSlotScan() - Executa 2 clusters em paralelo
- âœ… **Novo**: scanClusterInSlot() - Scan individual com port-forward temporÃ¡rio
- âœ… **Novo**: runScanForTarget() - LÃ³gica de scan extraÃ­da para reuso

**CÃ³digo key (engine.go):**
```go
// Start() - Inicializa TimeSlotManager
clusterNames := extractClusterNames(e.config.Targets)
e.timeSlotManager = NewTimeSlotManager(clusterNames)
log.Info().
    Int("clusters", len(clusterNames)).
    Int("slots", e.timeSlotManager.totalSlots).
    Dur("slot_duration", e.timeSlotManager.slotDuration).
    Msg("TimeSlotManager configurado")

go e.timeSlotScanLoop() // Loop de slots

// timeSlotScanLoop() - Verifica slot a cada 2s
ticker := time.NewTicker(2 * time.Second)
for {
    select {
    case <-ticker.C:
        assignment := e.timeSlotManager.GetCurrentAssignment()
        if assignment.SlotIndex != lastSlot {
            e.executeSlotScan(assignment)
            lastSlot = assignment.SlotIndex
        }
    }
}

// executeSlotScan() - 2 clusters em paralelo
var wg sync.WaitGroup
wg.Add(2)
go e.scanClusterInSlot(assignment.Port55553Cluster, 55553, &wg)
go e.scanClusterInSlot(assignment.Port55554Cluster, 55554, &wg)
wg.Wait()
```

### Fase 4: Dynamic Cluster Management âœ…

**AddTarget/RemoveTarget integrados:**
```go
// AddTarget() - Atualiza TimeSlotManager ao adicionar cluster
if e.running && e.timeSlotManager != nil {
    clusterNames := extractClusterNames(e.config.Targets)
    e.timeSlotManager.UpdateClusters(clusterNames)
    log.Info().
        Int("clusters", len(clusterNames)).
        Int("slots", e.timeSlotManager.totalSlots).
        Msg("TimeSlotManager atualizado apÃ³s adicionar cluster")
    
    // Baseline async (nÃ£o bloqueia)
    e.wg.Add(1)
    go e.collectHistoricalBaseline(target)
}

// RemoveTarget() - Recalcula slots apÃ³s remoÃ§Ã£o
if e.running && e.timeSlotManager != nil {
    clusterNames := extractClusterNames(e.config.Targets)
    e.timeSlotManager.UpdateClusters(clusterNames)
    log.Info().
        Int("clusters", len(clusterNames)).
        Int("slots", e.timeSlotManager.totalSlots).
        Msg("TimeSlotManager atualizado apÃ³s remover cluster")
}
```

**BenefÃ­cios da arquitetura final:**
- âœ… **Escalabilidade ilimitada**: Suporta 2, 4, 10, 100+ clusters
- âœ… **Uso eficiente de recursos**: Apenas 2 portas para N clusters
- âœ… **Scanning paralelo**: 2 clusters simultÃ¢neos por slot
- âœ… **RotaÃ§Ã£o justa**: Todos clusters escaneados em ciclos regulares
- âœ… **Port-forward temporÃ¡rio**: Criado/destruÃ­do por scan (nÃ£o persistente)
- âœ… **Baseline obrigatÃ³ria**: SÃ³ monitora apÃ³s 3 dias de coleta
- âœ… **DinÃ¢mico**: Adicionar/remover clusters recalcula slots automaticamente
- âœ… **Performance**: DuraÃ§Ã£o de slot adapta-se ao nÃºmero de clusters

**Arquivos criados:**
- `internal/monitoring/engine/timeslot.go` (NOVO - 220+ linhas)

**Arquivos refatorados:**
- `internal/monitoring/engine/engine.go` (1267 â†’ 1126 linhas apÃ³s cleanup)

**TODO (Fase 5 - Signal Handling):**
- â³ SIGINT/SIGTERM handlers para cleanup garantido
- â³ Graceful shutdown de port-forwards ativos
- â³ Flush de SQLite antes de terminar

---

### ğŸ”„ TODO: Fase 6 - BaselineQueue com Port-Forwards Dedicados (Novembro 2025) â³

**Data proposta:** 06 de novembro de 2025

**Problema atual:** Coleta de baseline de 3 dias (72h) entra em conflito com scans normais porque usa as mesmas portas (55553/55554) e port-forwards temporÃ¡rios sÃ£o destruÃ­dos antes da coleta terminar.

**SoluÃ§Ã£o proposta pelo usuÃ¡rio:**

> "crie mais 2 novos port-forwards para o baseline com a mesma logica dos scans dos clusters normais, e que serÃ£o criados no momento da demanda e destruidos depois que a fila ficar vazia. e cada scan da baseline deve acontecer uma vez a cada dia. se o intervalo de um scan for igual ou maior que 2 dias, entÃ£o um novo scan deve ser executado."

### **ğŸ“‹ Arquitetura:**

```
SCANS NORMAIS (mÃ©tricas em tempo real):
â”œâ”€ Porta 55553/55554
â”œâ”€ TimeSlotManager (rotaÃ§Ã£o 15-30s)
â”œâ”€ Scan rÃ¡pido (segundos)
â””â”€ Port-forward temporÃ¡rio por slot

BASELINE (coleta histÃ³rica 3 dias):
â”œâ”€ Porta 55555/55556 (NOVAS)
â”œâ”€ BaselineQueue (fila de HPAs pendentes)
â”œâ”€ Scan demorado (minutos - 72h de dados)
â”œâ”€ Port-forward criado sob demanda
â”œâ”€ Rescan 1x por dia (se Ãºltimo scan > 24h)
â””â”€ DestruÃ­do quando fila vazia
```

### **âœ… Vantagens:**

1. **Escalabilidade mantida**: Continua suportando 10+ clusters
2. **SeparaÃ§Ã£o de responsabilidades**: Scans normais nÃ£o bloqueiam baseline
3. **Sem conflito de portas**: 4 portas totais (2 para scans + 2 para baseline)
4. **EficiÃªncia de recursos**: Port-forwards de baseline criados sob demanda
5. **Dados sempre atualizados**: Rescan automÃ¡tico a cada 24h
6. **Baseline de 3 dias preservado**: Tempo suficiente para anÃ¡lise honesta

### **ğŸ”„ Fluxo completo:**

1. âœ… UsuÃ¡rio clica "Monitorar HPA"
2. âœ… HPA adicionado Ã  **BaselineQueue** (prioridade 0 - primeira coleta)
3. âœ… **BaselineWorker** detecta item na fila
4. âœ… Cria port-forward em 55555 ou 55556
5. âœ… Coleta baseline de 3 dias via Prometheus (range queries)
6. âœ… Salva mÃ©tricas no SQLite com timestamp
7. âœ… Marca HPA como `baseline_ready = true`
8. âœ… Remove HPA da fila
9. âœ… Se fila vazia â†’ destrÃ³i port-forward (libera recursos)
10. âœ… **VerificaÃ§Ã£o diÃ¡ria**: Se `last_baseline_scan > 24h` â†’ adiciona Ã  fila (prioridade 1)

### **âš™ï¸ Componentes a implementar:**

**1ï¸âƒ£ BaselineQueue** (`internal/monitoring/baseline/queue.go` - NOVO)
```go
type BaselineQueue struct {
    items []BaselineTask
    mu    sync.RWMutex
}

type BaselineTask struct {
    Cluster      string
    Namespace    string
    HPAName      string
    LastScan     time.Time
    Priority     int  // 0=primeira coleta, 1=rescan diÃ¡rio
    AddedAt      time.Time
}

// MÃ©todos:
// - Add(task) - Adiciona Ã  fila (evita duplicatas)
// - Pop() - Remove e retorna prÃ³ximo item (maior prioridade)
// - IsEmpty() - Verifica se fila estÃ¡ vazia
// - List() - Lista todos os itens (para debug/UI)
// - Remove(hpaKey) - Remove HPA especÃ­fico da fila
```

**2ï¸âƒ£ BaselineWorker** (`internal/monitoring/baseline/worker.go` - NOVO)
```go
type BaselineWorker struct {
    id           int        // 1 ou 2
    port         int        // 55555 ou 55556
    queue        *BaselineQueue
    pfManager    *PortForwardManager
    persistence  *storage.Persistence
    ctx          context.Context
    cancel       context.CancelFunc
    wg           sync.WaitGroup
}

// MÃ©todos:
// - Start() - Inicia worker em goroutine
// - Stop() - Para worker gracefully
// - processQueue() - Loop principal (busca itens da fila)
// - collectBaseline(task) - Coleta baseline de 3 dias
// - createPortForward() - Cria port-forward na porta dedicada
// - destroyPortForward() - DestrÃ³i port-forward
```

**3ï¸âƒ£ BaselineScheduler** (`internal/monitoring/baseline/scheduler.go` - NOVO)
```go
type BaselineScheduler struct {
    queue       *BaselineQueue
    persistence *storage.Persistence
    ticker      *time.Ticker
    ctx         context.Context
    cancel      context.CancelFunc
}

// MÃ©todos:
// - Start() - Inicia verificaÃ§Ã£o periÃ³dica (a cada 1 hora)
// - Stop() - Para scheduler
// - checkRescans() - Verifica HPAs com last_scan > 24h
// - addToQueue(hpaKey) - Adiciona HPA para rescan
```

**4ï¸âƒ£ IntegraÃ§Ã£o com PortForwardManager** (`internal/monitoring/portforward/portforward.go`)
```go
// Adicionar suporte para portas 55555 e 55556
const (
    PortScanOdd       = 55553  // Scans normais (cluster Ã­mpar)
    PortScanEven      = 55554  // Scans normais (cluster par)
    PortBaselineOdd   = 55555  // Baseline (worker 1)
    PortBaselineEven  = 55556  // Baseline (worker 2)
)

// MÃ©todo novo:
// - StartBaseline(cluster, port) - Cria port-forward para baseline
```

**5ï¸âƒ£ AtualizaÃ§Ã£o do ScanEngine** (`internal/monitoring/engine/engine.go`)
```go
type ScanEngine struct {
    // ... campos existentes ...

    // NOVO: Sistema de baseline
    baselineQueue     *baseline.BaselineQueue
    baselineWorker1   *baseline.BaselineWorker
    baselineWorker2   *baseline.BaselineWorker
    baselineScheduler *baseline.BaselineScheduler
}

// AlteraÃ§Ãµes:
// - Start() - Inicia workers de baseline e scheduler
// - Stop() - Para workers e scheduler gracefully
// - AddTarget() - Adiciona HPA Ã  BaselineQueue ao invÃ©s de coletar inline
```

**6ï¸âƒ£ Schema SQLite** (`internal/monitoring/storage/persistence.go`)
```sql
-- Adicionar campo last_baseline_scan
ALTER TABLE hpa_snapshots ADD COLUMN last_baseline_scan INTEGER; -- Unix timestamp

-- Index para busca rÃ¡pida de HPAs pendentes de rescan
CREATE INDEX idx_last_baseline_scan ON hpa_snapshots(last_baseline_scan);
```

### **ğŸ“Š Exemplo de execuÃ§Ã£o:**

```
T=0s:    UsuÃ¡rio adiciona 5 HPAs
         BaselineQueue = [HPA1(p0), HPA2(p0), HPA3(p0), HPA4(p0), HPA5(p0)]

T=1s:    Worker 1 (55555) â†’ port-forward cluster A â†’ coleta HPA1
         Worker 2 (55556) â†’ port-forward cluster B â†’ coleta HPA2

T=180s:  Worker 1 termina HPA1 (baseline_ready=true, last_scan=now)
         Worker 1 pega HPA3 â†’ port-forward cluster C

T=200s:  Worker 2 termina HPA2 (baseline_ready=true, last_scan=now)
         Worker 2 pega HPA4 â†’ port-forward cluster D

T=380s:  Worker 1 termina HPA3, pega HPA5 â†’ port-forward cluster E
T=400s:  Worker 2 termina HPA4, fila vazia â†’ destrÃ³i port-forward 55556

T=560s:  Worker 1 termina HPA5, fila vazia â†’ destrÃ³i port-forward 55555
         BaselineQueue = [] (vazia)

T=24h:   Scheduler detecta HPA1.last_scan > 24h
         BaselineQueue = [HPA1(p1)] (prioridade 1 = rescan)
         Worker 1 cria port-forward 55555 â†’ rescaneia HPA1

T=24h+3m: Worker 1 termina rescan, fila vazia â†’ destrÃ³i port-forward
```

### **ğŸ” DetecÃ§Ã£o de HPAs para rescan:**

```go
// BaselineScheduler.checkRescans() - roda a cada 1 hora
func (s *BaselineScheduler) checkRescans() {
    // Busca todos os HPAs do cache
    allSnapshots := s.persistence.GetAllHPAs()

    cutoff := time.Now().Add(-24 * time.Hour)

    for _, hpa := range allSnapshots {
        if hpa.BaselineReady && hpa.LastBaselineScan.Before(cutoff) {
            task := BaselineTask{
                Cluster:   hpa.Cluster,
                Namespace: hpa.Namespace,
                HPAName:   hpa.Name,
                LastScan:  hpa.LastBaselineScan,
                Priority:  1, // Rescan (menor prioridade que primeira coleta)
                AddedAt:   time.Now(),
            }
            s.queue.Add(task)

            log.Info().
                Str("hpa", hpa.Name).
                Time("last_scan", hpa.LastBaselineScan).
                Msg("HPA adicionado para rescan diÃ¡rio")
        }
    }
}
```

### **ğŸ“ Checklist de implementaÃ§Ã£o:**

- [ ] 1. Criar `internal/monitoring/baseline/queue.go` com BaselineQueue
- [ ] 2. Criar `internal/monitoring/baseline/worker.go` com BaselineWorker
- [ ] 3. Criar `internal/monitoring/baseline/scheduler.go` com BaselineScheduler
- [ ] 4. Atualizar PortForwardManager para suportar portas 55555/55556
- [ ] 5. Adicionar campo `last_baseline_scan` no schema SQLite
- [ ] 6. Integrar BaselineQueue/Workers/Scheduler no ScanEngine
- [ ] 7. Atualizar `AddTarget()` para adicionar Ã  fila ao invÃ©s de coletar inline
- [ ] 8. Remover lÃ³gica antiga de coleta de baseline sÃ­ncrona
- [ ] 9. Adicionar logs detalhados para debug (inÃ­cio/fim de coleta)
- [ ] 10. Testar com 10 HPAs de clusters diferentes
- [ ] 11. Testar rescan automÃ¡tico apÃ³s 24h
- [ ] 12. Testar destruiÃ§Ã£o de port-forwards quando fila vazia
- [ ] 13. Atualizar CLAUDE.md com documentaÃ§Ã£o final

### **ğŸ¯ Resultado esperado:**

- âœ… Scans normais continuam funcionando (15-30s por ciclo)
- âœ… Baseline de 3 dias coletado corretamente sem conflitos
- âœ… Port-forwards de baseline criados/destruÃ­dos sob demanda
- âœ… Rescan automÃ¡tico a cada 24h mantÃ©m dados atualizados
- âœ… Sistema escalÃ¡vel para 100+ clusters sem problemas
- âœ… MÃ©tricas aparecem na UI imediatamente apÃ³s baseline completar
- âœ… Nenhum "Sem dados disponÃ­veis" para HPAs em coleta

**Estimativa de implementaÃ§Ã£o:** 2-3 horas

---

### CorreÃ§Ã£o: AddTarget e Coleta de Baseline (Novembro 2025) âœ…

**Data:** 06 de novembro de 2025

**Problema identificado:** Ao adicionar novo HPA ao monitoramento, mensagem "Sem dados disponÃ­veis" aparecia mesmo com engine rodando e outros clusters coletando mÃ©tricas.

**Root Cause:**
1. `collectHistoricalBaselineAsync()` tentava criar port-forward prÃ³prio ao adicionar HPA
2. As 2 portas (55553/55554) jÃ¡ estavam ocupadas pelo TimeSlotManager
3. CriaÃ§Ã£o de port-forward falhava silenciosamente
4. Baseline nunca era coletado
5. HPA ficava sem dados indefinidamente

**CorreÃ§Ãµes aplicadas:**

**1ï¸âƒ£ Removida chamada de `collectHistoricalBaselineAsync()`** (`engine.go:273-281`)
```go
// ANTES (ERRADO - tentava criar port-forward prÃ³prio)
e.wg.Add(1)
go e.collectHistoricalBaselineAsync(target)

// DEPOIS (CORRETO - aguarda prÃ³ximo scan)
log.Info().Msg("Cluster adicionado - baseline serÃ¡ coletado no prÃ³ximo scan")
```

**2ï¸âƒ£ Melhorada funÃ§Ã£o `AddTarget()`** (`engine.go:234-308`)
```go
// ANTES: SubstituÃ­a lista de HPAs (perdia HPAs anteriores)
t.HPAs = target.HPAs

// DEPOIS: Mescla HPAs e namespaces (evita duplicatas)
hpaMap := make(map[string]bool)
for _, hpa := range t.HPAs { hpaMap[hpa] = true }
for _, hpa := range target.HPAs { hpaMap[hpa] = true }
t.HPAs = make([]string, 0, len(hpaMap))
for hpa := range hpaMap { t.HPAs = append(t.HPAs, hpa) }
```

**Fluxo corrigido:**
1. âœ… UsuÃ¡rio clica "Monitorar HPA" (qualquer cluster)
2. âœ… Frontend â†’ Backend â†’ `AddTarget()` mescla HPA Ã  lista
3. âœ… Se cluster novo: TimeSlotManager recalcula slots
4. âœ… TimeSlotManager escaneia cluster em seu slot (15-30s)
5. âœ… Durante scan: Port-forward temporÃ¡rio criado
6. âœ… `runScanForTarget()` detecta HPA sem baseline (linha 1072)
7. âœ… `collectBaselineForHPA()` coleta baseline usando port-forward ativo
8. âœ… HPA marcado como `baseline_ready`
9. âœ… Dados aparecem na interface web!

**Tempo atÃ© dados aparecerem:**
- Cluster existente: 15-30 segundos (prÃ³ximo slot)
- Cluster novo: 15-30 segundos (slot recalculado)

**Arquivos modificados:**
- `internal/monitoring/engine/engine.go`:
  - `AddTarget()` - Mescla de HPAs/namespaces + log claro
  - Removida chamada de `collectHistoricalBaselineAsync()`

**BenefÃ­cios:**
- âœ… Coleta de baseline funciona para qualquer cluster
- âœ… Sem conflito de portas (usa port-forward ativo do scan)
- âœ… EscalÃ¡vel para 100+ clusters
- âœ… HPAs anteriores nÃ£o sÃ£o perdidos ao adicionar novos

---

### EdiÃ§Ã£o Inline de Node Pools + CorreÃ§Ã£o Editor Staging (Novembro 2025) âœ…

**Data:** 03 de novembro de 2025

**Feature implementada:** Menu de ediÃ§Ã£o inline para Node Pools no modal "Confirmar AlteraÃ§Ãµes" (NodePoolApplyModal), idÃªntico ao jÃ¡ existente para HPAs.

**Problema anterior:**
- HPAs tinham menu â‹® com opÃ§Ãµes "Editar ConteÃºdo" e "Remover da Lista"
- Node Pools sÃ³ tinham botÃ£o "Aplicar" sem possibilidade de ediÃ§Ã£o inline
- Editor no StagingPanel fechava automaticamente apÃ³s salvar (tanto HPAs quanto Node Pools)

**SoluÃ§Ã£o implementada:**

**1ï¸âƒ£ Menu Dropdown com 3 pontos (â‹®)**
- Adicionado ao lado do botÃ£o "Aplicar" em cada Node Pool
- OpÃ§Ãµes disponÃ­veis:
  - **Editar ConteÃºdo**: Abre modal inline para ediÃ§Ã£o
  - **Remover da Lista**: Remove Node Pool da lista de alteraÃ§Ãµes

**2ï¸âƒ£ Modal de EdiÃ§Ã£o Inline**
- Checkbox "Autoscaling Habilitado"
- **Modo Manual**: Campo "Node Count"
- **Modo Autoscaling**: Campos "Min Nodes" e "Max Nodes"
- ValidaÃ§Ãµes:
  - Node Count â‰¥ 0
  - Min Nodes â‰¥ 0
  - Max Nodes â‰¥ Min Nodes
- BotÃµes "Cancelar" e "Salvar AlteraÃ§Ãµes"

**3ï¸âƒ£ FunÃ§Ãµes Implementadas**
```typescript
handleOpenEdit()        // Abre modal com valores atuais
handleSaveEdit()        // Valida e salva no staging
handleRemoveIndividual() // Remove do staging e adiciona ao removedKeys
```

**4ï¸âƒ£ CorreÃ§Ã£o: Editor nÃ£o fecha apÃ³s salvar**
- **Problema**: `onApplied` callback em `StagingPanel.tsx` executava `setSelectedItem(null)`
- **SoluÃ§Ã£o**: Removido callback `onApplied` de HPAEditor e NodePoolEditor (linhas 251 e 255)
- **Resultado**: Editor permanece aberto apÃ³s salvar, permitindo mÃºltiplas ediÃ§Ãµes sequenciais

**Arquivos modificados:**
- `internal/web/frontend/src/components/NodePoolApplyModal.tsx` (+93 linhas)
  - Imports: `DropdownMenu`, `MoreVertical`, `Edit`, `Input`, `Label`, `Checkbox`
  - Estados: `editingKey`, `editNodeCount`, `editMinNodes`, `editMaxNodes`, `editAutoscaling`, `removedKeys`, `refreshCounter`
  - Handlers: `handleOpenEdit()`, `handleSaveEdit()`, `handleRemoveIndividual()`
  - UI: DropdownMenu apÃ³s botÃ£o "Aplicar" + Modal de ediÃ§Ã£o inline
- `internal/web/frontend/src/components/StagingPanel.tsx` (-2 linhas)
  - Removido `onApplied={() => setSelectedItem(null)}` (HPAEditor e NodePoolEditor)

**BenefÃ­cios:**
- âœ… Paridade completa entre HPAs e Node Pools no ApplyAllModal
- âœ… EdiÃ§Ã£o inline sem sair do modal de confirmaÃ§Ã£o
- âœ… ValidaÃ§Ã£o de campos antes de salvar
- âœ… Editor permanece aberto para mÃºltiplas ediÃ§Ãµes
- âœ… UX consistente em toda aplicaÃ§Ã£o

---

### SimplificaÃ§Ã£o Load Session Modal + CorreÃ§Ã£o Scroll Staging (Novembro 2025) âœ…

**Data:** 02 de novembro de 2025

**Problemas identificados:**
1. BotÃ£o "Apply Directly (Recovery)" podia levar a erros de operaÃ§Ã£o
2. Scroll no painel de itens do Staging movia o painel do editor junto
3. PÃ¡gina ficava em branco ao clicar em "Carregar no Staging" apÃ³s remoÃ§Ã£o do Apply Directly

**SoluÃ§Ãµes implementadas:**

**1ï¸âƒ£ RemoÃ§Ã£o do "Apply Directly"**
- Removida funÃ§Ã£o `handleApplyDirectly()` completa (~260 linhas)
- Removidos estados: `selectedHPAs`, `selectedNodePools`, `applyingDirectly`, `currentProcessing`, `recoveryProgress`
- Removidos checkboxes de seleÃ§Ã£o granular de itens
- Removido botÃ£o "Apply Directly (Recovery)" do footer
- Removido progress indicator overlay
- Interface simplificada: Apenas visualizaÃ§Ã£o + "Carregar no Staging"

**2ï¸âƒ£ CorreÃ§Ã£o Scroll Independente**
- Removido `overflow-auto` e `p-4` do container da aba Staging em Index.tsx
- SplitView agora gerencia scroll independente para cada painel
- Scroll no painel esquerdo nÃ£o afeta painel direito

**3ï¸âƒ£ Bug Fix: PÃ¡gina em Branco**
- Root cause: Estados removidos ainda eram referenciados em `useEffect()`
- Removidos 2 `useEffect()` que tentavam usar estados inexistentes
- Limpeza completa de referÃªncias a `setSelectedHPAs`, `setSelectedNodePools`, etc.

**Arquivos modificados:**
- `internal/web/frontend/src/components/LoadSessionModal.tsx` (-290 linhas)
- `internal/web/frontend/src/pages/Index.tsx` (linha 355-356)

**BenefÃ­cios:**
- âœ… Interface mais simples e segura (sem Apply Directly)
- âœ… Scroll independente por painel (UX melhorada)
- âœ… CÃ³digo limpo sem estados Ã³rfÃ£os
- âœ… Bundle reduzido (~8KB menor)

---

### Redesign Completo: Staging Page (Novembro 2025) âœ…

**Data:** 02 de novembro de 2025

**Feature implementada:** Redesign completo da pÃ¡gina Staging para alinhar com o padrÃ£o visual das pÃ¡ginas CronJobs e Prometheus.

**Problema anterior:**
- Layout diferente das outras pÃ¡ginas (nÃ£o usava SplitView)
- Sem busca integrada
- EdiÃ§Ã£o em modais ao invÃ©s de painel inline
- InconsistÃªncia visual com resto da aplicaÃ§Ã£o

**SoluÃ§Ã£o implementada:**

**1ï¸âƒ£ SplitView Layout (2/5 + 3/5)**
- Painel esquerdo: Lista unificada de HPAs + Node Pools com busca
- Painel direito: Editor inline (HPAEditor/NodePoolEditor)
- PadrÃ£o consistente com CronJobs e Prometheus

**2ï¸âƒ£ Lista unificada com badges:**
```typescript
// Combinar HPAs e Node Pools em uma lista Ãºnica
const allItems = [
  ...staging.stagedHPAs.map(hpa => ({ type: 'hpa' as const, item: hpa })),
  ...staging.stagedNodePools.map(np => ({ type: 'nodepool' as const, item: np }))
];
```

**3ï¸âƒ£ Busca integrada:**
- Filtra por nome, namespace (HPA) ou cluster
- Case-insensitive
- Feedback visual quando nenhum item encontrado

**4ï¸âƒ£ UI compacta e consistente:**
- Cards clicÃ¡veis para seleÃ§Ã£o (border-primary quando selecionado)
- Badges visuais: HPA (azul) e Node Pool (verde)
- Badge "Modified" quando hÃ¡ alteraÃ§Ãµes
- Preview inline das mudanÃ§as (ex: "Min: 2 â†’ 5 | Max: 10 â†’ 12")
- BotÃ£o trash inline para remover item

**5ï¸âƒ£ Editor inline no painel direito:**
- Sem modais (ediÃ§Ã£o direta no painel)
- TÃ­tulo dinÃ¢mico mostra item selecionado
- Empty state quando nenhum item selecionado

**Arquivos modificados:**
- `internal/web/frontend/src/components/StagingPanel.tsx` - RefatoraÃ§Ã£o completa

**BenefÃ­cios:**
- âœ… UI 100% consistente com CronJobs e Prometheus
- âœ… Busca rÃ¡pida em listas longas (HPAs + Node Pools misturados)
- âœ… EdiÃ§Ã£o mais fluida (inline ao invÃ©s de modais)
- âœ… Workflow KISS (filosofia mantida)
- âœ… PadrÃ£o SplitView facilita futuras manutenÃ§Ãµes

---

### Sistema de Temp Staging para "Aplicar Agora" (Novembro 2025) âœ…

**Data:** 02 de novembro de 2025

**Problema identificado:** No fluxo "Aplicar Agora", quando o usuÃ¡rio editava valores no modal de confirmaÃ§Ã£o, as alteraÃ§Ãµes nÃ£o apareciam porque o sistema buscava valores do staging normal (que estava vazio para esse fluxo).

**Root Cause:**
- Fluxo "Aplicar Agora" passava valores diretamente via props para ApplyAllModal
- Quando usuÃ¡rio editava no modal, `handleSaveEdit()` salvava no staging normal via `updateHPAInStaging()`
- MAS o HPA nÃ£o existia no staging normal (apenas foi passado via props)
- `freshModifiedHPAs` nÃ£o encontrava o HPA no staging â†’ usava valores stale das props
- Resultado: EdiÃ§Ãµes no modal nÃ£o apareciam

**SoluÃ§Ã£o Implementada: "Temp Staging"**

Criado sistema de staging temporÃ¡rio exclusivo para fluxo "Aplicar Agora":

**1ï¸âƒ£ StagingContext** (`internal/web/frontend/src/contexts/StagingContext.tsx`):
- **Estado**: `tempHPA: { current: HPA; original: HPA } | null`
- **MÃ©todos**:
  - `setTempHPA(current, original)` - Salva HPA no temp staging
  - `updateTempHPA(updates)` - Atualiza valores (usado pela ediÃ§Ã£o no modal)
  - `clearTempHPA()` - Limpa temp staging (ao fechar modal)
  - `getTempHPA()` - Retorna HPA temporÃ¡rio

**2ï¸âƒ£ Index.tsx** (`internal/web/frontend/src/pages/Index.tsx`):
```typescript
const handleApplySingle = (current: HPA, original: HPA) => {
  // Salvar no temp staging para permitir ediÃ§Ã£o no modal
  staging?.setTempHPA(current, original);

  const key = `${current.cluster}/${current.namespace}/${current.name}`;
  setHpasToApply([{ key, current, original }]);
  setShowApplyModal(true);
};
```

**3ï¸âƒ£ ApplyAllModal** (`internal/web/frontend/src/components/ApplyAllModal.tsx`):

**a) freshModifiedHPAs - Busca do temp staging primeiro:**
```typescript
const freshModifiedHPAs = useMemo(() => {
  return modifiedHPAs.map(({ key, current, original }) => {
    // 1. Tentar buscar do temp staging (para "Aplicar Agora")
    const tempHPA = staging?.tempHPA;
    if (tempHPA && /* match cluster/namespace/name */) {
      return { key, current: tempHPA.current, original: tempHPA.original };
    }

    // 2. Tentar buscar do staging normal (para "Aplicar Todas")
    const freshHPA = staging?.stagedHPAs.find(/* ... */);
    return { key, current: freshHPA || current || original, original };
  });
}, [modifiedHPAs, staging?.stagedHPAs, staging?.tempHPA, refreshCounter]);
```

**b) handleSaveEdit - Detecta origem e atualiza corretamente:**
```typescript
const handleSaveEdit = () => {
  // ... validaÃ§Ãµes ...

  const isFromTempStaging = /* verifica se HPA estÃ¡ no tempHPA */;

  if (isFromTempStaging) {
    staging?.updateTempHPA(updates);  // Atualiza temp staging
    toast.success(`HPA ${name} atualizado (Aplicar Agora)`);
  } else {
    staging?.updateHPAInStaging(/* ... */, updates);  // Atualiza staging normal
    toast.success(`HPA ${name} atualizado no staging`);
  }

  setRefreshCounter(prev => prev + 1);  // Force refresh do useMemo
};
```

**c) useEffect - Limpa temp staging ao fechar modal:**
```typescript
useEffect(() => {
  if (!open) {
    staging?.clearTempHPA();
  }
}, [open, staging]);
```

**Fluxos apÃ³s correÃ§Ã£o:**

**Fluxo "Aplicar Agora":**
1. UsuÃ¡rio edita HPA â†’ Clica "Aplicar Agora"
2. `handleApplySingle()` salva no **temp staging**
3. ApplyAllModal abre â†’ `freshModifiedHPAs` busca do temp staging
4. âœ… Modal mostra alteraÃ§Ãµes (cluster â†’ editado)
5. UsuÃ¡rio edita no modal â†’ `updateTempHPA()` atualiza temp staging
6. `refreshCounter++` â†’ `useMemo` re-executa â†’ busca valores atualizados
7. âœ… Modal reflete ediÃ§Ãµes (cluster â†’ editado â†’ editado no modal)
8. Modal fecha â†’ `clearTempHPA()` limpa

**Fluxo "Aplicar Todas"** (inalterado):
1. UsuÃ¡rio adiciona HPAs ao staging normal
2. `freshModifiedHPAs` busca do staging normal
3. EdiÃ§Ãµes no modal atualizam staging normal
4. âœ… Funciona como antes

**Arquivos modificados:**
- `internal/web/frontend/src/contexts/StagingContext.tsx` (+40 linhas)
  - Interface `StagingContextType` com mÃ©todos temp staging
  - Estado `tempHPA` e funÃ§Ãµes (`setTempHPA`, `updateTempHPA`, etc)
  - Adicionado ao `value` do Provider

- `internal/web/frontend/src/pages/Index.tsx` (+3 linhas)
  - `handleApplySingle()` chama `staging.setTempHPA()`

- `internal/web/frontend/src/components/ApplyAllModal.tsx` (+50 linhas, -10 linhas)
  - `freshModifiedHPAs`: Busca temp staging primeiro
  - `handleSaveEdit()`: Detecta origem e usa mÃ©todo correto
  - `useEffect`: Limpa temp staging ao fechar modal
  - Import `useEffect`

**BenefÃ­cios:**
- âœ… EdiÃ§Ãµes no modal "Aplicar Agora" agora funcionam corretamente
- âœ… SeparaÃ§Ã£o clara entre fluxos "Aplicar Agora" e "Aplicar Todas"
- âœ… Staging normal preservado para aplicaÃ§Ãµes em lote
- âœ… Limpeza automÃ¡tica de temp staging ao fechar modal
- âœ… Toasts informativos indicam qual staging foi atualizado

---

### CorreÃ§Ã£o: ApplyAllModal NÃ£o Atualiza ApÃ³s EdiÃ§Ã£o (Novembro 2025) âœ…

**Data:** 02 de novembro de 2025

**Problema identificado:** Valores editados no modal "Confirmar AlteraÃ§Ãµes" nÃ£o refrescavam para mostrar as alteraÃ§Ãµes mais recentes.

**Root Cause:**
- ApplyAllModal usava `modifiedHPAs` (dados stale do prop) ao invÃ©s de `freshModifiedHPAs` (dados frescos do staging)
- `freshModifiedHPAs` Ã© derivado do staging via `useMemo` e sincroniza com mudanÃ§as em tempo real
- TrÃªs locais crÃ­ticos estavam usando dados stale:
  1. Linha 148: `hpaToEdit` busca HPA para ediÃ§Ã£o inline
  2. Linha 228: `handleApplyAll` itera sobre HPAs para aplicar
  3. Linha 542: Nome do HPA no modal de ediÃ§Ã£o

**SoluÃ§Ã£o implementada:**

**Arquivo**: `internal/web/frontend/src/components/ApplyAllModal.tsx`

```typescript
// Linha 148 - Modal de ediÃ§Ã£o inline
// âŒ ANTES:
const hpaToEdit = modifiedHPAs.find(({ key }) => key === editingKey);
// âœ… DEPOIS:
const hpaToEdit = freshModifiedHPAs.find(({ key }) => key === editingKey);

// Linha 228 - AplicaÃ§Ã£o em lote
// âŒ ANTES:
for (const { key, current } of modifiedHPAs) {
// âœ… DEPOIS:
for (const { key, current } of freshModifiedHPAs) {

// Linha 542 - Nome no modal de ediÃ§Ã£o
// âŒ ANTES:
{modifiedHPAs.find(({ key }) => key === editingKey)?.current.name}
// âœ… DEPOIS:
{freshModifiedHPAs.find(({ key }) => key === editingKey)?.current.name}
```

**Contexto tÃ©cnico:**
```typescript
// freshModifiedHPAs sincroniza com staging em tempo real
const freshModifiedHPAs = useMemo(() => {
  return modifiedHPAs.map(({ key, original }) => {
    const freshHPA = staging?.stagedHPAs.find(
      h => h.cluster === original.cluster &&
           h.namespace === original.namespace &&
           h.name === original.name
    );
    return {
      key,
      current: freshHPA || original, // Sempre pega valor ATUAL do staging
      original
    };
  });
}, [modifiedHPAs, staging?.stagedHPAs, refreshCounter]);
```

**BenefÃ­cios:**
- âœ… EdiÃ§Ãµes inline refletem imediatamente na lista
- âœ… Valores aplicados sÃ£o sempre os mais recentes
- âœ… Preview de alteraÃ§Ãµes 100% preciso
- âœ… ConsistÃªncia entre modal de ediÃ§Ã£o e visualizaÃ§Ã£o

---

### CorreÃ§Ã£o: "Nenhuma mudanÃ§a visÃ­vel" ApÃ³s Editar Valores (Novembro 2025) âœ…

**Data:** 02 de novembro de 2025

**Problema identificado:** Ao editar um HPA no modal inline (ex: Min Replicas 2 â†’ 5) e salvar, a mensagem "Nenhuma mudanÃ§a visÃ­vel (valores idÃªnticos)" ainda aparecia.

**Root Cause:**

**Arquivo**: `internal/web/frontend/src/pages/Index.tsx` (linha 405)

O objeto `original` estava sendo criado incorretamente, misturando valores atuais com valores originais:

```typescript
// âŒ ANTES (ERRADO):
original: { ...hpa, ...hpa.originalValues } as HPA,
```

**O que causava o bug:**

1. `{ ...hpa, ...hpa.originalValues }` cria um objeto:
   - Primeiro: Copia TODOS os campos de `hpa` (valores ATUAIS modificados)
   - Depois: Sobrescreve apenas com campos que existem em `hpa.originalValues`

2. `originalValues` Ã© um objeto **parcial**, nÃ£o contÃ©m todos os campos

3. Resultado: `original` ficava com mix de valores atuais + alguns valores originais

4. Exemplo prÃ¡tico:
   ```typescript
   // Estado quando vocÃª edita Min Replicas: 2 â†’ 5
   hpa.originalValues = { min_replicas: 2, max_replicas: 10, target_cpu: 80 }
   hpa (atual) = { min_replicas: 5, max_replicas: 10, target_cpu: 80, target_memory: 90 }

   // Com { ...hpa, ...hpa.originalValues }:
   original = {
     min_replicas: 2,        // De originalValues âœ…
     max_replicas: 10,       // De originalValues âœ…
     target_cpu: 80,         // De originalValues âœ…
     target_memory: 90,      // De hpa (ATUAL) âŒ BUG!
     // ... outros campos de hpa (atual)
   }

   // ComparaÃ§Ã£o current vs original:
   // - min_replicas: 5 vs 2 â†’ Mostra diferenÃ§a âœ…
   // - target_memory: 90 vs 90 â†’ NÃƒO mostra diferenÃ§a âŒ (ambos iguais!)
   ```

5. `renderChange()` retorna `null` para campos iguais, array `changes` ficava vazio â†’ mensagem "Nenhuma mudanÃ§a visÃ­vel"

**SoluÃ§Ã£o implementada:**

```typescript
// âœ… DEPOIS (CORRETO):
original: hpa.originalValues as HPA,
```

Agora `original` contÃ©m **APENAS** os valores originais puros salvos no staging, sem contaminaÃ§Ã£o de valores atuais.

**BenefÃ­cios:**
- âœ… ComparaÃ§Ã£o precisa entre valores originais e modificados
- âœ… Todas as ediÃ§Ãµes aparecem corretamente no preview de mudanÃ§as
- âœ… Mensagem "Nenhuma mudanÃ§a visÃ­vel" sÃ³ aparece quando realmente nÃ£o hÃ¡ mudanÃ§as
- âœ… Diff completo e preciso para todas as alteraÃ§Ãµes

---

### CorreÃ§Ã£o: History Tracker com Campos Vazios (Novembro 2025) âœ…

**Data:** 02 de novembro de 2025

**Problema identificado:** History Tracker salvava campos de recursos vazios (`cpu_request`, `memory_request`, `cpu_limit`, `memory_limit`) impossibilitando comparaÃ§Ã£o completa "Antes vs Depois".

**Root Cause:**
- Handler `hpas.go` usava campos **errados** para capturar recursos do deployment
- âŒ **Antes**: Usava `Current*` fields (mÃ©tricas de uso real - ainda nÃ£o implementadas)
- âœ… **CorreÃ§Ã£o**: Usar `Target*` fields (configuraÃ§Ã£o do deployment - implementados em `EnrichHPAWithDeploymentResources`)

**ExplicaÃ§Ã£o tÃ©cnica:**
```go
// internal/kubernetes/client.go (linha 1168-1223)
func EnrichHPAWithDeploymentResources(ctx context.Context, hpa *models.HPA) error {
    // Preenche Target* fields com configuraÃ§Ã£o do deployment
    hpa.TargetCPURequest = cpuReq.String()      // âœ… ConfiguraÃ§Ã£o real
    hpa.TargetMemoryRequest = memReq.String()   // âœ… ConfiguraÃ§Ã£o real
    // ...

    // Current* fields sÃ£o para mÃ©tricas de USO REAL (TODO via Metrics Server)
    // hpa.CurrentCPURequest = ...  // âŒ Ainda nÃ£o implementado
}
```

**SoluÃ§Ã£o implementada:**

**Arquivo**: `internal/web/handlers/hpas.go`

**1ï¸âƒ£ Estado ANTES da alteraÃ§Ã£o (linha 232-246):**
```go
// ANTES (ERRADO)
beforeState = map[string]interface{}{
    "cpu_request":     beforeHPA.CurrentCPURequest,    // âŒ Vazio
    "memory_request":  beforeHPA.CurrentMemoryRequest, // âŒ Vazio
    "cpu_limit":       beforeHPA.CurrentCPULimit,      // âŒ Vazio
    "memory_limit":    beforeHPA.CurrentMemoryLimit,   // âŒ Vazio
}

// DEPOIS (CORRETO)
beforeState = map[string]interface{}{
    "cpu_request":     beforeHPA.TargetCPURequest,     // âœ… Configurado
    "memory_request":  beforeHPA.TargetMemoryRequest,  // âœ… Configurado
    "cpu_limit":       beforeHPA.TargetCPULimit,       // âœ… Configurado
    "memory_limit":    beforeHPA.TargetMemoryLimit,    // âœ… Configurado
}
```

**2ï¸âƒ£ Estado DEPOIS da alteraÃ§Ã£o (linha 289-299):**
```go
// ANTES (ERRADO)
afterState = map[string]interface{}{
    "cpu_request":    updatedHPA.CurrentCPURequest,    // âŒ Vazio
    "memory_request": updatedHPA.CurrentMemoryRequest, // âŒ Vazio
    "cpu_limit":      updatedHPA.CurrentCPULimit,      // âŒ Vazio
    "memory_limit":   updatedHPA.CurrentMemoryLimit,   // âŒ Vazio
}

// DEPOIS (CORRETO)
afterState = map[string]interface{}{
    "cpu_request":    updatedHPA.TargetCPURequest,     // âœ… Configurado
    "memory_request": updatedHPA.TargetMemoryRequest,  // âœ… Configurado
    "cpu_limit":      updatedHPA.TargetCPULimit,       // âœ… Configurado
    "memory_limit":   updatedHPA.TargetMemoryLimit,    // âœ… Configurado
}
```

**Fluxo de dados corrigido:**
1. `GetHPA()` busca HPA do Kubernetes (linha 233)
2. `EnrichHPAWithDeploymentResources()` preenche `Target*` com recursos do deployment (linha 284)
3. Captura BEFORE state com `Target*` fields (linha 236-245)
4. `UpdateHPA()` aplica mudanÃ§as no HPA e deployment (linha 253)
5. `GetHPA()` busca HPA atualizado (linha 279)
6. Captura AFTER state com `Target*` fields (linha 290-299)
7. `historyTracker.Log()` salva comparaÃ§Ã£o completa (linha 302-313)

**Resultado:**
```json
// ANTES (campos vazios)
{
  "cpu_limit": "",
  "cpu_request": "",
  "memory_limit": "",
  "memory_request": ""
}

// DEPOIS (campos preenchidos)
{
  "cpu_limit": "2",
  "cpu_request": "500m",
  "memory_limit": "4Gi",
  "memory_request": "2Gi"
}
```

**Arquivos modificados:**
- `internal/web/handlers/hpas.go` (linhas 241-244, 295-298)

**BenefÃ­cios:**
- âœ… History Viewer mostra comparaÃ§Ã£o completa "Antes vs Depois"
- âœ… Rastreabilidade completa de mudanÃ§as de recursos
- âœ… Compliance e auditoria melhorados
- âœ… Troubleshooting facilitado com histÃ³rico detalhado

---

### Redesign Completo: CronJobs e Prometheus Pages (Novembro 2025) âœ…

**Data:** 01 de novembro de 2025

**Feature implementada:** Redesign completo das pÃ¡ginas de CronJobs e Prometheus Stack para alinhar com o padrÃ£o visual das pÃ¡ginas de HPAs e Node Pools.

**Problema anterior:**
- Layout desalinhado com resto da aplicaÃ§Ã£o
- Controles dispersos e pouco intuitivos
- Sem busca integrada
- Estado nÃ£o atualizava em tempo real apÃ³s alteraÃ§Ãµes

**SoluÃ§Ã£o implementada:**

**1ï¸âƒ£ SplitView Layout (2/5 + 3/5)**
- Painel esquerdo: Lista de recursos com busca
- Painel direito: Editor com formulÃ¡rios de ediÃ§Ã£o
- PadrÃ£o consistente com HPAs e Node Pools

**2ï¸âƒ£ Componentes criados:**
```typescript
// Lista compacta com badges de status
CronJobListItem.tsx
PrometheusListItem.tsx

// Editores com aplicaÃ§Ã£o direta (sem staging)
CronJobEditor.tsx    â†’ Suspend/Resume compacto (grid 2 botÃµes)
PrometheusEditor.tsx â†’ EdiÃ§Ã£o de recursos + Rollout
```

**3ï¸âƒ£ Auto-refresh apÃ³s alteraÃ§Ãµes:**
```typescript
// Pattern implementado em ambas as pÃ¡ginas
React.useEffect(() => {
  if (selectedItem && items.length > 0) {
    const updated = items.find(item => item.name === selectedItem.name);
    if (updated) setSelectedItem(updated);
  }
}, [items]);
```

**4ï¸âƒ£ UI compacta e intuitiva:**
- **CronJobEditor**: 2 botÃµes lado a lado (Ativar/Suspender)
  - Variant styling mostra estado ativo
  - BotÃ£o disabled quando jÃ¡ no estado desejado
- **PrometheusEditor**: Rollout movido para topo direito (seguro)
  - BotÃ£o "Editar Recursos" expande formulÃ¡rio inline
  - Salvamento direto no cluster (sem staging)
  - BotÃ£o Cancelar apenas no modo de ediÃ§Ã£o

**5ï¸âƒ£ Busca integrada:**
- CronJobs: Busca por nome e namespace
- Prometheus: Busca por nome, namespace e componente

**Arquivos criados:**
- `internal/web/frontend/src/components/CronJobListItem.tsx`
- `internal/web/frontend/src/components/PrometheusListItem.tsx`
- `internal/web/frontend/src/components/CronJobEditor.tsx`
- `internal/web/frontend/src/components/PrometheusEditor.tsx`

**Arquivos refatorados:**
- `internal/web/frontend/src/pages/CronJobsPage.tsx`
- `internal/web/frontend/src/pages/PrometheusPage.tsx`

**Build artifacts:**
- Frontend: `index-Ds3wDSKs.js` (628.21 kB)

**BenefÃ­cios:**
- âœ… UI consistente em toda a aplicaÃ§Ã£o
- âœ… Busca rÃ¡pida em listas longas
- âœ… Feedback visual imediato apÃ³s alteraÃ§Ãµes
- âœ… Controles compactos e seguros
- âœ… Salvamento direto no cluster (CronJobs e Prometheus nÃ£o usam staging)

---

### CorreÃ§Ã£o CrÃ­tica: MÃ©tricas de Dashboard + Gauge de Dois AnÃ©is (Novembro 2025) âœ…

**Data:** 01 de novembro de 2025

**Problema identificado:** MÃ©tricas de CPU e memÃ³ria no dashboard mostravam valores **diferentes** do K9s (diferenÃ§a de ~11% em memÃ³ria).

**Root Cause:**
- Backend usava `node.Status.Capacity` para cÃ¡lculo de percentuais
- K9s e `kubectl top` usam `node.Status.Allocatable`
- **Capacity** = Total de hardware (ex: 8 GB RAM)
- **Allocatable** = Capacity - Reservas do sistema (ex: 6.1 GB = 76% do total)
- Reservas: kubelet, OS, eviction threshold (~24% em memÃ³ria, ~4% em CPU)

**CorreÃ§Ã£o aplicada:**

**1ï¸âƒ£ Backend - CÃ¡lculo correto:**
```go
// ANTES (ERRADO)
if memory := node.Status.Capacity.Memory(); memory != nil {
    totalMemoryCapacity += memory.Value()
}

// DEPOIS (CORRETO)
if memory := node.Status.Allocatable.Memory(); memory != nil {
    totalMemoryAllocatable += memory.Value()
}
```

**2ï¸âƒ£ Backend - Novos campos de mÃ©tricas:**
```go
type ClusterMetrics struct {
    CPUUsagePercent       float64 // % de uso vs Allocatable
    MemoryUsagePercent    float64 // % de uso vs Allocatable
    CPUCapacityPercent    float64 // % de Allocatable vs Capacity (novo)
    MemoryCapacityPercent float64 // % de Allocatable vs Capacity (novo)
}
```

**3ï¸âƒ£ Frontend - Gauge de dois anÃ©is concÃªntricos:**
- **Anel externo (Capacity):**
  - ğŸŸ¦ Azul: Allocatable (ex: 76% da memÃ³ria total)
  - âš« Cinza: System Reserved (ex: 24% reservado para OS/kubelet)
- **Anel interno (Usage):**
  - ğŸŸ¢/ğŸŸ¡/ğŸ”´ Verde/Amarelo/Vermelho: Uso real (ex: 48.5% do allocatable)

**4ï¸âƒ£ Frontend - Legenda educativa:**
```
âœ“ Allocatable:       76.1%  (disponÃ­vel para pods)
âœ“ System Reserved:   23.9%  (kubelet, OS, eviction)
âœ“ Current Usage:     48.5%  (uso real)
```

**Resultados:**

**Antes:**
```
K9s:       CPU 19%,  Memory 48%
Dashboard: CPU 19.5%, Memory 36.9%  âŒ 11% de diferenÃ§a!
```

**Depois:**
```
K9s:       CPU 19%,  Memory 48%
Dashboard: CPU 19.7%, Memory 48.5%  âœ… <1% de diferenÃ§a (timing)
```

**BenefÃ­cios:**
- âœ… MÃ©tricas agora **100% precisas** (idÃªnticas ao K9s)
- âœ… VisualizaÃ§Ã£o **educativa** do overhead do sistema
- âœ… DiagnÃ³stico facilitado de clusters com overhead alto
- âœ… TransparÃªncia total sobre uso de recursos

**Arquivos modificados:**
- `internal/config/kubeconfig.go` - CÃ¡lculo de Allocatable vs Capacity
- `internal/web/handlers/clusters.go` - Novos campos na API
- `internal/web/frontend/src/lib/api/types.ts` - Tipos TypeScript
- `internal/web/frontend/src/components/MetricsGauge.tsx` - Gauge de dois anÃ©is
- `internal/web/frontend/src/components/DashboardCharts.tsx` - Layout otimizado

---

### Feature: Combobox de Busca de Clusters no Header (Outubro 2025) âœ…

**Data:** 31 de outubro de 2025

**Feature implementada:** Combobox com busca integrada para seleÃ§Ã£o de clusters no header da interface web.

**Problema anterior:**
- Select dropdown simples sem busca
- UsuÃ¡rio tinha que rolar lista completa de clusters (70+ clusters)
- DifÃ­cil encontrar cluster especÃ­fico rapidamente

**SoluÃ§Ã£o implementada:**
- âœ… **Combobox completo** usando componentes shadcn/ui (Command + Popover)
- âœ… **Busca integrada** - Campo de busca dentro do dropdown
- âœ… **Filtragem em tempo real** - CommandInput filtra automaticamente
- âœ… **Keyboard navigation** - Setas, Enter, Esc funcionam nativamente
- âœ… **Check visual** - Ãcone âœ“ mostra cluster selecionado
- âœ… **Auto-close** - Dropdown fecha automaticamente apÃ³s seleÃ§Ã£o
- âœ… **Acessibilidade** - role="combobox" e ARIA attributes corretos

**Componentes utilizados:**
```typescript
<Popover>
  <PopoverTrigger>
    <Button role="combobox">
      {selectedCluster || "Selecione ou busque um cluster..."}
      <ChevronsUpDown />
    </Button>
  </PopoverTrigger>
  <PopoverContent>
    <Command>
      <CommandInput placeholder="Buscar cluster..." />
      <CommandList>
        <CommandEmpty>Nenhum cluster encontrado.</CommandEmpty>
        <CommandGroup>
          {clusters.map((cluster) => (
            <CommandItem onSelect={handleSelect}>
              <Check /> {cluster}
            </CommandItem>
          ))}
        </CommandGroup>
      </CommandList>
    </Command>
  </PopoverContent>
</Popover>
```

**Arquivos modificados:**
- `Header.tsx` - SubstituÃ­do Select por Combobox completo
- Removido `ClusterSelectorForTab.tsx` modificaÃ§Ãµes (nÃ£o Ã© usado no header)

**BenefÃ­cios:**
- âœ… **Busca rÃ¡pida**: Digite parte do nome e encontre instantaneamente
- âœ… **UX melhorada**: Um componente unificado ao invÃ©s de dois separados
- âœ… **EscalÃ¡vel**: Funciona perfeitamente com 70+ clusters
- âœ… **Keyboard-friendly**: NavegaÃ§Ã£o completa via teclado
- âœ… **Feedback visual**: Check mark no item selecionado

**Exemplos de uso:**
- Digite "hlg" â†’ Filtra todos os clusters de homologaÃ§Ã£o
- Digite "faturamento" â†’ Mostra `akspriv-faturamento-hlg-admin`
- Setas â†‘â†“ â†’ Navega entre clusters filtrados
- Enter â†’ Seleciona e fecha dropdown
- Esc â†’ Fecha sem selecionar

---

### CorreÃ§Ã£o CrÃ­tica: Input Fields e Modal Auto-Update (Outubro 2025) âœ…

**Data:** 31 de outubro de 2025

**Problema 1 identificado:** Campos de input numÃ©ricos na interface web nÃ£o podiam ser limpos completamente, sempre retinham pelo menos um dÃ­gito.

**CenÃ¡rio que causava bug:**
- UsuÃ¡rio tenta deletar valor "4" â†’ Campo deveria ficar vazio â†’ Digita "25" â†’ Deveria mostrar "25"
- **Comportamento errado**: Delete "4" â†’ Campo mostra "1" â†’ Digita "25" â†’ Campo mostra "125"

**SoluÃ§Ã£o aplicada:**
1. **MudanÃ§a de tipo de input**: `type="number"` â†’ `type="text"` com validaÃ§Ã£o regex `/^\d+$/`
2. **Estados de string**: Mudado de `number` â†’ `string` para permitir campo vazio
3. **Removido onBlur**: Handler que restaurava valores default foi removido
4. **UX melhorada**: Adicionado `select()` em `onClick` e `onFocus` para selecionar todo texto

**Arquivos modificados:**
- `HPAEditor.tsx` - Campos Min/Max Replicas, Target CPU/Memory, Resources
- `NodePoolEditor.tsx` - Campos Node Count, Min/Max Nodes

---

**Problema 2 identificado:** Modal de confirmaÃ§Ã£o (ApplyAllModal) nÃ£o refletia alteraÃ§Ãµes feitas no editor inline, exigindo fechar e reabrir o modal para ver mudanÃ§as.

**CenÃ¡rio que causava bug:**
1. Carregar sessÃ£o no staging
2. Abrir modal de confirmaÃ§Ã£o
3. Clicar "Editar ConteÃºdo" (â‹® menu)
4. Alterar valores (ex: Max Replicas 11 â†’ 10)
5. Salvar
6. **Bug**: Modal nÃ£o atualizava, usuÃ¡rio tinha que fechar e reabrir

**Root Cause:**
- Modal renderizava dados da **prop** `modifiedHPAs` (fixa e imutÃ¡vel)
- Staging era atualizado corretamente, mas React nÃ£o detectava mudanÃ§a
- `refreshCounter` existia mas nÃ£o forÃ§ava re-render dos dados

**SoluÃ§Ã£o aplicada:**
1. **Criado `freshModifiedHPAs` com `useMemo`**: Deriva dados frescos do staging a cada render
2. **SubstituÃ­do `modifiedHPAs` por `freshModifiedHPAs`**: Modal agora renderiza dados dinÃ¢micos
3. **`refreshCounter` nas dependÃªncias do useMemo**: ForÃ§a recÃ¡lculo quando incrementado

**CÃ³digo implementado:**
```typescript
// Deriva dados frescos do staging
const freshModifiedHPAs = useMemo(() => {
  return modifiedHPAs.map(({ key, original }) => {
    const freshHPA = staging?.stagedHPAs.find(
      h => h.cluster === original.cluster &&
           h.namespace === original.namespace &&
           h.name === original.name
    );

    return {
      key,
      current: freshHPA || original, // Dados frescos do staging
      original
    };
  });
}, [modifiedHPAs, staging?.stagedHPAs, refreshCounter]);

// Renderiza usando dados frescos
{freshModifiedHPAs.map(...)}
```

**Arquivos modificados:**
- `ApplyAllModal.tsx` - Import useMemo, freshModifiedHPAs, rendering atualizado

**Workflow completo agora:**
1. UsuÃ¡rio edita HPA no modal "Editar ConteÃºdo"
2. Salva â†’ `staging.updateHPAInStaging()` atualiza dados
3. `setRefreshCounter(prev => prev + 1)` incrementa contador
4. `useMemo` detecta mudanÃ§a e busca dados frescos do staging
5. React detecta mudanÃ§a em `freshModifiedHPAs`
6. **Modal atualiza automaticamente** sem fechar/reabrir

**BenefÃ­cios:**
- âœ… Input fields podem ser limpos completamente (ex: "4" â†’ "" â†’ "25" = "25")
- âœ… Modal reflete alteraÃ§Ãµes instantaneamente apÃ³s ediÃ§Ã£o
- âœ… Workflow mais fluido sem passos desnecessÃ¡rios
- âœ… Dados sempre sincronizados com staging

---

### Melhorias no Sistema de Recovery (Snapshot) - Outubro 2025 âœ…

**Data:** 29 de outubro de 2025

**Problema identificado:** Sistema de recovery (Apply Directly) nÃ£o validava cluster, nÃ£o mostrava progresso individual e nÃ£o tinha resumo final de estatÃ­sticas.

**Melhorias implementadas:**

**1ï¸âƒ£ ValidaÃ§Ã£o de Cluster AutomÃ¡tica**
- Detecta clusters dos itens selecionados
- Valida se hÃ¡ apenas 1 cluster (recovery multi-cluster nÃ£o suportado)
- Troca contexto Kubernetes automaticamente (`cluster-admin`)
- Configura subscription Azure se necessÃ¡rio
- Exibe mensagem de erro clara se VPN desconectada

**2ï¸âƒ£ Feedback de Progresso Individual**
- Progress bar visual durante execuÃ§Ã£o
- Contador de progresso: `[3/10] Restaurando HPA: namespace/name...`
- EstatÃ­sticas em tempo real: `âœ… 5 OK | âŒ 2 Erros`
- Estado visual atualizado dinamicamente

**3ï¸âƒ£ Resumo Final com EstatÃ­sticas**
- Toast notification com resumo completo:
  - âœ… **100% sucesso**: `Recovery 100% concluÃ­do: 10 itens restaurados`
  - âš ï¸ **Parcial**: `Recovery parcial: 8 OK, 2 falhas | Itens falhados: HPA: ns/name1, Node Pool: pool2`
  - âŒ **Falha total**: `Recovery falhou: 10 erros | Verifique conectividade e logs`
- Logs detalhados no console (`[Recovery] âœ… HPA restaurado (3/5): namespace/name`)
- Modal fecha automaticamente apÃ³s 2s se houver sucesso

**4ï¸âƒ£ Tratamento de Erros Robusto**
- Continua execuÃ§Ã£o mesmo com erros individuais
- Lista de itens falhados para troubleshooting
- Previne fechamento de modal se todos os itens falharem
- Mensagens de erro especÃ­ficas (VPN, cluster nÃ£o encontrado, timeout)

**Arquivos modificados:**
- `internal/web/frontend/src/components/LoadSessionModal.tsx`:
  - Estados de progresso: `currentProcessing`, `recoveryProgress`
  - FunÃ§Ã£o `handleApplyDirectly()` reescrita (linhas 260-519)
  - Progress bar visual (linhas 1104-1140)
- Build: Frontend v1.2.7-dirty (assets atualizados)

**Workflow completo:**
```
1. UsuÃ¡rio seleciona sessÃ£o de rollback
2. Marca/desmarca HPAs e Node Pools (checkboxes)
3. Clica "Apply Directly (Recovery)"
4. Sistema valida cluster e troca contexto
5. Progress bar mostra progresso individual
6. EstatÃ­sticas em tempo real (OK/Erros)
7. Resumo final com toast notification
8. Modal fecha automaticamente (se sucesso)
```

**BenefÃ­cios:**
- âœ… Recovery mais confiÃ¡vel com validaÃ§Ã£o de cluster
- âœ… Visibilidade completa do progresso
- âœ… Troubleshooting facilitado com logs e lista de falhas
- âœ… UX melhorada com feedback em tempo real
- âœ… PrevenÃ§Ã£o de erros (multi-cluster, VPN desconectada)

---

### CorreÃ§Ã£o de Assets NÃ£o Embeddados - go:embed (Outubro 2025) âœ…

**Release:** v1.2.6 (28 de outubro de 2025)
**Commit:** 0f05463

**Problema identificado:** Webpage em branco em qualquer computador apÃ³s instalaÃ§Ã£o da release.

**Root Cause:**
- `go:embed` **APENAS** embeda arquivos versionados no Git
- `internal/web/static/*` estava no `.gitignore`
- GitHub Actions gerava os arquivos, mas `go:embed` nÃ£o os encontrava
- Resultado: BinÃ¡rio compilado sem assets embeddados â†’ webpage em branco

**SoluÃ§Ã£o:**
1. âœ… Removido `internal/web/static/*` do `.gitignore`
2. âœ… Commitados arquivos de build no repositÃ³rio:
   - `internal/web/static/assets/index-CW0HINYd.css` (76 KB)
   - `internal/web/static/assets/index-QahD77AR.js` (577 KB)
   - `internal/web/static/index.html`, `favicon.ico`
3. âœ… Release v1.2.6 criada com assets embeddados

**ValidaÃ§Ã£o:**
```bash
curl http://localhost:8080/assets/index-QahD77AR.js  # âœ… 200 OK (590.689 bytes)
curl http://localhost:8080/assets/index-CW0HINYd.css # âœ… 200 OK (76 KB)
```

**LiÃ§Ã£o aprendida:**
- `go:embed` requer arquivos commitados no Git
- Arquivos gerados em build-time devem ser versionados **OU** copiados para local nÃ£o-ignorado
- Usar `all:` prefix para incluir subdiretÃ³rios (`//go:embed all:static`)

---

### CorreÃ§Ã£o web-server.sh - DetecÃ§Ã£o de Porta Real (Outubro 2025) âœ…

**Problema identificado:** Comando `status` sempre mostrava porta 8080, mesmo quando servidor rodava em porta diferente.

**SoluÃ§Ã£o:**
- Script agora extrai porta real do processo em execuÃ§Ã£o via `ps aux`
- Usa regex para encontrar flag `--port` na linha de comando
- Fallback para 8080 se nÃ£o encontrar porta especificada

**Testes:**
```bash
./web-server.sh 9000 start  # Inicia na porta 9000
./web-server.sh status      # âœ… Mostra "ğŸ“ URL: http://localhost:9000"
```

**Arquivo modificado:** `web-server.sh` (linhas 114-140)

---

### CorreÃ§Ã£o de Cross-Compilation para Windows/macOS (Outubro 2025) âœ…

**Commit:** b84461c (27 de outubro de 2025)

**Problema identificado:** Build multi-plataforma falhava durante `make release` com erro de compilaÃ§Ã£o.

**Erro:**
```
Error: cmd/root.go:239:59: undefined: unix.TCGETS
```

**Causa:**
- FunÃ§Ã£o `isatty()` nÃ£o utilizada no cÃ³digo usava `unix.IoctlGetTermios()` e `unix.TCGETS`
- `golang.org/x/sys/unix` Ã© especÃ­fico do Linux/Unix
- Cross-compilation para Windows e macOS falhava no GitHub Actions

**SoluÃ§Ã£o:**
- âŒ Removido import `golang.org/x/sys/unix`
- âŒ Removida funÃ§Ã£o `isatty()` nÃ£o utilizada (cÃ³digo morto)
- âœ… CÃ³digo agora Ã© cross-platform compatÃ­vel

**Nota tÃ©cnica:** O projeto jÃ¡ possui `github.com/mattn/go-isatty` como dependÃªncia (via Gin framework), que Ã© cross-platform. Se precisar verificar TTY no futuro, usar essa biblioteca ao invÃ©s de `unix.IoctlGetTermios()`.

**Testes realizados:**
- âœ… `make release` compila para todas as plataformas:
  - Linux amd64:        82M âœ“
  - macOS amd64 (Intel): 82M âœ“
  - macOS arm64 (Apple): 80M âœ“
  - Windows amd64:       82M âœ“

**Arquivos modificados:**
- `cmd/root.go` (-7 linhas)
  - Removido import `golang.org/x/sys/unix`
  - Removida funÃ§Ã£o `isatty()` (linhas 237-241)

**Impacto:**
- âœ… GitHub Actions CI/CD agora compila binÃ¡rios para todas as plataformas
- âœ… Releases automatizadas funcionando corretamente
- âœ… Sem perda de funcionalidade (cÃ³digo removido nÃ£o era usado)

---

### Sistema de Log Viewer para Interface Web (Outubro 2025) âœ…

**Feature:** Sistema completo de visualizaÃ§Ã£o de logs com captura em tempo real, auto-refresh, exportaÃ§Ã£o CSV e limpeza.

**ImplementaÃ§Ã£o:**
- **Backend** (`internal/web/handlers/logs.go`):
  - `LogBuffer` - Buffer circular thread-safe (RWMutex) com 1000 logs em memÃ³ria
  - `LogsHandler` - Handler com mÃ©todos `GetLogs()` e `ClearLogs()`
  - MÃºltiplas fontes de logs:
    - Buffer em memÃ³ria (logs da aplicaÃ§Ã£o)
    - Arquivos de log (`/tmp/k8s-hpa-manager-web-*.log`)
    - Sistema (journalctl - opcional, comentado)

- **Middleware de Logging** (`internal/web/server.go`):
  - `loggingMiddleware()` - Captura TODAS as requisiÃ§Ãµes HTTP
  - Formato: `[timestamp] METHOD path | Status: XXX | Latency: XXXms`
  - Filtro inteligente: Ignora `/health` e `/heartbeat` para nÃ£o poluir logs
  - Thread-safe com acesso protegido ao buffer

- **Frontend** (`internal/web/frontend/src/components/LogViewer.tsx`):
  - Modal responsivo (max-w-6xl, h-85vh)
  - **Auto-refresh** - Toggle on/off, atualiza a cada 3 segundos
  - **Copiar** - Copia logs para clipboard
  - **Exportar CSV** - Parsing inteligente de logs estruturados
  - **Limpar** - Limpa buffer com confirmaÃ§Ã£o
  - **EstatÃ­sticas** - Badges de total/errors/warnings/info

- **IntegraÃ§Ã£o no Header** (`internal/web/frontend/src/components/Header.tsx`):
  - BotÃ£o discreto com Ã­cone ğŸ“„ (FileText)
  - Tooltip "View System Logs"

**API Routes:**
- `GET /api/v1/logs` - Buscar logs (buffer + arquivos)
- `DELETE /api/v1/logs` - Limpar buffer

**Workflow:**
1. UsuÃ¡rio clica no Ã­cone ğŸ“„ no header
2. Modal abre com logs divididos por fonte:
   - **Application Logs (In-Memory)** - RequisiÃ§Ãµes HTTP capturadas
   - **Web Server Logs** - Logs do arquivo do servidor
3. Auto-refresh mantÃ©m logs atualizados automaticamente
4. Exportar CSV para anÃ¡lise offline
5. Limpar buffer quando necessÃ¡rio

**Testes realizados:**
- âœ… Captura de requisiÃ§Ãµes HTTP em tempo real
- âœ… Auto-refresh funcionando (3s)
- âœ… Copiar para clipboard
- âœ… Exportar CSV com parsing correto
- âœ… Limpar buffer com confirmaÃ§Ã£o
- âœ… EstatÃ­sticas de logs (total, errors, warnings)
- âœ… Thread-safe (RWMutex)

**Arquivos criados:**
- `internal/web/handlers/logs.go` (NOVO)
- `internal/web/frontend/src/components/LogViewer.tsx` (NOVO)

**Arquivos modificados:**
- `internal/web/server.go` - Middleware + rotas de logs
- `internal/web/frontend/src/components/Header.tsx` - BotÃ£o de logs
- `internal/web/frontend/src/pages/Index.tsx` - IntegraÃ§Ã£o do modal

**BenefÃ­cios:**
- âœ… Debugging facilitado com logs em tempo real
- âœ… InvestigaÃ§Ã£o de erros sem acesso ao servidor
- âœ… ExportaÃ§Ã£o para anÃ¡lise offline (CSV)
- âœ… Auto-refresh elimina necessidade de recarregar manualmente
- âœ… Filtros inteligentes (ignora health/heartbeat)

---

### CorreÃ§Ã£o CrÃ­tica: Sistema de Heartbeat/Auto-Shutdown (Outubro 2025) âœ…

**Commit:** 7e38820 (24 de outubro de 2025)

**Problema identificado:** Servidor web desligava prematuramente mesmo com heartbeats sendo enviados.

**Bug 1: Race Condition no Timer**
- **Problema:** O `shutdownTimer` nÃ£o tinha proteÃ§Ã£o mutex, permitindo race conditions entre mÃºltiplos heartbeats simultÃ¢neos ou durante o disparo do timer
- **SoluÃ§Ã£o:** Adicionado `timerMutex sync.Mutex` na struct Server para proteger todas as operaÃ§Ãµes de Stop() e AfterFunc()
- **Impacto:** Previne desligamentos inesperados durante operaÃ§Ãµes concorrentes

**Bug 2: Timer Inicial Prematuro**
- **Problema:** Timer de 20 minutos comeÃ§ava a contar imediatamente quando servidor iniciava, NÃƒO quando frontend conectava
- **CenÃ¡rio que causava o bug:**
  1. Servidor inicia Ã s 14:15 (cria timer para 14:35)
  2. Frontend envia primeiro heartbeat Ã s 14:25 (cria novo timer para 14:45)
  3. Heartbeats subsequentes em 14:30, 14:35...
  4. **MAS**: Timer original das 14:35 ainda estava ativo e disparava!
- **SoluÃ§Ã£o:** Timer inicial aumentado para 30 minutos (tempo de graÃ§a), primeiro heartbeat do frontend reseta para 20 minutos normais
- **Impacto:** Garante que servidor nÃ£o desligue antes do frontend conectar

**Melhorias de Logging:**
```
ğŸ’“ Heartbeat recebido: 15:44:49 | PrÃ³ximo shutdown em: 16:04:49
```
- Log detalhado em cada heartbeat mostrando timestamp recebido e prÃ³ximo shutdown
- Mensagem clara sobre timer inicial de 30 minutos
- Facilita debugging e monitoramento do sistema

**Testes realizados:**
- âœ… MÃºltiplos heartbeats recebidos e processados corretamente
- âœ… Timer resetado a cada heartbeat (verificado via logs)
- âœ… Servidor permanece ativo com pÃ¡gina aberta
- âœ… MÃºltiplas abas abertas simultaneamente (cada uma envia heartbeat)

**Arquivos modificados:**
- `internal/web/server.go` (+18 linhas, -4 linhas)
  - Adicionado `timerMutex sync.Mutex`
  - Protegido todas as operaÃ§Ãµes no timer com mutex
  - Timer inicial aumentado de 20min â†’ 30min
  - Log detalhado em cada heartbeat

**Impacto:** Sistema de auto-shutdown agora funciona corretamente sem desligar prematuramente.

---

### Campo de Busca e EdiÃ§Ã£o Inline na Interface Web (Outubro 2025) âœ…

**Release:** v1.2.1 (publicada em 24 de outubro de 2025)
**GitHub:** https://github.com/Paulo-Ribeiro-Log/Scale_HPA/releases/tag/v1.2.1

**Features:** Campo de busca inteligente, ediÃ§Ã£o inline de HPAs, e correÃ§Ãµes crÃ­ticas de estabilidade.

**ImplementaÃ§Ã£o:**
- **Campo de Busca Inteligente**:
  - Campo de busca no painel "Available HPAs" (busca por nome e namespace)
  - Campo de busca no painel "Available Node Pools" (busca por nome e cluster)
  - Interface consistente com Ã­cone de lupa
  - Busca case-insensitive em tempo real
  - Feedback visual quando nenhum item Ã© encontrado

- **Modal de EdiÃ§Ã£o Inline (ApplyAllModal)**:
  - EdiÃ§Ã£o completa de HPAs sem sair do modal de confirmaÃ§Ã£o
  - Dropdown menu (â‹®) com opÃ§Ãµes "Editar ConteÃºdo" e "Remover da Lista"
  - ValidaÃ§Ã£o de campos (Min/Max Replicas, Target CPU/Memory 1-100%)
  - Suporte a ediÃ§Ã£o de recursos (CPU/Memory Request/Limit)
  - Checkboxes de rollout (Deployment, DaemonSet, StatefulSet)
  - AtualizaÃ§Ã£o em staging apÃ³s ediÃ§Ã£o

- **CorreÃ§Ãµes de Bugs CrÃ­ticos**:
  - Remove `window.location.reload()` que causava restart da pÃ¡gina
  - Implementa sistema de eventos customizados (`rescanNodePools`)
  - Adiciona listener no hook `useNodePools` para refetch automÃ¡tico
  - Previne perda de dados durante operaÃ§Ãµes de Node Pools
  - MantÃ©m estado e contexto durante operaÃ§Ãµes longas

**Arquivos modificados:**
- `internal/web/frontend/src/pages/Index.tsx` (+129 linhas)
- `internal/web/frontend/src/hooks/useAPI.ts` (+32 linhas)
- `internal/web/frontend/src/components/ApplyAllModal.tsx` (+355 linhas)
- `internal/web/static/` (rebuild frontend)

**BenefÃ­cios:**
- âœ… Produtividade aumentada com busca rÃ¡pida (70+ HPAs/Node Pools)
- âœ… CorreÃ§Ã£o de erros sem interromper fluxo de trabalho
- âœ… Estabilidade em operaÃ§Ãµes longas (sem restart)
- âœ… ExperiÃªncia de usuÃ¡rio consistente e previsÃ­vel

---

### Sistema Completo de InstalaÃ§Ã£o e Updates (Outubro 2025) âœ…

**Release:** v1.2.0 (publicada em 23 de outubro de 2025)
**GitHub:** https://github.com/Paulo-Ribeiro-Log/Scale_HPA/releases/tag/v1.2.0

**Feature:** Scripts automatizados de instalaÃ§Ã£o, atualizaÃ§Ã£o e gerenciamento.

**ImplementaÃ§Ã£o:**
- **install-from-github.sh** - Instalador completo:
  - Clona repositÃ³rio automaticamente
  - Verifica requisitos (Go, Git, kubectl, Azure CLI)
  - Compila com injeÃ§Ã£o de versÃ£o via git tags
  - Instala em `/usr/local/bin/k8s-hpa-manager`
  - Copia scripts utilitÃ¡rios para `~/.k8s-hpa-manager/scripts/`
  - Cria atalho `k8s-hpa-web` para servidor web
  - Testa instalaÃ§Ã£o automaticamente

- **auto-update.sh** - Sistema de atualizaÃ§Ã£o automÃ¡tica:
  - `--yes` / `-y` - Auto-confirmaÃ§Ã£o (para scripts/cron)
  - `--dry-run` / `-d` - Modo simulaÃ§Ã£o (testes)
  - `--check` / `-c` - Apenas verificar status
  - `--force` / `-f` - ForÃ§ar reinstalaÃ§Ã£o
  - VerificaÃ§Ã£o automÃ¡tica 1x por dia (TUI startup)
  - NotificaÃ§Ã£o no StatusContainer (TUI) ou comando `version`
  - Cache em `~/.k8s-hpa-manager/.update-check` (24h TTL)

- **Sistema de versionamento**:
  - VersÃ£o injetada via `-ldflags` durante build
  - DetecÃ§Ã£o automÃ¡tica via `git describe --tags`
  - ComparaÃ§Ã£o semÃ¢ntica (MAJOR.MINOR.PATCH)
  - VerificaÃ§Ã£o via GitHub API (`/repos/.../releases/latest`)
  - Suporte a GitHub token (rate limiting)

**Testes realizados (v1.2.0):**
- âœ… DetecÃ§Ã£o de updates (1.1.0 â†’ 1.2.0)
- âœ… Comando `version` com preview de release notes
- âœ… Auto-update `--dry-run` (simulaÃ§Ã£o sem alteraÃ§Ãµes)
- âœ… Auto-update `--check` (status e versÃ£o disponÃ­vel)
- âœ… Auto-update `--yes` (auto-confirmaÃ§Ã£o)
- âœ… Cache de verificaÃ§Ã£o (24h TTL)
- âœ… Link de download correto
- âœ… BinÃ¡rio instalado em `/usr/local/bin/`

**Arquivos criados:**
- `install-from-github.sh` - Instalador completo
- `auto-update.sh` - Script de auto-update com flags
- `INSTALL_GUIDE.md` - Guia completo de instalaÃ§Ã£o
- `QUICK_INSTALL.md` - InstalaÃ§Ã£o rÃ¡pida
- `UPDATE_BEHAVIOR.md` - DocumentaÃ§Ã£o do sistema de updates
- `AUTO_UPDATE_EXAMPLES.md` - Exemplos de uso (cron, scripts, CI/CD)
- `INSTRUCTIONS_RELEASE.md` - Como publicar releases
- `create_release.sh` - Script de criaÃ§Ã£o de releases

**Workflow de uso:**
```bash
# InstalaÃ§Ã£o
curl -fsSL https://raw.githubusercontent.com/.../install-from-github.sh | bash

# Verificar updates
k8s-hpa-manager version

# Auto-update interativo
~/.k8s-hpa-manager/scripts/auto-update.sh

# Auto-update automÃ¡tico (cron)
~/.k8s-hpa-manager/scripts/auto-update.sh --yes

# Simular antes de aplicar
~/.k8s-hpa-manager/scripts/auto-update.sh --dry-run
```

**Scripts utilitÃ¡rios copiados:**
- `web-server.sh` - Gerenciar servidor web (com atalho `k8s-hpa-web`)
- `uninstall.sh` - Desinstalar aplicaÃ§Ã£o
- `auto-update.sh` - Auto-update com flags `--yes` e `--dry-run`
- `backup.sh` / `restore.sh` - Backup/restore para desenvolvimento
- `rebuild-web.sh` - Rebuild interface web

**BenefÃ­cios:**
- âœ… InstalaÃ§Ã£o em 1 comando (clone + build + install)
- âœ… Updates automÃ¡ticos com notificaÃ§Ã£o
- âœ… Versionamento semÃ¢ntico via Git tags
- âœ… Scripts utilitÃ¡rios sempre disponÃ­veis
- âœ… FÃ¡cil gerenciamento do servidor web
- âœ… Auto-update seguro com confirmaÃ§Ã£o (ou `--yes` para automaÃ§Ã£o)
- âœ… Dry-run para testes antes de aplicar
- âœ… DesinstalaÃ§Ã£o limpa e simples

**Arquivos modificados:**
- `cmd/root.go` - Flags `--check-updates`, funÃ§Ã£o `checkForUpdatesAsync()`
- `cmd/version.go` - Comando `version` com verificaÃ§Ã£o de updates
- `internal/updater/` (NOVO) - Sistema completo de versionamento
  - `version.go` - VersÃ£o injetada via ldflags, comparaÃ§Ã£o semÃ¢ntica
  - `github.go` - Cliente GitHub API para releases
  - `checker.go` - LÃ³gica de verificaÃ§Ã£o (cache 24h)
- `internal/tui/app.go` - NotificaÃ§Ã£o no StatusContainer (apÃ³s 3s)
- `makefile` - LDFLAGS com injeÃ§Ã£o de versÃ£o, targets `version` e `release`
- `README.md` - SeÃ§Ã£o de instalaÃ§Ã£o e updates atualizada
- `CLAUDE.md` - DocumentaÃ§Ã£o atualizada com instalaÃ§Ã£o e updates

### Rollout Individual para Prometheus Stack (Outubro 2025) âœ…

**Feature:** BotÃµes individuais de rollout para cada recurso do Prometheus Stack (Deployment/StatefulSet/DaemonSet).

**ImplementaÃ§Ã£o:**
- **Backend**:
  - FunÃ§Ãµes genÃ©ricas de rollout em `internal/kubernetes/client.go`:
    - `RolloutDeployment()` (jÃ¡ existia)
    - `RolloutStatefulSet()` (NOVO - linhas 1368-1389)
    - `RolloutDaemonSet()` (NOVO - linhas 1391-1412)
  - Handler `Rollout()` em `internal/web/handlers/prometheus.go` (linhas 506-562)
  - Rota API: `POST /api/v1/prometheus/:cluster/:namespace/:type/:name/rollout`

- **Frontend**:
  - BotÃ£o "Rollout" individual para cada recurso no card
  - Estado de loading com spinner durante execuÃ§Ã£o
  - Auto-refresh da lista apÃ³s 2 segundos
  - Toast notifications de sucesso/erro

**Workflow:**
1. UsuÃ¡rio acessa pÃ¡gina "Prometheus"
2. Cada card tem botÃµes "Rollout" e "Editar"
3. Click em "Rollout" adiciona annotation `kubectl.kubernetes.io/restartedAt` com timestamp
4. Pods do recurso sÃ£o reiniciados (rolling restart)

**Arquivos modificados:**
- `internal/kubernetes/client.go` - FunÃ§Ãµes de rollout genÃ©ricas
- `internal/web/handlers/prometheus.go` - Handler Rollout()
- `internal/web/server.go` - Rota POST rollout
- `internal/web/frontend/src/pages/PrometheusPage.tsx` - UI com botÃµes

### Aplicar Agora para Node Pools (Outubro 2025) âœ…

**Feature:** BotÃ£o "Aplicar Agora" no Node Pool Editor que aplica alteraÃ§Ãµes diretamente no cluster sem passar pelo staging.

**ImplementaÃ§Ã£o:**
- BotÃ£o verde "âœ… Aplicar Agora" ao lado de "ğŸ’¾ Salvar (Staging)"
- Layout idÃªntico ao HPA Editor (3 botÃµes na mesma linha)
- Estado de loading com spinner ("Aplicando...")
- Logs detalhados no console (before â†’ after)
- Toast notifications de sucesso/erro
- Chama diretamente `apiClient.updateNodePool()` para aplicaÃ§Ã£o imediata

**DiferenÃ§a entre botÃµes:**
- **ğŸ’¾ Salvar (Staging)**: Adiciona ao staging para aplicar em lote depois
- **âœ… Aplicar Agora**: Aplica imediatamente no cluster (Azure API)
- **Cancelar**: Volta aos valores originais

**Workflow:**
1. UsuÃ¡rio seleciona Node Pool â†’ Editor abre
2. Modifica valores (Node Count, Autoscaling, Min/Max)
3. Clica "Aplicar Agora"
4. API chama Azure CLI para update
5. Toast de sucesso/erro
6. Editor reseta para novo estado

**Arquivos modificados:**
- `internal/web/frontend/src/components/NodePoolEditor.tsx`:
  - Import: `Loader2`, `Zap`, `apiClient`, `toast`
  - Estado: `isApplying`
  - FunÃ§Ã£o: `handleApplyNow()` (linhas 110-162)
  - UI: Layout de botÃµes reorganizado (linhas 368-406)

**CorreÃ§Ã£o de Layout:**
- Removido `sticky bottom-0` que causava efeito flutuante
- Removido `p-4 overflow-y-auto h-full` do container
- Container simples `space-y-4` como no HPAEditor
- BotÃµes fixados no flow normal do documento

### Race Condition em Testes de Cluster (Outubro 2025) âœ…

**Problema:** Goroutines concorrentes causavam race condition ao testar conexÃµes com mÃºltiplos clusters simultaneamente.

**SoluÃ§Ã£o:**
- Adicionado `sync.RWMutex` em `KubeConfigManager`
- Double-check locking pattern para performance
- Read lock para leituras, write lock para criaÃ§Ã£o

**Arquivos modificados:**
- `internal/config/kubeconfig.go`

### Azure CLI Warnings como Erros (Outubro 2025) âœ…

**Problema:** Warnings do Azure CLI (`pkg_resources deprecated`) eram tratados como erros fatais.

**SoluÃ§Ã£o:**
- SeparaÃ§Ã£o stdout/stderr em `executeAzureCommand()`
- Lista de warnings conhecidos (ignorados)
- ValidaÃ§Ã£o inteligente via `isOnlyWarnings()`

**Arquivos modificados:**
- `internal/tui/app.go:3535-3683`

### Node Pool Sequence Logic (Outubro 2025) âœ…

**Problema:** Azure CLI nÃ£o permite `scale` com autoscaling habilitado - aplicaÃ§Ã£o tentava scale ANTES de desabilitar.

**SoluÃ§Ã£o:**
- 4 cenÃ¡rios detectados automaticamente:
  1. AUTO â†’ MANUAL: Disable autoscaling â†’ Scale
  2. MANUAL â†’ AUTO: Scale â†’ Enable autoscaling
  3. AUTO â†’ AUTO: Update min/max
  4. MANUAL â†’ MANUAL: Scale direto

**Arquivos modificados:**
- `internal/tui/app.go:3433-3545`

### Cluster Name Mismatch (Outubro 2025) âœ…

**Problema:** Node pools nÃ£o carregavam porque `findClusterInConfig()` nÃ£o fazia match correto entre nomes com/sem `-admin` suffix.

**SoluÃ§Ã£o:**
- Remove `-admin` suffix para comparaÃ§Ã£o
- Fallback para match exato (backward compatibility)

**Arquivos modificados:**
- `internal/web/handlers/nodepools.go:256-282`

### Web Interface Tela Branca (Outubro 2025) âœ…

**Problema:** NodePoolEditor e HPAEditor causavam tela branca porque mÃ©todos do StagingContext nÃ£o existiam.

**SoluÃ§Ã£o:**
- Corrigir chamadas para mÃ©todos existentes:
  - `staging.addHPAToStaging()` ao invÃ©s de `staging.add()`
  - `staging.stagedNodePools.find()` ao invÃ©s de `staging.getNodePool()`

**Arquivos modificados:**
- `internal/web/frontend/src/components/NodePoolEditor.tsx`
- `internal/web/frontend/src/components/HPAEditor.tsx`

### Sistema de Heartbeat e Auto-Shutdown (Outubro 2025) âœ…

**Funcionalidade NOVA:** Servidor web desliga automaticamente apÃ³s 20 minutos de inatividade.

**ImplementaÃ§Ã£o:**
- Frontend: `useHeartbeat` hook envia POST `/heartbeat` a cada 5 minutos
- Backend: Timer de 20 minutos resetado a cada heartbeat
- Thread-safe: `sync.RWMutex` protege timestamp

**Arquivos modificados:**
- `internal/web/server.go` - Monitor de inatividade
- `internal/web/frontend/src/hooks/useHeartbeat.ts` - Hook React

### Snapshot de Cluster para Rollback (Outubro 2025) âœ…

**Funcionalidade NOVA:** Captura estado atual do cluster (TODOS os HPAs + Node Pools) para rollback.

**ImplementaÃ§Ã£o:**
- `fetchClusterDataForSnapshot()` busca dados FRESCOS via API (nÃ£o usa cache)
- Salva como sessÃ£o com original_values = new_values
- IntegraÃ§Ã£o com TabManager para cluster selection

**Arquivos modificados:**
- `internal/web/frontend/src/components/SaveSessionModal.tsx`
- `internal/web/frontend/src/pages/Index.tsx` - SincronizaÃ§Ã£o TabManager

### Session Management (Rename/Edit/Delete) (Outubro 2025) âœ…

**Funcionalidade NOVA:** UI completa para gerenciamento de sessÃµes salvas.

**ImplementaÃ§Ã£o:**
- Dropdown menu (â‹®) em cada sessÃ£o
- Modais de confirmaÃ§Ã£o (delete) e ediÃ§Ã£o (rename)
- EditSessionModal para editar conteÃºdo (HPAs/Node Pools)

**Arquivos modificados:**
- `internal/web/frontend/src/components/LoadSessionModal.tsx`
- `internal/web/frontend/src/components/EditSessionModal.tsx` (NOVO)
- `internal/web/handlers/sessions.go` - Endpoint rename e update

---

**Happy coding!** ğŸš€
- "NÃ£o faÃ§a over-enginnering"
