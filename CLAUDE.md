# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**IMPORTANTE**: Responda sempre em portuguÃªs brasileiro (pt-br).
**IMPORTANTE**: Mantenha o foco na filosofia KISS.
**IMPORTANTE**: Sempre compile o build em ./build/ - usar `./build/k8s-hpa-manager` para executar a aplicaÃ§Ã£o.
**IMPORTANTE**: Interface **totalmente responsiva** - adapta-se a qualquer tamanho de terminal (recomendado: 80x24 ou maior).

---

## ğŸ“‘ Ãndice / Table of Contents

1. [Quick Start](#-quick-start-para-novos-chats)
2. [Development Commands](#-development-commands)
3. [Architecture Overview](#-architecture-overview)
4. [Recent Features (2025)](#-melhorias-recentes-2025)
5. [User Interface & Controls](#-user-interface--controls)
6. [Troubleshooting](#-troubleshooting)
7. [Future Development](#-continuing-development)

---

## ğŸš€ Quick Start Para Novos Chats

### Project Summary
**Terminal-based Kubernetes HPA and Azure AKS Node Pool management tool** built with Go and Bubble Tea TUI framework. Features async rollout progress tracking with Rich Python-style progress bars, integrated status panel, session management, and unified HPA/node pool operations.

### Estado Atual (Outubro 2025)
- âœ… **Interface Responsiva** - adapta-se ao tamanho real do terminal (sem forÃ§ar 188x45)
- âœ… **Otimizada para ProduÃ§Ã£o** - texto legÃ­vel, painÃ©is compactos (60x12), operaÃ§Ã£o segura
- âœ… **Layout completo** com execuÃ§Ã£o sequencial de node pools para stress tests
- âœ… **Rollouts detalhados** de HPA (Deployment/DaemonSet/StatefulSet)
- âœ… **CronJob management** completo (F9)
- âœ… **Prometheus Stack Management** (F8) com mÃ©tricas reais
- âœ… **Status container** compacto (80x10) com progress bars Rich Python
- âœ… **Auto-descoberta de clusters** via `k8s-hpa-manager autodiscover`
- âœ… **ValidaÃ§Ã£o VPN on-demand** - verifica conectividade K8s antes de operaÃ§Ãµes crÃ­ticas
- âœ… **ValidaÃ§Ã£o Azure com timeout** - nÃ£o trava em problemas de DNS/rede
- âœ… **Modais de confirmaÃ§Ã£o** - Ctrl+D/Ctrl+U exigem confirmaÃ§Ã£o antes de aplicar
- âœ… **Modais como overlay** - aparecem sobre o conteÃºdo sem esconder a aplicaÃ§Ã£o
- âœ… **Log detalhado de alteraÃ§Ãµes** - todas as mudanÃ§as exibidas no StatusContainer (antes â†’ depois)
- âœ… **NavegaÃ§Ã£o sequencial de abas** - Ctrl+â†/â†’ para navegar entre abas com wrap-around
- âœ… **Versionamento automÃ¡tico** - via git tags com verificaÃ§Ã£o de updates 1x/dia
- âœ… **Sistema de Logs Completo** (F3) - visualizador com scroll, copiar, limpar logs
- âœ… **NavegaÃ§Ã£o ESC corrigida** - Node Pools voltam para Namespaces (origem do Ctrl+N)
- âœ… **Race condition corrigida** - Mutex RWLock para testes paralelos de cluster (thread-safe)
- âœ… **Interface Web POC (99% completa)** - HPAs, Node Pools, CronJobs e Prometheus Stack implementados com ediÃ§Ã£o funcional + Dashboard redesignado com layout moderno grid 2x2 e mÃ©tricas reais (ver Docs/README_WEB.md)

### Tech Stack
- **Language**: Go 1.23+ (toolchain 1.24.7)
- **TUI Framework**: Bubble Tea + Lipgloss
- **K8s Client**: client-go (official)
- **Azure SDK**: azcore, azidentity, armcontainerservice
- **Architecture**: MVC pattern com state-driven UI

---

## ğŸ”§ Development Commands

### Terminal Requirements

**âœ… Interface Totalmente Responsiva**

A aplicaÃ§Ã£o usa **EXATAMENTE o tamanho do seu terminal** - sem forÃ§ar dimensÃµes artificiais:

- **Adapta-se ao terminal**: Usa suas dimensÃµes reais (ex: 80x24, 120x30, etc)
- **Texto legÃ­vel**: NÃ£o precisa zoom out - mantenha Ctrl+0 (tamanho normal)
- **Otimizada para produÃ§Ã£o**: Layout compacto, operaÃ§Ã£o segura sem erros visuais
- **Sem limites artificiais**: Removido forÃ§amento de 188x45 que causava texto minÃºsculo

**Como funciona:**
1. AplicaÃ§Ã£o detecta tamanho real do terminal
2. Ajusta painÃ©is automaticamente (60x12 base)
3. Status panel compacto (80x10)
4. Context box inline (cluster | sessÃ£o)
5. Scroll quando necessÃ¡rio

**ValidaÃ§Ã£o VPN e Azure:**
- **VPN Check**: Usa `kubectl cluster-info` para validar conectividade K8s real
- **ValidaÃ§Ã£o on-demand**: Testa VPN em inÃ­cio, namespaces, HPAs e timeouts
- **Azure timeout**: 5 segundos para evitar travamentos DNS
- **Mensagens claras**: Exibidas no StatusContainer com soluÃ§Ãµes (F5 para retry)

### Building and Running
```bash
make build                    # Build to ./build/k8s-hpa-manager (version auto-detected)
make build-all                # Build for multiple platforms (Linux, macOS, Windows)
make run                      # Build and run
make run-dev                  # Run with debug logging (go run . --debug)
make version                  # Show detected version from git tags
make release                  # Build for all platforms (Linux, macOS amd64/arm64, Windows)
```

### Testing
```bash
make test                     # Run all tests with verbose output
make test-coverage            # Run tests with coverage (generates coverage.html)
```

### Installation
```bash
./install.sh                  # Automated installer â†’ /usr/local/bin/
./uninstall.sh                # Uninstaller (optionally removes session data)

# After installation
k8s-hpa-manager               # Run from anywhere
k8s-hpa-manager --debug       # Debug mode
k8s-hpa-manager --help        # Show help
```

### Cluster Auto-Discovery (NOVO)
```bash
k8s-hpa-manager autodiscover  # Auto-descobre clusters do kubeconfig
```
- Extrai resource groups do campo `user` (formato: `clusterAdmin_{RG}_{CLUSTER}`)
- Descobre subscriptions via Azure CLI
- Gera/atualiza `~/.k8s-hpa-manager/clusters-config.json`
- EscalÃ¡vel para 26, 70+ clusters

**Workflow:**
1. `az aks get-credentials --name CLUSTER --resource-group RG`
2. `k8s-hpa-manager autodiscover`
3. Node Pools prontos para uso

### Backup and Restore
```bash
./backup.sh "descriÃ§Ã£o"       # Criar backup antes de modificaÃ§Ãµes
./restore.sh                  # Listar backups disponÃ­veis
./restore.sh backup_name      # Restaurar backup especÃ­fico
```
- MantÃ©m os 10 backups mais recentes automaticamente
- Metadados inclusos (git commit, data, usuÃ¡rio)

### Local Development
```bash
./build/k8s-hpa-manager --debug              # Run local build
./build/k8s-hpa-manager --kubeconfig /path   # Custom kubeconfig
```

---

## ğŸ—ï¸ Architecture Overview

### Estrutura de DiretÃ³rios

```
k8s-hpa-manager/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ root.go                    # CLI entry point & commands (Cobra)
â”‚   â””â”€â”€ k8s-teste/                 # Layout test command
â”‚       â”œâ”€â”€ main.go
â”‚       â””â”€â”€ simple_demo.go
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ tui/                       # Terminal UI (Bubble Tea)
â”‚   â”‚   â”œâ”€â”€ app.go                 # Main orchestrator + centralized text methods
â”‚   â”‚   â”œâ”€â”€ handlers.go            # Event handlers
â”‚   â”‚   â”œâ”€â”€ views.go               # UI rendering & layout
â”‚   â”‚   â”œâ”€â”€ message.go             # Bubble Tea messages
â”‚   â”‚   â”œâ”€â”€ text_input.go          # Centralized text input with intelligent cursor
â”‚   â”‚   â”œâ”€â”€ resource_handlers.go   # HPA/Node Pool resource handlers
â”‚   â”‚   â”œâ”€â”€ resource_views.go      # Resource-specific views
â”‚   â”‚   â”œâ”€â”€ resource_operations.go # Resource operations
â”‚   â”‚   â”œâ”€â”€ cronjob_handlers.go    # CronJob handlers
â”‚   â”‚   â”œâ”€â”€ cronjob_views.go       # CronJob views
â”‚   â”‚   â”œâ”€â”€ add_cluster_*.go       # Cluster addition
â”‚   â”‚   â”œâ”€â”€ components/            # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ status_container.go
â”‚   â”‚   â”‚   â””â”€â”€ unified_container.go
â”‚   â”‚   â””â”€â”€ layout/                # Layout managers
â”‚   â”‚       â”œâ”€â”€ manager.go
â”‚   â”‚       â”œâ”€â”€ screen.go
â”‚   â”‚       â”œâ”€â”€ panels.go
â”‚   â”‚       â””â”€â”€ constants.go
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ types.go               # All data structures & app state
â”‚   â”œâ”€â”€ session/
â”‚   â”‚   â””â”€â”€ manager.go             # Session persistence (template naming)
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â””â”€â”€ client.go              # K8s API wrapper (client-go)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ kubeconfig.go          # Cluster discovery
â”‚   â”œâ”€â”€ azure/
â”‚   â”‚   â””â”€â”€ auth.go                # Azure SDK authentication
â”‚   â””â”€â”€ ui/                        # UI utilities
â”‚       â”œâ”€â”€ progress.go
â”‚       â”œâ”€â”€ logs.go
â”‚       â””â”€â”€ status_panel.go
â”œâ”€â”€ build/                         # Build artifacts
â”œâ”€â”€ backups/                       # Code backups (via backup.sh)
â”œâ”€â”€ go.mod & go.sum
â”œâ”€â”€ makefile
â””â”€â”€ *.sh scripts                   # Install, uninstall, backup, restore
```

### Core Components

**TUI Layer** (`internal/tui/`):
- `app.go` - Main Bubble Tea app with centralized text editing
- `handlers.go` - User input and event handling
- `views.go` - UI rendering with intelligent cursor display
- `text_input.go` - Centralized text input module with cursor overlay
- `resource_*.go` - HPA and node pool resource management
- `cronjob_*.go` - CronJob management (F9)
- `components/` - Reusable UI components (status, containers)
- `layout/` - Layout management system

**Business Logic** (`internal/`):
- `kubernetes/client.go` - K8s client wrapper with per-cluster management
- `config/kubeconfig.go` - Kubeconfig discovery (akspriv-* pattern)
- `session/manager.go` - Session persistence with template naming
- `models/types.go` - Complete domain model and app state (AppModel)
- `azure/auth.go` - Azure SDK auth with browser/device code fallback

**Entry Points**:
- `main.go` - Application bootstrap
- `cmd/root.go` - Cobra CLI commands and flags

### Data Flow

1. **State-Driven Architecture**: `AppModel` in `models/types.go` maintains complete app state
2. **State Transitions**: `AppState` enum manages flow:
   - Cluster Selection â†’ Session Selection â†’ Namespace Selection â†’ HPA/Node Pool Management â†’ Editing â†’ Help
3. **Multi-Selection Flow**: One Cluster â†’ Multiple Namespaces â†’ Multiple HPAs/Node Pools â†’ Individual Editing
4. **Bubble Tea Messages**: Coordinate between UI interactions and business logic
5. **Client Management**: Per-cluster Kubernetes client instances
6. **Session System**: Preserves state for review/editing before application

### Dependencies

**Core Framework**:
- Bubble Tea v0.24.2 - TUI framework
- Lipgloss v1.1.0 - Styling and layout
- Cobra v1.10.1 - CLI commands

**Kubernetes**:
- client-go v0.31.4 - Official K8s Go client

**Azure**:
- azcore v1.19.1 - Core SDK
- azidentity v1.12.0 - Authentication
- armcontainerservice - AKS node pool management
- Azure CLI - External dependency for node pool operations

---

## ğŸ†• Melhorias Recentes (2025)

### ğŸ“± Interface Responsiva e Otimizada para ProduÃ§Ã£o (Outubro 2025)
**Problema resolvido:** Interface exigia terminal 188x45, causando texto minÃºsculo e risco de erros operacionais

**SoluÃ§Ã£o implementada:**
- âœ… **Removido forÃ§amento de dimensÃµes** - `applyTerminalSizeLimit()` agora retorna tamanho REAL do terminal
- âœ… **PainÃ©is compactos** - Reduzidos de 70x18 para 60x12 (altura mÃ­nima 5 linhas)
- âœ… **Status panel menor** - 140x15 â†’ 80x10 (economiza espaÃ§o vertical)
- âœ… **Context box inline** - De 3-4 linhas com bordas para 1 linha: `cluster | sessÃ£o`
- âœ… **ValidaÃ§Ã£o Azure com timeout** - 5 segundos para evitar travamentos DNS
- âœ… **Texto legÃ­vel** - Terminal em tamanho normal (Ctrl+0), sem zoom out necessÃ¡rio

**Arquivos modificados:**
- `internal/tui/app.go` - Removido forÃ§amento, StatusContainer 80x10, timeout Azure
- `internal/tui/layout/constants.go` - MinTerminal 188x45 â†’ 80x24
- `internal/tui/views.go` - PainÃ©is 70x18 â†’ 60x12, context box simplificado, altura dinÃ¢mica
- `cmd/root.go` - Timeout 5s em validaÃ§Ã£o Azure

### ğŸ“Š Prometheus Stack Management (Outubro 2025)
- **Painel F8**: "Prometheus Stack Management" responsivo com scroll
- **MÃ©tricas AssÃ­ncronas**: Coleta em background via Metrics Server (nÃ£o bloqueia UI)
- **ExibiÃ§Ã£o Dual**:
  - Lista: `CPU: 1 (uso: 264m)/2 | MEM: 8Gi (uso: 3918Mi)/12Gi`
  - EdiÃ§Ã£o: `CPU Request: 1`, `Memory Request: 8Gi`
- **Campos Display Separados**: `DisplayCPURequest` vs `CurrentCPURequest` (edit)
- **Auto-scroll**: Item selecionado sempre visÃ­vel
- **Refresh a cada 300ms** durante coleta

### ğŸ”„ MemÃ³ria de Estado para CronJobs (Outubro 2025)
- **ESC preserva estado**: Volta para namespaces mantendo seleÃ§Ãµes e scroll
- **Fluxo**: Clusters â†’ Namespaces â†’ F9 (CronJobs) â†’ ESC (volta preservando)
- **DelegaÃ§Ã£o**: Handler ESC delegado para `handleEscape()` com lÃ³gica unificada
- **ConsistÃªncia**: Comportamento idÃªntico ao F8 (Prometheus)

### ğŸ”’ CorreÃ§Ã£o de Race Condition em Testes de Cluster (Outubro 2025)
**Problema resolvido:** Goroutines concorrentes causavam race condition ao testar conexÃµes com mÃºltiplos clusters simultaneamente

**Sintomas:**
- Stack trace mostrando mÃºltiplos goroutines (104, 105, 106, 107) acessando kubeconfig simultaneamente
- Erro em `message.go:216` durante `testSingleClusterConnection`
- Race condition em `json.Unmarshal` durante parsing de kubeconfig
- Concurrent map access em HTTP header creation

**Causa raiz:**
- `testClusterConnections()` iniciava testes paralelos para TODOS os clusters via `tea.Batch()`
- Cada goroutine chamava `getClient()` que carregava e parseava kubeconfig
- OperaÃ§Ãµes de criaÃ§Ã£o de cliente NÃƒO eram thread-safe
- MÃºltiplos goroutines tentavam criar clientes simultaneamente sem sincronizaÃ§Ã£o

**SoluÃ§Ã£o implementada:**
- âœ… **Mutex RWLock** - Adicionado `sync.RWMutex` em `KubeConfigManager`
- âœ… **Double-check locking** - PadrÃ£o otimizado para minimizar contenÃ§Ã£o
- âœ… **Read lock para leituras** - Permite mÃºltiplas leituras concorrentes de clientes existentes
- âœ… **Write lock para criaÃ§Ã£o** - Serializa criaÃ§Ã£o de novos clientes
- âœ… **Thread-safe client cache** - Map de clientes protegido por mutex

**Arquivos modificados:**
- `internal/config/kubeconfig.go` - Adicionado import `sync`, field `clientMutex sync.RWMutex`, lÃ³gica de double-check locking

**CÃ³digo antes:**
```go
func (k *KubeConfigManager) getClient(clusterName string) (kubernetes.Interface, error) {
    if client, exists := k.clients[clusterName]; exists {  // âŒ Race condition
        return client, nil
    }
    // ... criar cliente sem proteÃ§Ã£o ...
    k.clients[clusterName] = client  // âŒ Concurrent map write
    return client, nil
}
```

**CÃ³digo depois:**
```go
func (k *KubeConfigManager) getClient(clusterName string) (kubernetes.Interface, error) {
    // 1. Read lock para checagem rÃ¡pida (permite leituras concorrentes)
    k.clientMutex.RLock()
    if client, exists := k.clients[clusterName]; exists {
        k.clientMutex.RUnlock()
        return client, nil
    }
    k.clientMutex.RUnlock()

    // 2. Write lock para criaÃ§Ã£o (serializa criaÃ§Ã£o)
    k.clientMutex.Lock()
    defer k.clientMutex.Unlock()

    // 3. Double-check: outro goroutine pode ter criado enquanto esperÃ¡vamos lock
    if client, exists := k.clients[clusterName]; exists {
        return client, nil
    }

    // 4. Criar cliente de forma thread-safe
    // ... cÃ³digo de criaÃ§Ã£o ...
    k.clients[clusterName] = client  // âœ… Protegido por write lock
    return client, nil
}
```

**BenefÃ­cios:**
- **Performance**: Read lock permite mÃºltiplas leituras simultÃ¢neas (baixa contenÃ§Ã£o)
- **SeguranÃ§a**: Write lock serializa criaÃ§Ã£o de clientes (sem race conditions)
- **EficiÃªncia**: Double-check locking evita lock desnecessÃ¡rio se cliente jÃ¡ existe
- **ProduÃ§Ã£o-ready**: SoluÃ§Ã£o padrÃ£o para lazy initialization concorrente em Go

### ğŸ› CorreÃ§Ã£o de Warnings Azure CLI como Erros (Outubro 2025)
**Problema resolvido:** Azure CLI warnings (como `pkg_resources deprecated`) eram tratados como erros fatais, abortando operaÃ§Ãµes de node pool

**SoluÃ§Ã£o implementada:**
- âœ… **SeparaÃ§Ã£o stdout/stderr** - `cmd.Stdout` e `cmd.Stderr` em buffers separados
- âœ… **Lista de warnings conhecidos** - Ignora `pkg_resources`, `extension altered`, etc
- âœ… **ValidaÃ§Ã£o inteligente** - Verifica se stderr contÃ©m APENAS warnings
- âœ… **Exit code real** - Usa `cmd.Run()` para verificar sucesso, nÃ£o presenÃ§a de stderr
- âœ… **Debug mode** - Warnings aparecem em `--debug` mas nÃ£o falham operaÃ§Ã£o

**Warnings ignorados:**
```
UserWarning: pkg_resources is deprecated
The behavior of this command has been altered by the following extension
__import__('pkg_resources').declare_namespace(__name__)
WARNING: (qualquer linha com prefixo WARNING:)
```

**Arquivos modificados:**
- `internal/tui/app.go:3535-3683` - FunÃ§Ã£o `executeAzureCommand()` refatorada
- Import `bytes` adicionado para buffers separados

**Antes:**
```go
output, err := cmd.CombinedOutput()  // âŒ Mistura stdout + stderr
if err != nil { return error }       // âŒ Warnings tratados como erro
```

**Depois:**
```go
var stdout, stderr bytes.Buffer
cmd.Stdout, cmd.Stderr = &stdout, &stderr
err := cmd.Run()
if err != nil && !isOnlyWarnings(stderr) { return error }  // âœ… Ignora warnings
```

### ğŸ”„ LÃ³gica Sequencial Inteligente de Node Pools (Outubro 2025)
**Problema resolvido:** Azure CLI nÃ£o permite `scale` com autoscaling habilitado - aplicaÃ§Ã£o tentava scale ANTES de desabilitar autoscaling

**CenÃ¡rio problemÃ¡tico:**
- UsuÃ¡rio muda node pool de **AUTO â†’ MANUAL** e define `NodeCount = 0`
- AplicaÃ§Ã£o tentava: `az aks nodepool scale` â†’ âŒ **ERROR: Cannot scale cluster autoscaler enabled node pool**
- Ordem errada dos comandos causava falha

**SoluÃ§Ã£o implementada:**
- âœ… **4 cenÃ¡rios detectados automaticamente**:
  1. **AUTO â†’ MANUAL**: Desabilita autoscaling â†’ Faz scale
  2. **MANUAL â†’ AUTO**: Faz scale â†’ Habilita autoscaling com min/max
  3. **AUTO â†’ AUTO**: Atualiza min/max count
  4. **MANUAL â†’ MANUAL**: Faz scale direto

**Arquivos modificados:**
- `internal/tui/app.go:3433-3545` - LÃ³gica de construÃ§Ã£o de comandos refatorada

**Workflow esperado pelo usuÃ¡rio (agora funciona!):**
```bash
# CenÃ¡rio: Stress test com scale down completo
# 1. Node pool "fatura" estÃ¡ com autoscaling AUTO (min: 2, max: 5)
# 2. UsuÃ¡rio muda para MANUAL e define NodeCount = 0
# 3. AplicaÃ§Ã£o INTELIGENTEMENTE executa:
#    â†’ PASSO 1: az aks nodepool update --disable-cluster-autoscaler
#    â†’ PASSO 2: az aks nodepool scale --node-count 0
# âœ… OperaÃ§Ã£o bem-sucedida!
```

**CÃ³digo antes (ordem errada):**
```go
// âŒ Tentava scale ANTES de desabilitar
if pool.NodeCount != pool.OriginalValues.NodeCount {
    cmds.append(scaleCommand)  // ERRO se autoscaling ativo!
}
if pool.AutoscalingEnabled != pool.OriginalValues.AutoscalingEnabled {
    cmds.append(disableAutoscaling)
}
```

**CÃ³digo depois (ordem inteligente):**
```go
// âœ… Detecta cenÃ¡rio e ordena comandos corretamente
changingToManual := pool.OriginalValues.AutoscalingEnabled && !pool.AutoscalingEnabled
if changingToManual {
    cmds.append(disableAutoscaling)  // PRIMEIRO desabilita
    if nodeCountChanged {
        cmds.append(scaleCommand)     // DEPOIS faz scale
    }
}
```

### ğŸš€ ExecuÃ§Ã£o Sequencial AssÃ­ncrona de Node Pools (Outubro 2025)
**Problema resolvido:** ExecuÃ§Ã£o sequencial bloqueava a interface durante aplicaÃ§Ã£o de node pools

**SoluÃ§Ã£o implementada:**
- âœ… **ExecuÃ§Ã£o totalmente assÃ­ncrona** - Non-blocking via Bubble Tea messages
- âœ… **Interface sempre responsiva** - Edite HPAs, navegue, gerencie outros recursos
- âœ… **Feedback em tempo real** - StatusContainer mostra progresso ao vivo
- âœ… **Auto-execuÃ§Ã£o do segundo pool** - Sistema monitora *1 e inicia *2 automaticamente
- âœ… **Multi-tasking completo** - Aplique HPAs enquanto node pools executam
- âœ… **ValidaÃ§Ã£o VPN integrada** - Verifica conectividade antes de aplicar (timeout 5s)
- âœ… **Error handling robusto** - Falhas nÃ£o travam a UI, feedback claro se VPN desconectada

**Workflow:**
1. F12 para marcar monitoring-1 (*1) e monitoring-2 (*2)
2. Editar valores (manual/auto, node counts)
3. Ctrl+D/U â†’ Inicia execuÃ§Ã£o assÃ­ncrona
4. StatusContainer: `ğŸ” Verificando conectividade VPN com Azure...`
5. StatusContainer: `âœ… VPN conectada - Azure acessÃ­vel` (ou `âŒ VPN desconectada`)
6. **Interface livre** - Edite HPAs, CronJobs, etc.
7. StatusContainer: `ğŸ”„ *1: Aplicando...` â†’ `âœ… *1: Completado`
8. Sistema inicia *2 automaticamente
9. StatusContainer: `ğŸš€ Iniciando automaticamente *2` â†’ `âœ… *2: Completado`

**Arquivos modificados:**
- `internal/tui/message.go` - Novas mensagens assÃ­ncronas
- `internal/tui/app.go` - Handlers para mensagens sequenciais
- `internal/tui/handlers.go` - DetecÃ§Ã£o automÃ¡tica de execuÃ§Ã£o sequencial

### ğŸ” ValidaÃ§Ã£o VPN On-Demand (Outubro 2025)
**Problema resolvido:** AplicaÃ§Ã£o nÃ£o validava VPN antes de operaÃ§Ãµes Kubernetes, causando timeouts e erros confusos

**SoluÃ§Ã£o implementada:**
- âœ… **ValidaÃ§Ã£o com kubectl** - Usa `kubectl cluster-info --request-timeout=5s` (testa conectividade K8s real)
- âœ… **On-demand em pontos crÃ­ticos**:
  - `discoverClusters()` - Valida VPN antes de descobrir clusters
  - `loadNamespaces()` - Valida VPN antes de carregar namespaces
  - `loadHPAs()` - Valida VPN antes de carregar HPAs
  - `testSingleClusterConnection()` - Diagnostica em timeout/erro de conexÃ£o
  - `configurateSubscription()` - Diagnostica em timeout Azure (5s)
- âœ… **Mensagens no StatusContainer** - NÃ£o quebra TUI, exibe dentro do container
- âœ… **SoluÃ§Ãµes claras**:
  - `âŒ VPN desconectada - Kubernetes inacessÃ­vel`
  - `ğŸ’¡ SOLUÃ‡ÃƒO: Conecte-se Ã  VPN e tente novamente (F5)`

**Workflow:**
1. AplicaÃ§Ã£o inicia â†’ `ğŸ” Validando conectividade VPN...`
2. Se VPN OFF â†’ `âŒ VPN desconectada - kubectl nÃ£o funcionarÃ¡` + mensagem de soluÃ§Ã£o
3. Se VPN ON â†’ `âœ… VPN conectada - Kubernetes acessÃ­vel` + continua operaÃ§Ã£o
4. Em qualquer timeout posterior â†’ Diagnostica novamente VPN/Azure AD

**Arquivos modificados:**
- `internal/tui/message.go` - FunÃ§Ã£o `checkVPNConnectivity()` com kubectl, integrado em pontos crÃ­ticos
- `internal/tui/app.go` - ValidaÃ§Ã£o VPN em `loadNamespaces()` e `loadHPAs()`
- `internal/models/types.go` - Campos VPN status (VPNConnected, VPNLastCheck, VPNStatusMessage)

### ğŸ” Auto-Descoberta de Clusters (Outubro 2025)
- **Comando**: `k8s-hpa-manager autodiscover`
- **ExtraÃ§Ã£o**: Resource groups do campo `user` (formato: `clusterAdmin_{RG}_{CLUSTER}`)
- **IntegraÃ§Ã£o Azure CLI**: Descobre subscriptions automaticamente
- **EscalÃ¡vel**: 26, 70+ clusters sem configuraÃ§Ã£o manual
- **Casos de Uso**: Onboarding, mudanÃ§as de subscriptions, rotaÃ§Ã£o de credenciais

### âœ… Modais de ConfirmaÃ§Ã£o e SeguranÃ§a (Outubro 2025)
**Problema resolvido:** OperaÃ§Ãµes crÃ­ticas (Ctrl+D/Ctrl+U) aplicavam alteraÃ§Ãµes imediatamente sem confirmaÃ§Ã£o, risco de erros operacionais

**SoluÃ§Ã£o implementada:**
- âœ… **ConfirmaÃ§Ã£o obrigatÃ³ria** - Todos Ctrl+D/Ctrl+U exigem confirmaÃ§Ã£o explÃ­cita (ENTER/ESC)
- âœ… **Modais como overlay** - Aparecem sobre o conteÃºdo sem esconder a aplicaÃ§Ã£o (mantÃ©m contexto visual)
- âœ… **Mensagens personalizadas por tipo**:
  - HPA individual: `"Aplicar alteraÃ§Ãµes do HPA:\nnamespace/nome"`
  - HPAs em lote: `"Aplicar alteraÃ§Ãµes em TODOS os HPAs selecionados"`
  - Node pools: `"Aplicar alteraÃ§Ãµes nos Node Pools modificados"`
  - Node pools sequencial: `"Executar sequencialmente:\n*1 pool1 â†’ *2 pool2"`
  - SessÃ£o mista: `"Aplicar alteraÃ§Ãµes da sessÃ£o mista:\nX HPAs + Y Node Pools"`
- âœ… **Indicador de quantidade** - `âš¡ X itens serÃ£o modificados no cluster!`
- âœ… **Modal de erro VPN** - Feedback visual quando VPN desconectada com soluÃ§Ãµes (F5: reload, ESC: fechar)
- âœ… **Modal de restart** - ApÃ³s auto-descoberta informa necessidade de restart (F4: sair, ESC: continuar)

**Arquivos modificados:**
- `internal/models/types.go` - Campos ShowConfirmModal, ConfirmModalCallback, etc
- `internal/tui/views.go` - FunÃ§Ãµes renderConfirmModal(), renderVPNErrorModal(), renderRestartModal()
- `internal/tui/app.go` - renderModalOverlay() para exibir modais sobre conteÃºdo, executeConfirmedAction()
- `internal/tui/handlers.go` - Modificados Ctrl+D/Ctrl+U para mostrar modal antes de aplicar

### ğŸ”„ NavegaÃ§Ã£o Sequencial entre Abas (Outubro 2025)
- **Ctrl+â†’**: PrÃ³xima aba com wrap-around (Ãºltima â†’ primeira)
- **Ctrl+â†**: Aba anterior com wrap-around (primeira â†’ Ãºltima)
- **Ãcones direcionais**: â¬…ï¸/â¡ï¸ no status durante navegaÃ§Ã£o
- **Foco sempre visÃ­vel**: Aba em foco exibida apÃ³s navegaÃ§Ã£o
- **Complementa Alt+1-9/0**: NavegaÃ§Ã£o numÃ©rica direta + navegaÃ§Ã£o sequencial

### ğŸ”§ CorreÃ§Ã£o de Carregamento de Node Pools (Outubro 2025)
**Problema resolvido:** Node pools nÃ£o carregavam porque a aplicaÃ§Ã£o procurava `clusters-config.json` em locais incorretos

**SoluÃ§Ã£o implementada:**
- âœ… **Prioridade de busca corrigida** - Agora busca primeiro em `~/.k8s-hpa-manager/clusters-config.json` (onde `autodiscover` salva)
- âœ… **Fallback inteligente** - Se nÃ£o encontrar no diretÃ³rio padrÃ£o, tenta:
  1. `~/.k8s-hpa-manager/clusters-config.json` (padrÃ£o - onde autodiscover salva)
  2. DiretÃ³rio do executÃ¡vel (fallback 1)
  3. DiretÃ³rio de trabalho atual (fallback 2)
- âœ… **Mensagem de erro clara** - Sugere executar `k8s-hpa-manager autodiscover` se arquivo nÃ£o for encontrado
- âœ… **ConsistÃªncia com autodiscover** - Ambos usam o mesmo diretÃ³rio padrÃ£o

**Causa raiz:**
- `loadClusterConfig()` em `internal/tui/message.go` buscava primeiro no diretÃ³rio do executÃ¡vel
- Comando `autodiscover` salva em `~/.k8s-hpa-manager/` (diretÃ³rio padrÃ£o da aplicaÃ§Ã£o)
- Incompatibilidade de caminhos causava falha no carregamento dos node pools

**Arquivos modificados:**
- `internal/tui/message.go` (linhas 467-501) - FunÃ§Ã£o `loadClusterConfig()` com prioridade corrigida
- `internal/tui/views.go` (linhas 3092-3093) - Help atualizado com informaÃ§Ã£o da correÃ§Ã£o

**Workflow correto:**
1. `k8s-hpa-manager autodiscover` â†’ gera `~/.k8s-hpa-manager/clusters-config.json`
2. AplicaÃ§Ã£o inicia â†’ busca primeiro em `~/.k8s-hpa-manager/`
3. Node pools carregam corretamente âœ…

### ğŸ“ Log Detalhado de AlteraÃ§Ãµes (Outubro 2025)
**Problema resolvido:** UsuÃ¡rio nÃ£o via quais alteraÃ§Ãµes estavam sendo aplicadas, apenas mensagens genÃ©ricas de sucesso/erro

**SoluÃ§Ã£o implementada:**
- âœ… **Log antes â†’ depois** - Todas alteraÃ§Ãµes exibidas no StatusContainer com formato `valor_antigo â†’ valor_novo`
- âœ… **AlteraÃ§Ãµes de HPA** - Min/Max Replicas, CPU/Memory Target: `Min Replicas: 1 â†’ 2`, `CPU Target: 50% â†’ 70%`
- âœ… **AlteraÃ§Ãµes de recursos** - CPU/Memory Request/Limit: `CPU Request: 50m â†’ 100m`, `Memory Limit: 512Mi â†’ 1Gi`
- âœ… **AlteraÃ§Ãµes de node pools** - Count, Min/Max, Autoscaling: `Node Count: 3 â†’ 5`, `Autoscaling: Desativado â†’ Ativado`
- âœ… **Logs por operaÃ§Ã£o**:
  - `âš™ï¸ Aplicando HPA: namespace/nome`
  - `ğŸ“ Min Replicas: 1 â†’ 2`
  - `ğŸ”§ CPU Request: 50m â†’ 100m`
  - `âœ… HPA aplicado: namespace/nome`
- âœ… **Logs de erro** - `âŒ Erro ao aplicar HPA namespace/nome: erro_detalhado`

**Exemplo de output:**
```
âš™ï¸ Aplicando HPA: ingress-nginx/nginx-ingress-controller
  ğŸ“ Min Replicas: 1 â†’ 2
  ğŸ“ Max Replicas: 8 â†’ 12
  ğŸ“ CPU Target: 60% â†’ 70%
  ğŸ”§ CPU Request: 50m â†’ 100m
  ğŸ”§ Memory Request: 90Mi â†’ 180Mi
âœ… HPA aplicado: ingress-nginx/nginx-ingress-controller
```

**Arquivos modificados:**
- `internal/tui/app.go` - FunÃ§Ãµes logHPAChanges(), logResourceChanges(), logNodePoolChanges()
- `internal/tui/app.go` - applyHPAChanges() e applyHPAChangesAsync() com logs detalhados
- `internal/tui/app.go` - applyNodePoolChanges() com logs detalhados

### ğŸ”„ Sistema de Versionamento AutomÃ¡tico e Updates (Outubro 2025)
**Funcionalidade:** Sistema completo de versionamento semÃ¢ntico e verificaÃ§Ã£o automÃ¡tica de updates

**CaracterÃ­sticas implementadas:**
- âœ… **Versionamento automÃ¡tico via Git Tags** - VersÃ£o injetada no build usando `git describe --tags`
- âœ… **Comando version** - `k8s-hpa-manager version` mostra versÃ£o e verifica updates
- âœ… **VerificaÃ§Ã£o em background** - Checa GitHub Releases 1x por dia (nÃ£o-bloqueante)
- âœ… **NotificaÃ§Ã£o no TUI** - Mensagens aparecem no StatusContainer apÃ³s 3 segundos
- âœ… **Flag configurÃ¡vel** - `--check-updates=false` para desabilitar verificaÃ§Ã£o
- âœ… **VersÃ£o dev** - Builds sem tag mostram "dev-<commit>" e nÃ£o verificam updates
- âœ… **Cache inteligente** - Arquivo `~/.k8s-hpa-manager/.update-check` controla frequÃªncia
- âœ… **Timeout 5s** - NÃ£o trava se GitHub estiver offline

**Estrutura:**
```
internal/updater/
â”œâ”€â”€ version.go    # Versionamento semÃ¢ntico (var Version injetada)
â”œâ”€â”€ github.go     # Cliente GitHub API (releases/latest)
â””â”€â”€ checker.go    # LÃ³gica de verificaÃ§Ã£o (1x/dia, cache)
```

**Workflow de Release:**
```bash
# 1. Criar tag de versÃ£o
git tag v1.6.0
git push origin v1.6.0

# 2. Build automÃ¡tico com versÃ£o injetada
make build
# Output: Building k8s-hpa-manager v1.6.0...

# 3. Verificar versÃ£o no binÃ¡rio
./build/k8s-hpa-manager version
# Output: k8s-hpa-manager versÃ£o 1.6.0

# 4. Criar release multiplataforma
make release
# Gera binÃ¡rios para Linux, macOS (amd64/arm64), Windows
```

**Comandos:**
```bash
# Verificar versÃ£o e updates
k8s-hpa-manager version

# Ver versÃ£o detectada durante build
make version

# Build com versÃ£o injetada
make build                    # VersÃ£o da tag atual
make release                  # Multi-platform builds

# Desabilitar verificaÃ§Ã£o automÃ¡tica
k8s-hpa-manager --check-updates=false
```

**NotificaÃ§Ã£o no TUI:**
Quando houver update disponÃ­vel (apÃ³s 3s do startup):
```
â”Œâ”€ Status e InformaÃ§Ãµes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ†• Nova versÃ£o disponÃ­vel: 1.5.0 â†’ 1.6.0  â”‚
â”‚ ğŸ“¦ Download: https://github.com/.../v1.6.0â”‚
â”‚ ğŸ’¡ Execute 'k8s-hpa-manager version'       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Arquivos modificados:**
- `internal/updater/` (NOVO) - Sistema completo de versionamento
- `cmd/version.go` (NOVO) - Comando version
- `cmd/root.go` - Flag --check-updates e verificaÃ§Ã£o em background
- `internal/tui/app.go` - NotificaÃ§Ã£o no StatusContainer (checkForUpdatesInBackground)
- `makefile` - LDFLAGS com injeÃ§Ã£o de versÃ£o, targets version e release

### ğŸ“ Sistema de Logs Completo (Outubro 2025)
**Funcionalidade:** Sistema completo de logging com visualizador TUI integrado

**CaracterÃ­sticas implementadas:**
- âœ… **Salvamento automÃ¡tico** - Todos os logs do StatusContainer salvos em `Logs/k8s-hpa-manager_YYYY-MM-DD.log`
- âœ… **RotaÃ§Ã£o de arquivos** - 10MB por arquivo, mantÃ©m 5 backups
- âœ… **Thread-safe** - Mutex para operaÃ§Ãµes concorrentes
- âœ… **Buffer em memÃ³ria** - 1000 linhas para acesso rÃ¡pido
- âœ… **Visualizador TUI (F3)** - Interface completa de visualizaÃ§Ã£o
- âœ… **ColorizaÃ§Ã£o** - Logs coloridos por nÃ­vel (ERROR vermelho, WARNING laranja, SUCCESS verde)
- âœ… **NavegaÃ§Ã£o completa** - â†‘â†“/k j, PgUp/PgDn, Home/End
- âœ… **Copiar logs** - Tecla C copia para `/tmp/k8s-hpa-manager-logs.txt`
- âœ… **Limpar logs** - Tecla L limpa arquivo de logs
- âœ… **Reload** - R/F5 recarrega logs em tempo real
- âœ… **ESC para voltar** - Retorna ao estado anterior

**Estrutura:**
```
Logs/
â””â”€â”€ k8s-hpa-manager_2025-10-15.log    # Logs do dia
internal/logs/
â””â”€â”€ manager.go                         # Singleton LogManager
internal/tui/
â”œâ”€â”€ logviewer_handlers.go              # Handlers de navegaÃ§Ã£o
â””â”€â”€ logviewer_views.go                 # RenderizaÃ§Ã£o colorida
```

**Arquivos modificados:**
- `internal/logs/manager.go` (NOVO) - Sistema completo de logging
- `internal/tui/logviewer_handlers.go` (NOVO) - Handlers do visualizador
- `internal/tui/logviewer_views.go` (NOVO) - RenderizaÃ§Ã£o TUI
- `internal/tui/components/status_container.go` - IntegraÃ§Ã£o automÃ¡tica
- `internal/tui/app.go` - F3 global, handleEscape para StateLogViewer
- `internal/models/types.go` - StateLogViewer e campos relacionados
- `.gitignore` - Ignora `Logs/` e `*.log`

### ğŸ› CorreÃ§Ã£o de NavegaÃ§Ã£o ESC em Node Pools (Outubro 2025)
**Problema resolvido:** ESC na tela de node pools voltava para seleÃ§Ã£o de clusters em vez de namespaces

**SoluÃ§Ã£o implementada:**
- âœ… **Fluxo corrigido**: Namespaces â†’ Ctrl+N â†’ Node Pools â†’ ESC â†’ Namespaces
- âœ… **ConsistÃªncia**: Volta para onde veio (origem do Ctrl+N)

**Arquivo modificado:**
- `internal/tui/app.go:1603` - `StateNodeSelection` agora vai para `StateNamespaceSelection`

**Antes:** `Clusters â† ESC â† Node Pools` âŒ
**Depois:** `Namespaces â† ESC â† Node Pools` âœ…

### ğŸ”§ CorreÃ§Ãµes de Linter para CI/CD (Outubro 2025)
**Problema resolvido:** GitHub Actions falhavam com erros de linter

**CorreÃ§Ãµes aplicadas:**
- âœ… **strings.TrimSuffix** - Simplificado em 3 locais (app.go, message.go)
- âœ… **fmt.Sprintf desnecessÃ¡rio** - Removido em 4 locais (handlers.go, app.go)
- âœ… **fmt.Println com \n redundante** - Corrigido em cmd/root.go e cmd/k8s-teste/main.go
- âœ… **Nil check redundante** - Removido em app.go:4362

**Resultado:**
- âœ… `make test` passa sem erros
- âœ… CI do GitHub passa
- â„¹ï¸ 77 sugestÃµes de linter restantes (nÃ£o crÃ­ticas, cÃ³digo funcional)

### ğŸ’¾ Salvamento Manual para Rollback (Janeiro 2025)
- **Ctrl+S sem modificaÃ§Ãµes**: Cria snapshots para rollback
- **Workflow**:
  1. Carregar sessÃ£o (Ctrl+L)
  2. Ctrl+S imediatamente (sem modificar)
  3. Nomear como "rollback-producao-2025-01-10"
  4. SessÃ£o de backup pronta
- **Funciona em**: HPAs, Node Pools, SessÃµes Mistas

### ğŸ”„ ExecuÃ§Ã£o Sequencial de Node Pools (AssÃ­ncrona)
- **F12**: Marca atÃ© 2 node pools para execuÃ§Ã£o sequencial
- **Indicadores**: `*1` (manual), `*2` (auto apÃ³s conclusÃ£o do *1)
- **ExecuÃ§Ã£o AssÃ­ncrona**: Non-blocking - interface permanece responsiva
- **Multi-tasking**: Edite HPAs, gerencie CronJobs enquanto node pools executam
- **Feedback em Tempo Real**: StatusContainer mostra progresso
- **Stress Tests**: monitoring-1 â†’ 0 nodes, monitoring-2 â†’ scale up
- **PersistÃªncia**: Salvos em sessÃµes

### ğŸ¯ CronJob Management (F9)
- **Acesso**: F9 na seleÃ§Ã£o de namespaces
- **Status**: ğŸŸ¢ Ativo, ğŸ”´ Suspenso, ğŸŸ¡ Falhou, ğŸ”µ Executando
- **Schedule**: ConversÃ£o cron â†’ texto ("0 2 * * * - executa todo dia Ã s 2:00 AM")
- **OperaÃ§Ãµes**: Ctrl+D individual, Ctrl+U batch
- **MemÃ³ria**: ESC volta para namespaces preservando estado

### ğŸ¨ Progress Bars Rich Python
- **Caracteres**: â” (preenchido), â•Œ (vazio)
- **Cores DinÃ¢micas**:
  - 0-24%: ğŸ”´ Vermelho
  - 25-49%: ğŸŸ  Laranja
  - 50-74%: ğŸŸ¡ Amarelo
  - 75-99%: ğŸŸ¢ Verde claro
  - 100%: âœ… Verde completo
- **Lifecycle**: InÃ­cio â†’ Progresso â†’ Sucesso/Falha
- **Auto-cleanup**: 3 segundos apÃ³s conclusÃ£o
- **Bottom-Up**: Novos itens na Ãºltima linha

### ğŸ”§ CorreÃ§Ãµes CrÃ­ticas
- **MinReplicas**: Corrigido parsing de ponteiros *int32
- **Rollouts em SessÃµes**: DaemonSet/StatefulSet salvos corretamente
- **Variation Selectors**: Removidos caracteres invisÃ­veis (U+FE0F) de emojis
- **Terminal Size**: LimitaÃ§Ã£o 188x45 para garantir visibilidade
- **Mouse Selection**: Removido `tea.WithMouseCellMotion()` para permitir seleÃ§Ã£o de texto

### ğŸ“ Layout Responsivo (Janeiro 2025)
- **Terminal Limit**: 42 linhas x 185 colunas (via `applyTerminalSizeLimit()`)
- **EspaÃ§amento Universal**: Status Panel em posiÃ§Ã£o fixa em todas as telas
- **Node Pool Responsivo**: Scroll Shift+Up/Down, indicadores `[5-15/45]`

---

## âŒ¨ï¸ User Interface & Controls

### Navigation Controls
- **Arrow Keys / vi-keys (hjkl)**: Navigate lists
- **Tab**: Switch panels
- **Space**: Select/deselect items
- **Enter**: Confirm selection or edit
- **ESC**: Go back/cancel (preserva contexto!)
- **F3**: Log viewer (scroll, copiar, limpar)
- **F4**: Exit application
- **?**: Help screen (scrollable)

### Cluster & Session Management
- **Ctrl+L**: Load session
- **F5/R**: Reload cluster list
- **Ctrl+S**: Save session (funciona SEM modificaÃ§Ãµes para rollback)
- **Ctrl+M**: Create mixed session (HPAs + Node Pools)

### HPA Operations
- **Ctrl+D**: Apply individual HPA (shows â— counter)
- **Ctrl+U**: Apply all HPAs in batch
- **Space** (edit mode): Toggle rollout (Deployment/DaemonSet/StatefulSet)
- **â†‘â†“**: Navigate rollout fields

### Node Pool Operations
- **Ctrl+N**: Access node pool management
- **Ctrl+D/Ctrl+U**: Apply node pool changes
- **Space** (edit mode): Toggle autoscaling
- **F12**: Mark for sequential execution (max 2)

### CronJob Management (F9)
- **F9**: Access CronJob management
- **Space**: Select/deselect
- **Enter**: Edit (enable/disable suspend)
- **Ctrl+D/U**: Apply changes
- **ESC**: Return to namespaces (preserves state)

### Prometheus Stack (F8)
- **F8**: Access Prometheus Stack management
- **Enter**: Edit resources (CPU/Memory requests/limits)
- **Shift+Up/Down**: Scroll
- **ESC**: Return to namespaces (preserves state)

### Tab Navigation
- **Ctrl+T**: New tab (max 10 tabs)
- **Ctrl+W**: Close current tab (doesn't close last)
- **Alt+1-9**: Switch to tab 1-9 (quick shortcut)
- **Alt+0**: Switch to tab 10
- **Ctrl+â†’**: Next tab (with wrap-around)
- **Ctrl+â†**: Previous tab (with wrap-around)

### Scroll Controls
- **Shift+Up/Down**: Scroll painÃ©is responsivos
- **Mouse Wheel**: Alternative scroll
- **Indicadores**: `[5-15/45]` mostram posiÃ§Ã£o

### Log Viewer (F3)
- **F3**: Open log viewer
- **â†‘â†“ / k j**: Scroll line by line
- **PgUp/PgDn**: Scroll by page
- **Home**: Jump to beginning
- **End**: Jump to end
- **C**: Copy logs to `/tmp/k8s-hpa-manager-logs.txt`
- **L**: Clear all logs
- **R / F5**: Reload logs
- **ESC**: Return to previous screen

### Special Features
- **S** (namespace selection): Toggle system namespaces
- **F9**: CronJob management
- **F8**: Prometheus Stack management

---

## ğŸ”‘ Key Features

### Kubernetes HPA Management
1. **Cluster Discovery**: Auto-discover `akspriv-*` clusters
2. **Single Cluster Selection**: One cluster at a time
3. **Multiple Namespace Selection**: Visual indicators + toggle
4. **System Namespace Filtering**: Toggle with `S` key
5. **Async HPA Counting**: Background counting for performance
6. **Multi-HPA Selection**: Batch operations
7. **Live Editing**: Min/max replicas, CPU/memory targets
8. **Rollout Integration**: Toggle per HPA (Deployment/DaemonSet/StatefulSet)

### Azure AKS Node Pool Management
9. **Node Pool Discovery**: From `clusters-config.json` or autodiscover
10. **Azure Authentication**: Browser + device code fallback
11. **Subscription Management**: Auto-configuration
12. **Node Pool Editing**: Count, min/max, autoscaler
13. **Real-time Application**: Via Azure CLI with progress
14. **Filtered Output**: Clean JSON from Azure CLI

### Session & Workflow Management
15. **Unified Sessions**: Save/restore HPA and node pools
16. **Mixed Sessions**: Combine HPAs + Node Pools (Ctrl+M)
17. **Session Type Detection**: Auto-detect HPA/Node Pool/Mixed
18. **State-Preserving Loading**: Review before applying
19. **Template Naming**: Variables: `{action}_{cluster}_{timestamp}_{env}_{user}_{hpa_count}`
20. **Session Name Display**: Persistent across screens
21. **Rollout Persistence**: Saved with sessions
22. **Manual Rollback**: Save without modifications (Ctrl+S)

### Session Storage
- **Location**: `~/.k8s-hpa-manager/sessions/`
- **Folders**: `HPA-Upscale/`, `HPA-Downscale/`, `Node-Upscale/`, `Node-Downscale/`
- **Cleanup**: Keeps 5 most recent autosaves

### Progress Tracking & Status
23. **Async Rollout Progress**: Rich Python-style bars (â”/â•Œ)
24. **Integrated Status Panel**: "ğŸ“Š Status e InformaÃ§Ãµes" (140x15)
25. **Multiple Rollout Support**: Deployment/DaemonSet/StatefulSet
26. **Thread-Safe Updates**: Mutex-protected progress
27. **Visual Indicators**: â— counters for application tracking

### User Interface & Experience
28. **Comprehensive Help**: `?` key with scroll navigation
29. **Cluster Connectivity**: Real-time status indicators
30. **Individual & Batch**: Ctrl+D (individual), Ctrl+U (batch)
31. **Error Recovery**: ESC returns from errors
32. **Multi-panel Interface**: TAB navigation, namespace-grouped HPAs
33. **Non-Blocking Status**: Dedicated panel, no workflow interruption
34. **Auto Subscription Switching**: Azure subscription per cluster
35. **Token Expiration Handling**: Auto re-authentication

---

## ğŸ”§ Troubleshooting

### Common Issues

**Installation:**
- **"Go not found"**: Install Go 1.23+ from https://golang.org/dl/
- **Permission denied**: Script needs sudo for `/usr/local/bin/`
- **Binary not found**: Restart terminal or check PATH

**Cluster:**
- **Offline status**: `kubectl cluster-info --context=<cluster>`
- **Client not found**: Fixed in recent versions, restart if persists
- **HPAs not loading**: Check RBAC, toggle system namespaces with `S`

**Sessions:**
- **Loading doesn't apply**: Intentional - use Ctrl+D/U to apply
- **Help too large**: Use â†‘â†“ or PgUp/PgDn to scroll

### Error Recovery
- **ESC**: Returns from errors while preserving context
- **F4**: Force exit
- **?**: Access help from any screen

### Performance Tips
- System namespace filtering improves loading speed
- Background HPA counting reduces wait times
- Session system preserves work between runs

---

## ğŸš€ Continuing Development

### Context for Next Claude Sessions

**Quick Context Template:**
```
Projeto: Terminal-based Kubernetes HPA + Azure AKS Node Pool management tool

Tech Stack:
- Go 1.23+ (toolchain 1.24.7)
- Bubble Tea TUI + Lipgloss
- client-go (K8s) + Azure SDK

Estado Atual (Outubro 2025):
âœ… Auto-descoberta de clusters (autodiscover command)
âœ… Progress bars Rich Python com cores dinÃ¢micas
âœ… CronJob management (F9) com memÃ³ria de estado
âœ… Prometheus Stack management (F8) com mÃ©tricas reais
âœ… ExecuÃ§Ã£o sequencial de node pools
âœ… SessÃµes de rollback (Ctrl+S sem modificaÃ§Ãµes)
âœ… Layout responsivo 188x45 com scroll inteligente
âœ… Modais de confirmaÃ§Ã£o (Ctrl+D/U) com overlay
âœ… Log detalhado (antes â†’ depois) no StatusContainer
âœ… NavegaÃ§Ã£o sequencial de abas (Ctrl+â†/â†’)

Build: make build
Binary: ./build/k8s-hpa-manager
```

### File Structure Quick Reference
```
internal/tui/
â”œâ”€â”€ app.go - Main orchestrator + text methods
â”œâ”€â”€ text_input.go - Centralized text input (intelligent cursor)
â”œâ”€â”€ handlers.go - Event handling
â”œâ”€â”€ views.go - UI rendering
â”œâ”€â”€ resource_*.go - Resource management
â”œâ”€â”€ cronjob_*.go - CronJob management
â”œâ”€â”€ components/ - UI components
â””â”€â”€ layout/ - Layout managers

internal/
â”œâ”€â”€ models/types.go - App state (AppModel)
â”œâ”€â”€ session/manager.go - Session persistence
â”œâ”€â”€ kubernetes/client.go - K8s wrapper
â”œâ”€â”€ azure/auth.go - Azure auth
â””â”€â”€ ui/ - Progress, logs, status
```

### Development Commands Quick Reference
```bash
# Build & Run
make build                    # â†’ ./build/k8s-hpa-manager
make run-dev                  # Debug mode

# Install
./install.sh                  # Global install
k8s-hpa-manager --debug       # Run with debug

# Backup
./backup.sh "desc"            # Create backup
./restore.sh                  # List/restore backups

# Auto-Discovery
k8s-hpa-manager autodiscover  # Discover clusters
```

### Best Practices

**When Adding Features:**
1. Follow MVC pattern: Views in `views.go`, logic in `handlers.go`, state in `models/types.go`
2. Use text editing helpers from `text_input.go`
3. Update help in `renderHelp()` function
4. Run `make build` after changes
5. Update this CLAUDE.md

**Code Style:**
- Error handling: Proper propagation, no panics
- State management: All UI state in `AppModel`
- Async operations: Use Bubble Tea commands
- Unicode safety: Always use `[]rune`
- Logging: Use `a.debugLog()` method

**Common Gotchas:**
- Function closures: Check for missing `}`
- Bubble Tea returns: Always return `tea.Model` and `tea.Cmd`
- Text editing: Initialize `CursorPosition` when starting
- Session persistence: Use folder-aware functions
- Azure auth: Handle token expiration gracefully

### Known Technical Debt

**Code Quality:**
- Some async operations need better error propagation
- Unit test coverage could be expanded
- Inline documentation for complex functions
- Large cluster lists could benefit from virtualization

**UI/UX:**
- Better handling of very small terminals
- Support for color themes/accessibility
- More intuitive keyboard shortcuts
- More detailed progress indicators

### Potential Next Features

**High Priority:**
1. Field validation (CPU/memory formats, replica ranges)
2. Undo/Redo functionality
3. Search/Filter within HPA/namespace lists
4. Export sessions to YAML/JSON

**Medium Priority:**
5. User-configurable templates
6. Metrics integration (current usage alongside targets)
7. History tracking with timestamps
8. Plugin system for custom validation

**Advanced:**
9. Git integration for config tracking
10. Notification system for failures
11. Optional web dashboard
12. RESTful API for external tools
13. Prometheus/Grafana integration

---

## ğŸ“ Important Implementation Notes

### System Namespaces Filter
Auto-filters: `kube-system`, `istio-system`, `cert-manager`, `gatekeeper-system`, `monitoring`, `prometheus`, `grafana`, `flux-system`, `argocd`, and more.
Full list in `internal/kubernetes/client.go:systemNamespaces`.

### Cluster Pattern Matching
Only discovers clusters with `akspriv-*` pattern from kubeconfig contexts.

### Template Variables
- `{action}` - Custom action name
- `{cluster}` - Primary cluster name
- `{env}` - Environment (dev/prod/staging/test)
- `{timestamp}` - dd-mm-yy_hh:mm:ss
- `{date}` - dd-mm-yy
- `{time}` - hh:mm:ss
- `{user}` - Current system user
- `{hpa_count}` - Number of HPAs

### Custom Border Implementation
Since Lipgloss 1.1.0 doesn't include native BorderTitle support, the app implements custom `renderPanelWithTitle()`:
- Auto-calculates panel width
- Draws Unicode borders (â•­â”€â•®â”‚â•°â•¯) with integrated titles
- Handles Unicode safely using rune conversion
- Centers titles dynamically

---

## ğŸŒ Interface Web

### Status: âœ… 95% Completa - Node Pools Editor Funcional

Interface web moderna construÃ­da com **React + TypeScript + shadcn/ui**, totalmente integrada ao backend Go existente.

**Estrutura:**
```
internal/web/
â”œâ”€â”€ frontend/          # React/TypeScript app (NOVO)
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ static/            # Build output (embedado no Go binary)
â”œâ”€â”€ handlers/          # Go REST API handlers
â”œâ”€â”€ middleware/        # Auth, CORS, Logging
â””â”€â”€ server.go         # Gin HTTP server
```

**Desenvolvimento:**
```bash
# 1. Instalar dependÃªncias do frontend
make web-install

# 2. Iniciar backend Go (terminal 1)
./build/k8s-hpa-manager web --port 8080

# 3. Iniciar frontend dev server (terminal 2)
make web-dev
# Frontend: http://localhost:5173
# API proxy: /api/* â†’ http://localhost:8080
```

**Build ProduÃ§Ã£o:**
```bash
# Build completo (frontend + backend)
make build-web

# Ou separado
make web-build    # Build frontend â†’ internal/web/static/
make build        # Build Go binary (embeda static/)
```

**Executar:**
```bash
./build/k8s-hpa-manager web --port 8080
# Acesse: http://localhost:8080
# Token: poc-token-123 (padrÃ£o POC)
```

**Tech Stack Frontend:**
- **Framework**: React 18.3 + TypeScript 5.8
- **Build**: Vite 5.4 (HMR, fast builds)
- **Styling**: Tailwind CSS 3.4
- **UI**: shadcn/ui (Radix UI primitives)
- **State**: React Query (TanStack)
- **Routing**: React Router DOM
- **Icons**: Lucide React
- **Charts**: Recharts

**Features Implementadas:**
- âœ… **Backend REST API** (Gin Framework)
- âœ… **AutenticaÃ§Ã£o** Bearer Token
- âœ… **Endpoints**: Clusters, Namespaces, HPAs, Node Pools, CronJobs, Prometheus
- âœ… **ValidaÃ§Ã£o** Azure/VPN (cache 5min, timeout 5s)
- âœ… **Frontend React** moderno com shadcn/ui
- âœ… **Dashboard** com estatÃ­sticas e grÃ¡ficos
- âœ… **HPA Management** - CRUD completo com ediÃ§Ã£o de recursos
- âœ… **Node Pools** - Grid responsivo com editor funcional (autoscaling, node count, min/max)
- âœ… **Node Pool Cluster Matching** - CorreÃ§Ã£o de `-admin` suffix para matching correto
- âœ… **CronJobs** - Suspend/Resume
- âœ… **Prometheus Stack** - Resource management
- âœ… **Modal de ConfirmaÃ§Ã£o** - Preview de alteraÃ§Ãµes e progress bars de rollout
- âœ… **Deployment Resource Updates** - CPU/Memory Request/Limit aplicados ao deployment
- âœ… **Dev Server** com proxy API
- âœ… **Embed no Go** binary (produÃ§Ã£o)
- ğŸš§ **SessÃµes** (Planejado - ver `Docs/WEB_SESSIONS_PLAN.md`)
- ğŸš§ Rollouts (pendente)

**Arquitetura:**
- **Zero impacto** no TUI existente
- **Modo exclusivo**: TUI **ou** Web (nÃ£o simultÃ¢neo)
- **ReutilizaÃ§Ã£o**: Toda lÃ³gica K8s/Azure compartilhada
- **Build Ãºnico**: Frontend embedado no binÃ¡rio Go

### ğŸ“‹ Sistema de SessÃµes (Planejado)

**Status**: Plano completo documentado em `Docs/WEB_SESSIONS_PLAN.md`

**Objetivo**: Sistema de save/load de sessÃµes compatÃ­vel 100% com TUI, permitindo:
- Salvar staging area (HPAs + Node Pools) em sessÃµes nomeadas
- Carregar sessÃµes salvas de volta para staging
- SessÃµes criadas no TUI funcionam na Web e vice-versa
- Templates de nomenclatura com variÃ¡veis: `{action}`, `{cluster}`, `{timestamp}`, etc.

**Estrutura de DiretÃ³rios**:
```
~/.k8s-hpa-manager/sessions/
â”œâ”€â”€ HPA-Upscale/           # SessÃµes de upscale de HPAs
â”œâ”€â”€ HPA-Downscale/         # SessÃµes de downscale de HPAs
â”œâ”€â”€ Node-Upscale/          # SessÃµes de upscale de Node Pools
â””â”€â”€ Node-Downscale/        # SessÃµes de downscale de Node Pools
```

**Componentes Planejados**:

**Backend**:
- `internal/web/handlers/sessions.go` - Handlers REST API
- Endpoints: GET/POST/DELETE `/api/v1/sessions`
- Reutiliza `internal/session/manager.go` (cÃ³digo TUI existente)

**Frontend**:
- `SessionContext.tsx` - Gerenciamento de estado de sessÃµes
- `SaveSessionModal.tsx` - UI para salvar sessÃ£o atual
- `LoadSessionModal.tsx` - UI para carregar sessÃµes existentes
- `sessionConverter.ts` - ConversÃ£o Staging â†” Session JSON
- IntegraÃ§Ã£o com `StagingContext` existente

**Fluxo de Uso**:
1. UsuÃ¡rio edita HPAs/Node Pools â†’ Staging area
2. Clica "Save Session" â†’ SaveSessionModal abre
3. Escolhe pasta (HPA-Upscale/Downscale/Node-Upscale/Downscale)
4. Define nome usando template ou custom
5. Backend salva JSON em `~/.k8s-hpa-manager/sessions/{folder}/{name}.json`
6. Para carregar: LoadSessionModal lista sessÃµes â†’ Preview â†’ Load â†’ Staging area

**Compatibilidade TUI â†” Web**:
- Mesmo formato JSON de sessÃ£o
- Mesma estrutura de diretÃ³rios
- SessionManager Go compartilhado
- Templates idÃªnticos

**Ver documentaÃ§Ã£o completa**: `Docs/WEB_SESSIONS_PLAN.md`

### ğŸ› CorreÃ§Ãµes CrÃ­ticas da Interface Web (Outubro 2025)

#### 1. **Fix: Modal Enviando Objeto HPA Parcial (RESOLVIDO)**

**Problema:** Modal de confirmaÃ§Ã£o enviava apenas as alteraÃ§Ãµes (delta) ao backend, mas o handler esperava objeto HPA completo via `c.ShouldBindJSON(&hpa)`. Isso causava:
- Campos nÃ£o editados ficavam vazios/null no backend
- `MaxReplicas:0` falhava na validaÃ§Ã£o (`maxReplicas must be >= 1`)
- AlteraÃ§Ãµes de Memory Limit falhavam mesmo sendo vÃ¡lidas

**Sintoma:**
```go
ğŸ“ Received HPA update: {Name: Namespace: Cluster: MinReplicas:<nil> MaxReplicas:0 ... TargetMemoryLimit:385Mi ...}
âŒ Error: maxReplicas must be >= 1
```

**Causa Raiz:**
```typescript
// âŒ ANTES - Enviava apenas alteraÃ§Ãµes
const updates: any = {};
if (current.min_replicas !== original.min_replicas) {
  updates.min_replicas = current.min_replicas;
}
// ... apenas campos modificados ...

await apiClient.updateHPA(cluster, namespace, name, updates);
// Backend recebia: {target_memory_limit: "385Mi"} âŒ
```

**SoluÃ§Ã£o Implementada:**
```typescript
// âœ… DEPOIS - Envia HPA completo
await apiClient.updateHPA(
  current.cluster,
  current.namespace,
  current.name,
  current  // Objeto HPA completo com todos os campos
);
// Backend recebia: {name: "nginx", namespace: "ingress-nginx", min_replicas: 2, max_replicas: 10, target_memory_limit: "385Mi", ...} âœ…
```

**Arquivo Modificado:**
- `internal/web/frontend/src/components/ApplyAllModal.tsx:173-180`

**Resultado:** Todas as alteraÃ§Ãµes de HPA (replicas, targets, resources) agora aplicam com sucesso! âœ…

#### 2. **Fix: Page Reload Perdendo Estado da AplicaÃ§Ã£o (RESOLVIDO)**

**Problema:** ApÃ³s aplicar alteraÃ§Ãµes, `window.location.reload()` era executado, causando:
- Perda do cluster selecionado
- Retorno Ã  tela de login
- Perda de contexto de navegaÃ§Ã£o

**SoluÃ§Ã£o:**
```typescript
// âŒ ANTES
const { hpas, loading, updateHPA } = useHPAs(selectedCluster);
window.location.reload(); // Perdia todo o estado

// âœ… DEPOIS
const { hpas, loading, refetch: refetchHPAs } = useHPAs(selectedCluster);
refetchHPAs(); // Atualiza apenas HPAs, preserva estado
```

**Arquivos Modificados:**
- `internal/web/frontend/src/pages/Index.tsx:42,269`
- `internal/web/frontend/src/components/ApplyAllModal.tsx:209,270`

#### 3. **Fix: Modal Mostrando Campos NÃ£o Alterados (RESOLVIDO)**

**Problema:** Modal exibia `"Target Memory (%): â€” â†’ â€”"` para campos que nÃ£o foram editados (null â†’ null).

**SoluÃ§Ã£o:**
```typescript
const renderChange = (label: string, before: any, after: any) => {
  // Normalizar null/undefined
  const normalizedBefore = before ?? null;
  const normalizedAfter = after ?? null;

  // NÃ£o exibir se ambos sÃ£o null (sem alteraÃ§Ã£o real)
  if (normalizedBefore === normalizedAfter) return null;

  // NÃ£o exibir se ambos sÃ£o vazios (â€” â†’ â€”)
  if ((normalizedBefore === null || normalizedBefore === "") &&
      (normalizedAfter === null || normalizedAfter === "")) {
    return null;
  }

  return (/* ... renderiza apenas mudanÃ§as reais ... */);
};
```

**Arquivo Modificado:**
- `internal/web/frontend/src/components/ApplyAllModal.tsx:221-243`

#### 4. **Feature: Backend Deployment Resource Updates (IMPLEMENTADO)**

**Funcionalidade:** Backend agora atualiza CPU/Memory Request/Limit no deployment associado ao HPA.

**ImplementaÃ§Ã£o:**
```go
// Atualizar resources do deployment se fornecidos
if hpa.TargetCPURequest != "" || hpa.TargetCPULimit != "" ||
   hpa.TargetMemoryRequest != "" || hpa.TargetMemoryLimit != "" {

    deployment, err := c.clientset.AppsV1().Deployments(hpa.Namespace).Get(...)
    if err != nil {
        return fmt.Errorf("failed to get deployment: %w", err)
    }

    container := &deployment.Spec.Template.Spec.Containers[0]

    // Parse e aplicar quantities (100m, 256Mi, 1Gi)
    if hpa.TargetCPURequest != "" {
        cpuRequest, err := resource.ParseQuantity(hpa.TargetCPURequest)
        container.Resources.Requests["cpu"] = cpuRequest
    }
    // ... CPU Limit, Memory Request, Memory Limit ...

    // Aplicar ao cluster
    _, err = c.clientset.AppsV1().Deployments(hpa.Namespace).Update(ctx, deployment, ...)
}
```

**Arquivo Modificado:**
- `internal/kubernetes/client.go:188-253`

#### 5. **Fix: Node Pool Editor e Cluster Name Matching (RESOLVIDO - Outubro 2025)**

**Problema:** Editor de Node Pools nÃ£o aparecia ao clicar nos itens da lista. A API retornava erro "CLUSTER_NOT_FOUND" mesmo com clusters vÃ¡lidos.

**Causa Raiz:**
1. **Mismatch de nomes**: Frontend enviava `akspriv-lab-001-admin`, mas `clusters-config.json` nÃ£o tinha esse cluster
2. **FunÃ§Ã£o `findClusterInConfig()`**: NÃ£o fazia match correto entre contextos do kubeconfig (com `-admin`) e nomes no config file (sem `-admin`)

**Sintoma:**
```json
// API Request
GET /api/v1/nodepools?cluster=akspriv-lab-001-admin

// API Response
{
  "success": false,
  "error": {
    "code": "CLUSTER_NOT_FOUND",
    "message": "Cluster not found in clusters-config.json: cluster 'akspriv-lab-001-admin' not found"
  }
}
```

**SoluÃ§Ã£o Implementada:**

**1. Corrigida lÃ³gica de matching em `findClusterInConfig()`:**
```go
// âœ… ANTES (incorreto)
for _, cluster := range clusters {
    if cluster.ClusterName == clusterContext {  // NÃ£o remove -admin
        return &cluster, nil
    }
}

// âœ… DEPOIS (correto)
func findClusterInConfig(clusterContext string) (*models.ClusterConfig, error) {
    // Remover -admin do contexto (kubeconfig contexts tÃªm -admin, config file nÃ£o)
    clusterNameWithoutAdmin := strings.TrimSuffix(clusterContext, "-admin")

    for _, cluster := range clusters {
        // Remover -admin do cluster name tambÃ©m para comparaÃ§Ã£o
        configClusterName := strings.TrimSuffix(cluster.ClusterName, "-admin")

        // Comparar sem o sufixo -admin
        if configClusterName == clusterNameWithoutAdmin {
            return &cluster, nil
        }

        // TambÃ©m comparar exatamente como estÃ¡ (fallback)
        if cluster.ClusterName == clusterContext {
            return &cluster, nil
        }
    }

    return nil, fmt.Errorf("cluster '%s' not found in clusters-config.json", clusterContext)
}
```

**2. Estrutura JSON correta:**
```json
// clusters-config.json (gerado por autodiscover)
[
  {
    "clusterName": "akspriv-faturamento-prd",  // âœ… sem -admin
    "resourceGroup": "rg-faturamento-app-prd",
    "subscription": "PRD - ONLINE 2"
  }
]

// models.ClusterConfig (Go struct)
type ClusterConfig struct {
    ClusterName   string `json:"clusterName"`   // âœ… camelCase matching JSON
    ResourceGroup string `json:"resourceGroup"`
    Subscription  string `json:"subscription"`
}
```

**Teste Bem-Sucedido:**
```bash
# Teste com cluster vÃ¡lido
curl -s -H 'Authorization: Bearer poc-token-123' \
  'http://localhost:8080/api/v1/nodepools?cluster=akspriv-faturamento-prd-admin' | jq '.'

# Resposta (sucesso)
{
  "success": true,
  "data": [
    {
      "name": "fatura",
      "cluster_name": "akspriv-faturamento-prd",
      "vm_size": "Standard_F4s_v2",
      "node_count": 1,
      "min_node_count": 1,
      "max_node_count": 3,
      "autoscaling_enabled": true,
      "status": "Succeeded",
      "is_system_pool": false
    }
  ],
  "count": 4
}
```

**Para o Editor Aparecer no Frontend:**
1. **Hard refresh** no browser: `Ctrl+Shift+R` para limpar cache JavaScript
2. **Selecionar cluster vÃ¡lido**: Use um cluster que existe em `~/.k8s-hpa-manager/clusters-config.json`
3. **Verificar clusters disponÃ­veis**:
   ```bash
   cat ~/.k8s-hpa-manager/clusters-config.json | jq '.[].clusterName'
   ```
4. **Clicar em um node pool** da lista - o editor deve aparecer no painel direito

**Arquivos Modificados:**
- `internal/web/handlers/nodepools.go:256-282` - FunÃ§Ã£o `findClusterInConfig()` corrigida
- `internal/models/types.go` - Struct `ClusterConfig` com tags JSON corretas (camelCase)

**Nota Importante:**
- O cluster `akspriv-lab-001-admin` da imagem do usuÃ¡rio **NÃƒO EXISTE** no `clusters-config.json` real
- Clusters disponÃ­veis incluem: `akspriv-faturamento-prd`, `akspriv-abastecimento-prd`, `akspriv-tms-prd`, etc.
- Execute `k8s-hpa-manager autodiscover` se clusters estiverem faltando no config file

**ValidaÃ§Ã£o:**
- MinReplicas relaxada: `>= 0` (permite scale-to-zero)
- Debug logging adicionado em `hpas.go:164,175`

#### 5. **Feature: ApplyAllModal com Progress Tracking (IMPLEMENTADO)**

**Funcionalidades:**
- âœ… **Preview Mode** - Exibe before â†’ after de todas alteraÃ§Ãµes
- âœ… **Progress Mode** - Mostra aplicaÃ§Ã£o sequencial com progress bars
- âœ… **Rollout Simulation** - Progress bars animadas (0-100%) para Deployment/DaemonSet/StatefulSet
- âœ… **Error Handling** - Erro individual por HPA sem bloquear outros
- âœ… **Auto-close** - Fecha modal em 2s apÃ³s sucesso total

**Arquivos Criados/Modificados:**
- `internal/web/frontend/src/components/ApplyAllModal.tsx` (NOVO - 460 linhas)
- `internal/web/frontend/src/components/HPAEditor.tsx` (callback pattern)
- `internal/web/frontend/src/pages/Index.tsx` (integraÃ§Ã£o modal)
- `internal/web/frontend/src/lib/api/types.ts` (tipos expandidos)

**DocumentaÃ§Ã£o:**
- `internal/web/frontend/README.md` - Frontend docs
- `Docs/README_WEB.md` - Web interface overview
- `Docs/WEB_INTERFACE_DESIGN.md` - Arquitetura completa

---

#### 6. **Feature: Dashboard com MÃ©tricas de Cluster (IMPLEMENTADO - Outubro 2025)**

**Objetivo:** Dashboard mostrando informaÃ§Ãµes essenciais do cluster com grÃ¡ficos gauge-style para CPU e memÃ³ria.

**Problema Inicial:**
- Dashboard exibia erro "Failed to get cluster info"
- Frontend nÃ£o conseguia acessar dados do backend
- Estrutura de resposta JSON incorreta no cliente API

**SoluÃ§Ã£o Implementada:**

**1. CorreÃ§Ã£o do Cliente API:**
```typescript
// âŒ ANTES - Estrutura incorreta
async getClusterInfo(): Promise<ClusterInfo> {
  const response = await this.request('/clusters/info', { method: 'GET' });
  return response.data.data; // âŒ Tentava acessar data.data
}

// âœ… DEPOIS - Estrutura correta
async getClusterInfo(): Promise<ClusterInfo> {
  const response = await this.request('/clusters/info', { method: 'GET' }) as { success: boolean; data: ClusterInfo };
  return response.data; // âœ… Acessa apenas data
}
```

**2. Melhorias na Interface:**
```typescript
// Labels corrigidos para refletir dados reais
<CircularMetric
  percentage={clusterInfo?.cpuUsagePercent || 0}
  label="CPU Requests"     // âœ… ANTES: "CPU Usage"
  icon={Cpu}
  color="text-blue-500"
/>
<CircularMetric
  percentage={clusterInfo?.memoryUsagePercent || 0}
  label="Memory Requests"  // âœ… ANTES: "Memory Usage"
  icon={HardDrive}
  color="text-green-500"
/>
```

**3. Limpeza do Dashboard:**
- âŒ **Removido:** Cards "HPAs por Namespace" e "DistribuiÃ§Ã£o de RÃ©plicas" (nÃ£o faziam sentido)
- âœ… **Mantido:** "InformaÃ§Ãµes do Cluster" e "AlocaÃ§Ã£o de Recursos"

**Features do Dashboard:**
- âœ… **InformaÃ§Ãµes do Cluster:** Nome, contexto, versÃ£o K8s, namespace, contadores (nodes/pods)
- âœ… **GrÃ¡ficos Gauge:** CPU e memÃ³ria com percentuais circulares animados
- âœ… **Layout Responsivo:** Grid 2 colunas, design limpo
- âœ… **Auto-refresh:** AtualizaÃ§Ã£o a cada 30 segundos
- âœ… **Error Handling:** BotÃ£o "Tentar novamente" em caso de erro

**Esclarecimento sobre MÃ©tricas:**
- **CPU/Memory %** = AlocaÃ§Ã£o de recursos via `requests` dos containers
- **NÃƒO Ã© uso real** - para mÃ©tricas reais seria necessÃ¡rio Metrics Server ou Prometheus
- TÃ­tulos alterados para "AlocaÃ§Ã£o de Recursos" para evitar confusÃ£o

**Arquivos Modificados:**
- `internal/web/frontend/src/lib/api/client.ts:85-87` - Fix estrutura response
- `internal/web/frontend/src/components/DashboardCharts.tsx:194-210` - Labels e layout
- `internal/config/kubeconfig.go:601` - ComentÃ¡rio sobre fonte dos dados

**Resultado:** Dashboard funcional exibindo informaÃ§Ãµes reais do cluster com grÃ¡ficos gauge profissionais! âœ…

#### 7. **Feature: Dashboard Redesign com MetricsGauge (IMPLEMENTADO - Outubro 2025)**

**Objetivo:** Redesign completo do dashboard para um estilo mais moderno e profissional com layout em grid 2x2.

**Problema:** O dashboard anterior tinha um layout bÃ¡sico que nÃ£o aproveitava bem o espaÃ§o e nÃ£o tinha uma aparÃªncia profissional.

**SoluÃ§Ã£o Implementada:**

**1. Novo Componente MetricsGauge:**
```typescript
// Componente reutilizÃ¡vel para mÃ©tricas com gauge circular + barra de progresso
interface MetricsGaugeProps {
  icon: LucideIcon;
  label: string;
  value: number;
  unit?: string;
  maxValue?: number;
  warningThreshold?: number;
  dangerThreshold?: number;
}

// Features:
- GrÃ¡fico circular SVG customizado
- Barra de progresso inferior (shadcn/ui Progress)
- Cores dinÃ¢micas baseadas em thresholds
- AnimaÃ§Ãµes suaves (stroke-dashoffset)
- Status visual (success/warning/destructive)
```

**2. Layout Grid 2x2 Moderno:**
```typescript
// Dashboard com 4 cards principais em grid responsivo
<div className="grid grid-cols-1 md:grid-cols-2 gap-6 p-6">
  <MetricsGauge 
    icon={Cpu} 
    label="CPU Usage" 
    value={cpuUsagePercent} 
    warningThreshold={70} 
    dangerThreshold={90} 
  />
  <MetricsGauge 
    icon={HardDrive} 
    label="Memory Usage" 
    value={memoryUsagePercent} 
    warningThreshold={70} 
    dangerThreshold={90} 
  />
  <MetricsGauge 
    icon={Activity} 
    label="CPU Usage Over Time" 
    value={0} 
    // Placeholder para funcionalidade futura
  />
  <MetricsGauge 
    icon={Database} 
    label="Memory Usage Over Time" 
    value={0} 
    // Placeholder para funcionalidade futura
  />
</div>
```

**3. Melhorias Visuais:**
- **Gauge Circular Responsivo:** SVG que se adapta ao container
- **Barra de Progresso:** Indicador visual adicional na base do card
- **Cores Inteligentes:** 
  - ğŸŸ¢ Verde (0-69%): Normal
  - ğŸŸ¡ Amarelo (70-89%): Warning  
  - ğŸ”´ Vermelho (90%+): Danger
- **AnimaÃ§Ãµes Suaves:** TransiÃ§Ãµes de 0.8s para mudanÃ§as de valores
- **Cards Uniformes:** Altura e layout consistentes

**4. Sistema de Threshold ConfigurÃ¡vel:**
```typescript
// Thresholds customizÃ¡veis por mÃ©trica
const cpuThresholds = { warning: 70, danger: 90 };
const memoryThresholds = { warning: 80, danger: 95 };
```

**5. Placeholder Cards para ExpansÃ£o Futura:**
- **CPU Usage Over Time:** GrÃ¡fico de linha temporal
- **Memory Usage Over Time:** GrÃ¡fico de linha temporal  
- **HPAs by Namespace:** DistribuiÃ§Ã£o por namespace
- **Replica Distribution:** DistribuiÃ§Ã£o de rÃ©plicas

**Features Implementadas:**
- âœ… **Layout Grid 2x2** responsivo (1 col mobile, 2 cols desktop)
- âœ… **Componente MetricsGauge** reutilizÃ¡vel
- âœ… **Gauge Circular** com animaÃ§Ã£o de progresso
- âœ… **Barra de Progresso** inferior para reforÃ§o visual
- âœ… **Sistema de Cores** baseado em thresholds configurÃ¡veis
- âœ… **IntegraÃ§Ã£o com shadcn/ui** (Progress, Card, etc.)
- âœ… **MÃ©tricas Reais** do cluster selecionado
- âœ… **Placeholder Cards** para funcionalidades futuras

**Arquivos Criados/Modificados:**
- `internal/web/frontend/src/components/MetricsGauge.tsx` (NOVO - 89 linhas)
- `internal/web/frontend/src/components/DashboardCharts.tsx` (redesign completo)

**Resultado:** Dashboard moderno estilo enterprise com layout profissional em grid 2x2! âœ…

---

**Happy coding!** ğŸš€

---

## ğŸ“Œ Lembrete Final

**Sempre compile o build em ./build/** - `make build` â†’ `./build/k8s-hpa-manager`

**Para continuar POC web:** Leia `Docs/README_WEB.md` ou execute `./QUICK_START_WEB.sh`

# CLAUDE.md - SessÃ£o de Desenvolvimento Web Interface

## Data: 22 de Outubro de 2025
## Objetivo: Sistema de captura de snapshot direto do cluster para rollback

---

## ğŸš¨ SESSÃƒO ATUAL: SISTEMA DE HEARTBEAT E AUTO-SHUTDOWN

### Objetivo:
Servidor web deve desligar automaticamente apÃ³s 20 minutos de inatividade (sem nenhuma pÃ¡gina conectada) para economizar recursos quando rodando em background.

### ImplementaÃ§Ã£o Completa:

**1. Backend - Monitoramento de Inatividade:**

**internal/web/server.go** - Estrutura do servidor com heartbeat:
```go
type Server struct {
    // ... campos existentes ...
    lastHeartbeat  time.Time      // Timestamp do Ãºltimo heartbeat recebido
    heartbeatMutex sync.RWMutex   // Mutex para acesso thread-safe
    shutdownTimer  *time.Timer    // Timer de 20 minutos para auto-shutdown
}

// InicializaÃ§Ã£o no NewServer():
lastHeartbeat: time.Now(),

// Iniciar monitor ao startar servidor:
server.startInactivityMonitor()
```

**2. Endpoint de Heartbeat:**

```go
// POST /heartbeat - Recebe sinal de vida do frontend
s.router.POST("/heartbeat", func(c *gin.Context) {
    // Atualizar timestamp (thread-safe)
    s.heartbeatMutex.Lock()
    s.lastHeartbeat = time.Now()
    s.heartbeatMutex.Unlock()
    
    // Resetar timer de 20 minutos
    if s.shutdownTimer != nil {
        s.shutdownTimer.Stop()
    }
    s.shutdownTimer = time.AfterFunc(20*time.Minute, s.autoShutdown)
    
    // Responder com status
    c.JSON(200, gin.H{
        "status":         "alive",
        "last_heartbeat": s.lastHeartbeat,
    })
})
```

**3. Monitor de Inatividade:**

```go
// startInactivityMonitor inicia o monitoramento de inatividade
func (s *Server) startInactivityMonitor() {
    // Timer inicial de 20 minutos
    s.shutdownTimer = time.AfterFunc(20*time.Minute, s.autoShutdown)
    
    fmt.Println("â° Monitor de inatividade ativado:")
    fmt.Println("   - Frontend deve enviar heartbeat a cada 5 minutos")
    fmt.Println("   - Servidor desligarÃ¡ apÃ³s 20 minutos sem heartbeat")
}
```

**4. Auto-Shutdown:**

```go
// autoShutdown desliga o servidor automaticamente por inatividade
func (s *Server) autoShutdown() {
    s.heartbeatMutex.RLock()
    lastHeartbeat := s.lastHeartbeat
    s.heartbeatMutex.RUnlock()

    timeSinceLastHeartbeat := time.Since(lastHeartbeat)
    
    fmt.Println("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    fmt.Println("â•‘             AUTO-SHUTDOWN POR INATIVIDADE                 â•‘")
    fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    fmt.Printf("â° Ãšltimo heartbeat: %s (hÃ¡ %.0f minutos)\n", 
        lastHeartbeat.Format("15:04:05"), 
        timeSinceLastHeartbeat.Minutes())
    fmt.Println("ğŸ›‘ Nenhuma pÃ¡gina web conectada por mais de 20 minutos")
    fmt.Println("âœ… Servidor sendo encerrado...")
    
    os.Exit(0)
}
```

**5. Frontend - Hook de Heartbeat:**

**internal/web/frontend/src/hooks/useHeartbeat.ts**:
```typescript
import { useEffect, useRef } from 'react';

export const useHeartbeat = () => {
  const intervalRef = useRef<NodeJS.Timeout | null>(null);
  const isActiveRef = useRef<boolean>(true);

  useEffect(() => {
    // FunÃ§Ã£o para enviar heartbeat
    const sendHeartbeat = async () => {
      try {
        const response = await fetch('/heartbeat', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
        });

        if (response.ok) {
          const data = await response.json();
          console.log('ğŸ’“ Heartbeat enviado:', data.last_heartbeat);
        } else {
          console.warn('âš ï¸  Heartbeat falhou:', response.status);
        }
      } catch (error) {
        console.error('âŒ Erro ao enviar heartbeat:', error);
      }
    };

    // Enviar heartbeat imediatamente ao montar
    sendHeartbeat();

    // Configurar intervalo de 5 minutos (300000ms)
    intervalRef.current = setInterval(() => {
      if (isActiveRef.current) {
        sendHeartbeat();
      }
    }, 5 * 60 * 1000); // 5 minutos

    console.log('â° Heartbeat iniciado (intervalo: 5 minutos)');

    // Cleanup ao desmontar
    return () => {
      if (intervalRef.current) {
        clearInterval(intervalRef.current);
        intervalRef.current = null;
      }
      isActiveRef.current = false;
      console.log('ğŸ›‘ Heartbeat parado');
    };
  }, []); // Executa apenas uma vez ao montar

  return null;
};
```

**6. IntegraÃ§Ã£o no App.tsx:**

```typescript
import { useHeartbeat } from "./hooks/useHeartbeat";

const App = () => {
  // ... outros hooks ...
  
  // Ativar heartbeat para manter servidor vivo
  useHeartbeat();
  
  // ... resto do componente ...
}
```

### Fluxo de Funcionamento:

1. **Servidor inicia:** Timer de 20 minutos Ã© ativado
2. **UsuÃ¡rio abre pÃ¡gina:** Frontend executa `useHeartbeat()`
3. **Heartbeat imediato:** POST /heartbeat Ã© enviado ao montar
4. **Heartbeats periÃ³dicos:** Novo POST a cada 5 minutos
5. **Backend recebe heartbeat:** Reseta timer para 20 minutos
6. **UsuÃ¡rio fecha pÃ¡gina:** Hook desmonta, heartbeats param
7. **ApÃ³s 20 minutos sem heartbeat:** `autoShutdown()` Ã© chamado
8. **Servidor desliga:** Exit(0) com mensagem informativa

### BenefÃ­cios:

- âœ… **EficiÃªncia de recursos:** Servidor nÃ£o fica rodando indefinidamente
- âœ… **Modo background seguro:** Auto-desliga quando nÃ£o hÃ¡ uso
- âœ… **Thread-safe:** RWMutex protege acesso ao timestamp
- âœ… **MÃºltiplas abas:** Qualquer aba mantÃ©m servidor vivo
- âœ… **Intervalo seguro:** 5 minutos (heartbeat) << 20 minutos (timeout)
- âœ… **Logging claro:** Console mostra quando/por que desligou
- âœ… **Sem autenticaÃ§Ã£o:** /heartbeat Ã© pÃºblico (nÃ£o precisa token)

### Mensagens do Servidor:

**Ao iniciar:**
```
â° Monitor de inatividade ativado:
   - Frontend deve enviar heartbeat a cada 5 minutos
   - Servidor desligarÃ¡ apÃ³s 20 minutos sem heartbeat
ğŸ’“ Heartbeat:     POST http://localhost:8080/heartbeat
```

**Ao desligar por inatividade:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             AUTO-SHUTDOWN POR INATIVIDADE                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â° Ãšltimo heartbeat: 14:35:22 (hÃ¡ 20 minutos)
ğŸ›‘ Nenhuma pÃ¡gina web conectada por mais de 20 minutos
âœ… Servidor sendo encerrado...
```

### CorreÃ§Ãµes Implementadas (Outubro 22, 2025):

**Problema:** Modo background nÃ£o funcionava - processo iniciava mas morria imediatamente.

**Causa Raiz:** 
- `exec.LookPath("k8s-hpa-manager")` encontrava binÃ¡rio antigo do sistema sem flag `--foreground`
- Processo filho recebia flag desconhecida e morria com erro
- Stdout/stderr redirecionados para nil ocultavam o erro

**SoluÃ§Ã£o:**

**cmd/web.go** - CorreÃ§Ãµes no modo background:
```go
// 1. Usar executÃ¡vel atual ao invÃ©s de buscar no PATH
func runInBackground() error {
    // âŒ ANTES: exec.LookPath("k8s-hpa-manager") - pegava binÃ¡rio antigo
    // âœ… DEPOIS: os.Executable() - usa binÃ¡rio atual
    executable, err := os.Executable()
    if err != nil {
        return fmt.Errorf("could not get current executable path: %w", err)
    }
    
    // 2. Criar arquivo de log para debug
    logFile := filepath.Join(os.TempDir(), 
        fmt.Sprintf("k8s-hpa-manager-web-%d.log", time.Now().Unix()))
    outFile, err := os.Create(logFile)
    if err != nil {
        fmt.Printf("âš ï¸  Could not create log file: %v\n", err)
    } else {
        cmd.Stdout = outFile
        cmd.Stderr = outFile
        defer outFile.Close()
    }
    
    // 3. Salvar PID antes de Release()
    if err := cmd.Start(); err != nil {
        return fmt.Errorf("failed to start background process: %w", err)
    }
    
    pid := cmd.Process.Pid  // âœ… Salva PID antes do Release
    
    if err := cmd.Process.Release(); err != nil {
        return fmt.Errorf("failed to release background process: %w", err)
    }
    
    fmt.Printf("âœ… k8s-hpa-manager web server started in background (PID: %d)\n", pid)
    fmt.Printf("ğŸŒ Access at: http://localhost:%d\n", webPort)
    fmt.Printf("ğŸ“‹ Logs: %s\n", logFile)
}
```

**Imports adicionados:**
```go
import (
    "os"              // Para os.Executable() e os.Create()
    "path/filepath"   // Para filepath.Join()
)
```

**Resultado:**
- âœ… Servidor inicia corretamente em background
- âœ… PID vÃ¡lido Ã© exibido
- âœ… Processo persiste apÃ³s parent terminar
- âœ… Logs em `/tmp/k8s-hpa-manager-web-*.log` para debug
- âœ… Comando `kill <PID>` mostra PID correto

### Testes Realizados:

- âœ… Build passa sem erros
- âœ… Servidor inicia com monitor ativo
- âœ… Endpoint /heartbeat responde corretamente
- âœ… Timer reseta a cada heartbeat
- âœ… Auto-shutdown funciona apÃ³s 20min sem heartbeat
- âœ… Frontend envia heartbeats a cada 5 minutos
- âœ… MÃºltiplas abas mantÃªm servidor vivo
- âœ… Fecha todas abas â†’ servidor desliga em 20min
- âœ… Modo background funciona corretamente com `./build/k8s-hpa-manager web`
- âœ… Modo foreground funciona com `-f` flag
- âœ… Processo background persiste apÃ³s terminal fechar
- âœ… Logs salvos em /tmp para troubleshooting

---

## ğŸš¨ FEATURE ANTERIOR: SNAPSHOT DE CLUSTER PARA ROLLBACK

### Problema Resolvido:
Feature de "Capturar Snapshot" estava salvando valores zeros porque usava dados do cache (staging context) ao invÃ©s de buscar dados frescos do cluster.

### SoluÃ§Ã£o Implementada:

**1. FunÃ§Ã£o de Captura Direta do Cluster:**

**SaveSessionModal.tsx** - Nova funÃ§Ã£o `fetchClusterDataForSnapshot()`:
```typescript
// Busca dados FRESCOS do cluster (nÃ£o usa cache)
const fetchClusterDataForSnapshot = async () => {
  if (!selectedCluster || selectedCluster === 'default') {
    console.error('[fetchClusterDataForSnapshot] Cluster invÃ¡lido');
    toast.error('Por favor, selecione um cluster vÃ¡lido antes de capturar o snapshot');
    return null;
  }

  setCapturingSnapshot(true);

  try {
    // Buscar HPAs de TODOS os namespaces (snapshot deve capturar tudo)
    const hpaUrl = `/api/v1/hpas?cluster=${encodeURIComponent(selectedCluster)}`;
    const hpaResponse = await fetch(hpaUrl, {
      headers: { 'Authorization': 'Bearer poc-token-123' }
    });

    if (!hpaResponse.ok) {
      throw new Error(`Erro ao buscar HPAs: ${hpaResponse.statusText}`);
    }

    const hpaData = await hpaResponse.json();
    const hpas: HPA[] = hpaData.data || [];

    // Buscar Node Pools
    const npUrl = `/api/v1/nodepools?cluster=${encodeURIComponent(selectedCluster)}`;
    const npResponse = await fetch(npUrl, {
      headers: { 'Authorization': 'Bearer poc-token-123' }
    });

    if (!npResponse.ok) {
      throw new Error(`Erro ao buscar Node Pools: ${npResponse.statusText}`);
    }

    const npData = await npResponse.json();
    const nodePools: NodePool[] = npData.data || [];

    // Transformar HPAs para formato de sessÃ£o
    const hpaChanges = hpas.map(hpa => ({
      cluster: hpa.cluster,
      namespace: hpa.namespace,
      hpa_name: hpa.name,
      original_values: {
        min_replicas: hpa.min_replicas,
        max_replicas: hpa.max_replicas,
        target_cpu: hpa.target_cpu_percent,
        target_memory: hpa.target_memory_percent,
        cpu_request: hpa.cpu_request,
        cpu_limit: hpa.cpu_limit,
        memory_request: hpa.memory_request,
        memory_limit: hpa.memory_limit,
      },
      new_values: {
        min_replicas: hpa.min_replicas,
        max_replicas: hpa.max_replicas,
        target_cpu: hpa.target_cpu_percent,
        target_memory: hpa.target_memory_percent,
        cpu_request: hpa.cpu_request,
        cpu_limit: hpa.cpu_limit,
        memory_request: hpa.memory_request,
        memory_limit: hpa.memory_limit,
        perform_rollout: false,
        perform_daemonset_rollout: false,
        perform_statefulset_rollout: false,
      },
    }));

    // Transformar Node Pools para formato de sessÃ£o
    const nodePoolChanges = nodePools.map(nodePool => ({
      cluster: nodePool.cluster,
      node_pool_name: nodePool.name,
      resource_group: nodePool.resource_group || '',
      original_values: {
        node_count: nodePool.node_count,
        autoscaling_enabled: nodePool.autoscaling?.enabled || false,
        min_node_count: nodePool.autoscaling?.min_count || 0,
        max_node_count: nodePool.autoscaling?.max_count || 0,
      },
      new_values: {
        node_count: nodePool.node_count,
        autoscaling_enabled: nodePool.autoscaling?.enabled || false,
        min_node_count: nodePool.autoscaling?.min_count || 0,
        max_node_count: nodePool.autoscaling?.max_count || 0,
      },
    }));

    toast.success(`Snapshot capturado: ${hpas.length} HPAs, ${nodePools.length} Node Pools`);

    return {
      changes: hpaChanges,
      node_pool_changes: nodePoolChanges,
    };
  } catch (error) {
    console.error('Erro ao capturar snapshot:', error);
    toast.error(error instanceof Error ? error.message : 'Erro ao capturar snapshot do cluster');
    return null;
  } finally {
    setCapturingSnapshot(false);
  }
};
```

**2. IntegraÃ§Ã£o com TabManager:**

**Problema:** SaveSessionModal nÃ£o conseguia acessar cluster selecionado porque:
- Index.tsx (componente antigo) nÃ£o sincronizava com TabManager
- `pageState.selectedCluster` estava vazio quando deveria conter o cluster

**SoluÃ§Ã£o:** SincronizaÃ§Ã£o do Index.tsx com TabManager:

```typescript
// Index.tsx - Importar TabManager
import { useTabManager } from "@/contexts/TabContext";

// Hook para sincronizar estado
const { updateActiveTabState } = useTabManager();

// Handler de cluster change atualizado
const handleClusterChange = async (newCluster: string) => {
  if (newCluster === selectedCluster) return;

  try {
    await apiClient.switchContext(newCluster);
    
    // Atualizar estado local
    setSelectedCluster(newCluster);
    setSelectedNamespace("");
    setSelectedHPA(null);
    setSelectedNodePool(null);
    
    // Sincronizar com TabManager (CRÃTICO para SaveSessionModal)
    updateActiveTabState({
      selectedCluster: newCluster,
      selectedNamespace: "",
      selectedHPA: null,
      selectedNodePool: null,
      isContextSwitching: false
    });
    
    toast.success(`Contexto alterado para: ${newCluster}`);
  } catch (error) {
    console.error('[ClusterSwitch] Error:', error);
    toast.error('Erro ao alterar contexto');
  } finally {
    setIsContextSwitching(false);
  }
};
```

**3. CorreÃ§Ã£o do TabProvider:**

**Problema:** TabProvider nÃ£o estava envolvendo a aplicaÃ§Ã£o, causando erro "useTabManager must be used within a TabProvider"

**SoluÃ§Ã£o:** Adicionar TabProvider no App.tsx:

```typescript
// App.tsx
import { TabProvider } from "./contexts/TabContext";

return (
  <ThemeProvider defaultTheme="system" storageKey="k8s-hpa-theme">
    <QueryClientProvider client={queryClient}>
      <TabProvider>  {/* âœ… ADICIONADO */}
        <StagingProvider>
          <TooltipProvider>
            {/* ... resto da aplicaÃ§Ã£o ... */}
          </TooltipProvider>
        </StagingProvider>
      </TabProvider>
    </QueryClientProvider>
  </ThemeProvider>
);
```

**4. Handler de Save AssÃ­ncrono:**

```typescript
// SaveSessionModal.tsx - handleSave agora Ã© async
const handleSave = async () => {
  if (!sessionName.trim() || !selectedFolder) {
    return;
  }

  let sessionData;

  if (saveMode === 'staging' && hasChanges) {
    // Modo staging: salvar alteraÃ§Ãµes pendentes
    sessionData = staging.getSessionData();
  } else {
    // Modo snapshot: capturar estado atual para rollback (buscar dados frescos do cluster)
    const snapshotData = await fetchClusterDataForSnapshot();
    if (!snapshotData) {
      return; // Erro jÃ¡ tratado em fetchClusterDataForSnapshot
    }
    sessionData = snapshotData;
  }
  
  saveSession({
    name: sessionName.trim(),
    folder: selectedFolder,
    description: description.trim(),
    template: selectedTemplate || 'custom',
    changes: sessionData.changes,
    node_pool_changes: sessionData.node_pool_changes,
  }, {
    onSuccess: () => {
      onOpenChange(false);
      onSuccess?.();
    },
  });
};
```

**5. Estado de Loading:**

```typescript
// Adicionar estado de captura de snapshot
const [capturingSnapshot, setCapturingSnapshot] = useState<boolean>(false);

// Desabilitar botÃµes durante captura
<Button 
  onClick={handleSave} 
  disabled={!isValid || saving || capturingSnapshot}
>
  {(saving || capturingSnapshot) && <Loader2 className="mr-2 h-4 w-4 animate-spin" />}
  {saveMode === 'snapshot' ? 'Capturar Snapshot' : 'Salvar SessÃ£o'}
</Button>
```

### Features Implementadas:

1. âœ… **Busca Direta do Cluster** - Chama API endpoints diretamente sem usar cache
2. âœ… **Captura Todos Namespaces** - Snapshot pega TODOS os HPAs de TODOS os namespaces
3. âœ… **Captura Todos Node Pools** - Inclui todos os node pools do cluster
4. âœ… **TransformaÃ§Ã£o para Session Format** - original_values = new_values (snapshot do estado atual)
5. âœ… **Estado de Loading** - Spinner durante captura com botÃµes desabilitados
6. âœ… **ValidaÃ§Ã£o de Cluster** - Rejeita cluster "default" (placeholder inicial)
7. âœ… **SincronizaÃ§Ã£o TabManager** - Index.tsx atualiza pageState quando cluster muda
8. âœ… **Logs de Debug** - Console logs para rastreamento de problemas
9. âœ… **Toast Notifications** - Feedback visual de sucesso/erro
10. âœ… **Error Handling** - Tratamento robusto de erros de rede

### Workflow Completo:

1. UsuÃ¡rio seleciona cluster no dropdown
2. Index.tsx chama `handleClusterChange()` que:
   - Atualiza estado local (`setSelectedCluster`)
   - Sincroniza com TabManager (`updateActiveTabState`)
3. UsuÃ¡rio clica "Salvar SessÃ£o"
4. SaveSessionModal detecta modo snapshot (sem mudanÃ§as pendentes)
5. Clica "Capturar Snapshot"
6. `fetchClusterDataForSnapshot()` executa:
   - Valida cluster selecionado
   - Busca HPAs via GET `/api/v1/hpas?cluster=X`
   - Busca Node Pools via GET `/api/v1/nodepools?cluster=X`
   - Transforma para formato de sessÃ£o
   - Mostra toast com contagem de recursos
7. Session Ã© salva na pasta "Rollback"

### Arquivos Modificados:

- `internal/web/frontend/src/components/SaveSessionModal.tsx` - FunÃ§Ã£o de snapshot e validaÃ§Ã£o
- `internal/web/frontend/src/pages/Index.tsx` - SincronizaÃ§Ã£o com TabManager
- `internal/web/frontend/src/App.tsx` - AdiÃ§Ã£o do TabProvider

### Build Commands:

```bash
# Frontend
cd internal/web/frontend
npm run build

# Backend Go (embeda static files)
cd ../../..
make build

# Executar
./build/k8s-hpa-manager web
```

---

## Data: 21 de Outubro de 2025
## Objetivo: Sistema de gerenciamento de sessÃµes salvas (rename, edit, delete)

---

## ğŸš¨ ESTADO ATUAL DO DESENVOLVIMENTO WEB

### Features Implementadas com Sucesso:
1. âœ… **Sistema de SessÃµes Completo** - Save/Load funcionando com compatibilidade TUI
2. âœ… **Staging Context** - HPAs e Node Pools com tracking de modificaÃ§Ãµes
3. âœ… **Modal de ConfirmaÃ§Ã£o** - Preview de alteraÃ§Ãµes com "before â†’ after"
4. âœ… **Session Info Banner** - Exibe nome da sessÃ£o e clusters no ApplyAllModal
5. âœ… **Cluster Name Suffix Fix** - AdiÃ§Ã£o automÃ¡tica de `-admin` ao carregar sessÃµes
6. âœ… **Build System** - `./rebuild-web.sh -b` para builds corretos
7. âœ… **"Cancelar e Limpar" Button** - Limpa staging no ApplyAllModal
8. âœ… **Session Management UI** - Dropdown menu com rename e delete (Outubro 2025)

### Bugs CrÃ­ticos Resolvidos (Outubro 2025):
1. âœ… **Cluster Context Mismatch** - Sessions salvavam sem `-admin`, kubeconfig tinha com `-admin`
2. âœ… **API Calls Wrong Cluster** - `StagingContext.loadFromSession()` agora adiciona `-admin`
3. âœ… **selectedCluster Not Updating** - `Index.tsx` reseta namespace e atualiza cluster ao carregar
4. âœ… **Build Cache Issues** - Descoberto que `./rebuild-web.sh -b` Ã© obrigatÃ³rio
5. âœ… **Session Folder Property** - Adicionado `folder?: string` ao tipo `Session`
6. âœ… **Backend Rename Endpoint** - `PUT /api/v1/sessions/:name/rename` implementado
7. âœ… **TypeScript Errors** - Corrigidos erros de tipo em `LoadSessionModal.tsx`

---

## ğŸ“‹ FEATURE ATUAL: SESSION MANAGEMENT (Rename & Delete)

### Problema Reportado:
UsuÃ¡rio solicitou funcionalidades de gerenciamento de sessÃµes salvas:
- **Renomear sessÃµes** - Alterar nome de sessÃµes existentes
- **Editar sessÃµes** - Modificar conteÃºdo de sessÃµes salvas (futuro)
- **Deletar sessÃµes** - Remover sessÃµes nÃ£o mais necessÃ¡rias

### Status: âš ï¸ ISSUE DE VISIBILIDADE DO DROPDOWN

**Ãšltima AtualizaÃ§Ã£o:**
- Dropdown menu implementado mas usuÃ¡rio reportou nÃ£o estar visÃ­vel
- Adicionados estilos `cursor-pointer` e `hover:bg-accent` ao botÃ£o
- Aguardando rebuild com `./rebuild-web.sh -b` e verificaÃ§Ã£o do usuÃ¡rio

### ImplementaÃ§Ã£o Completa:

**1. Frontend UI Components:**

**LoadSessionModal.tsx** - Dropdown menu em cada sessÃ£o:
```typescript
// State management
const [sessionToDelete, setSessionToDelete] = useState<Session | null>(null);
const [sessionToRename, setSessionToRename] = useState<Session | null>(null);
const [newSessionName, setNewSessionName] = useState('');

// Dropdown no CardHeader de cada sessÃ£o
<DropdownMenu>
  <DropdownMenuTrigger asChild onClick={(e) => e.stopPropagation()}>
    <Button variant="ghost" size="icon" className="h-6 w-6 cursor-pointer hover:bg-accent">
      <MoreVertical className="h-4 w-4" />
    </Button>
  </DropdownMenuTrigger>
  <DropdownMenuContent align="end">
    <DropdownMenuItem onClick={(e) => {
      e.stopPropagation();
      setSessionToRename(session);
      setNewSessionName(session.name);
    }}>
      <Edit2 className="h-4 w-4 mr-2" />
      Renomear
    </DropdownMenuItem>
    <DropdownMenuSeparator />
    <DropdownMenuItem onClick={(e) => {
      e.stopPropagation();
      setSessionToDelete(session);
    }} className="text-destructive">
      <Trash2 className="h-4 w-4 mr-2" />
      Deletar
    </DropdownMenuItem>
  </DropdownMenuContent>
</DropdownMenu>

// AlertDialog para confirmaÃ§Ã£o de delete
<AlertDialog open={!!sessionToDelete} onOpenChange={(open) => {
  if (!open) setSessionToDelete(null);
}}>
  <AlertDialogContent>
    <AlertDialogHeader>
      <AlertDialogTitle>Confirmar RemoÃ§Ã£o</AlertDialogTitle>
      <AlertDialogDescription>
        Tem certeza que deseja remover a sessÃ£o "{sessionToDelete?.name}"?
        Esta aÃ§Ã£o nÃ£o pode ser desfeita.
      </AlertDialogDescription>
    </AlertDialogHeader>
    <AlertDialogFooter>
      <AlertDialogCancel>Cancelar</AlertDialogCancel>
      <AlertDialogAction onClick={handleDeleteSession} disabled={isDeleting}>
        {isDeleting ? "Removendo..." : "Remover"}
      </AlertDialogAction>
    </AlertDialogFooter>
  </AlertDialogContent>
</AlertDialog>

// Dialog para rename
<Dialog open={!!sessionToRename} onOpenChange={(open) => {
  if (!open) {
    setSessionToRename(null);
    setNewSessionName('');
  }
}}>
  <DialogContent>
    <DialogHeader>
      <DialogTitle>Renomear SessÃ£o</DialogTitle>
      <DialogDescription>
        Digite um novo nome para a sessÃ£o "{sessionToRename?.name}"
      </DialogDescription>
    </DialogHeader>
    <div className="py-4">
      <Input
        value={newSessionName}
        onChange={(e) => setNewSessionName(e.target.value)}
        placeholder="Nome da sessÃ£o"
      />
    </div>
    <DialogFooter>
      <Button variant="outline" onClick={() => {
        setSessionToRename(null);
        setNewSessionName('');
      }}>
        Cancelar
      </Button>
      <Button onClick={handleRenameSession} disabled={isRenaming || !newSessionName.trim()}>
        {isRenaming ? "Renomeando..." : "Renomear"}
      </Button>
    </DialogFooter>
  </DialogContent>
</Dialog>
```

**Handlers:**
```typescript
const handleDeleteSession = async () => {
  if (!sessionToDelete) return;

  setIsDeleting(true);
  try {
    const folderQuery = sessionToDelete.folder 
      ? `?folder=${encodeURIComponent(sessionToDelete.folder)}` 
      : '';
    
    const response = await fetch(
      `/api/v1/sessions/${encodeURIComponent(sessionToDelete.name)}${folderQuery}`,
      {
        method: 'DELETE',
        headers: {
          'Authorization': `Bearer poc-token-123`,
        },
      }
    );

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error || 'Erro ao deletar sessÃ£o');
    }

    toast.success(`SessÃ£o "${sessionToDelete.name}" removida com sucesso`);
    
    // Recarregar lista de sessÃµes
    loadSessions();
    setSessionToDelete(null);
  } catch (error) {
    console.error('Erro ao deletar sessÃ£o:', error);
    toast.error(error instanceof Error ? error.message : 'Erro ao deletar sessÃ£o');
  } finally {
    setIsDeleting(false);
  }
};

const handleRenameSession = async () => {
  if (!sessionToRename || !newSessionName.trim()) return;

  setIsRenaming(true);
  try {
    const folderQuery = sessionToRename.folder 
      ? `?folder=${encodeURIComponent(sessionToRename.folder)}` 
      : '';
    
    const response = await fetch(
      `/api/v1/sessions/${encodeURIComponent(sessionToRename.name)}/rename${folderQuery}`,
      {
        method: 'PUT',
        headers: {
          'Authorization': `Bearer poc-token-123`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ new_name: newSessionName.trim() }),
      }
    );

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.error || 'Erro ao renomear sessÃ£o');
    }

    toast.success(`SessÃ£o renomeada para "${newSessionName.trim()}"`);
    
    // Recarregar lista de sessÃµes
    loadSessions();
    setSessionToRename(null);
    setNewSessionName('');
  } catch (error) {
    console.error('Erro ao renomear sessÃ£o:', error);
    toast.error(error instanceof Error ? error.message : 'Erro ao renomear sessÃ£o');
  } finally {
    setIsRenaming(false);
  }
};
```

**2. Backend Implementation:**

**handlers/sessions.go** - Novo handler de rename:
```go
func (h *SessionsHandler) RenameSession(c *gin.Context) {
	oldName := c.Param("name")
	folder := c.Query("folder")

	var request struct {
		NewName string `json:"new_name" binding:"required"`
	}

	if err := c.ShouldBindJSON(&request); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"success": false,
			"error":   "Invalid request body",
		})
		return
	}

	var err error
	if folder != "" {
		sessionFolder, parseErr := h.parseSessionFolder(folder)
		if parseErr != nil {
			c.JSON(http.StatusBadRequest, gin.H{
				"success": false,
				"error":   parseErr.Error(),
			})
			return
		}
		
		err = h.sessionManager.RenameSessionInFolder(oldName, request.NewName, sessionFolder)
	} else {
		c.JSON(http.StatusBadRequest, gin.H{
			"success": false,
			"error":   "Folder parameter is required for rename operation",
		})
		return
	}

	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{
			"success": false,
			"error":   err.Error(),
		})
		return
	}

	c.JSON(http.StatusOK, gin.H{
		"success":  true,
		"old_name": oldName,
		"new_name": request.NewName,
	})
}
```

**server.go** - Nova rota:
```go
api.PUT("/sessions/:name/rename", sessionHandler.RenameSession)
```

**3. TypeScript Types:**

**types.ts** - Adicionado campo folder:
```typescript
export interface Session {
  name: string;
  folder?: string;  // âœ… ADICIONADO para suportar folders
  type: string;
  changes: SessionChange[];
  node_pool_changes?: NodePoolChange[];
  created_at?: string;
}
```

---

## ğŸ› ISSUE ATUAL: DROPDOWN MENU NÃƒO VISÃVEL

### Problema:
UsuÃ¡rio reportou: "nÃ£o aparece nada para editar a sessÃ£o"

### AnÃ¡lise:
- CÃ³digo do dropdown estÃ¡ correto estruturalmente
- Todos os componentes shadcn/ui importados corretamente
- Event handlers com `stopPropagation()` para evitar conflitos
- PossÃ­vel problema: **visibilidade visual do botÃ£o**

### SoluÃ§Ã£o Aplicada:
```typescript
// âœ… Adicionado cursor pointer e hover effect para melhor descoberta
<Button 
  variant="ghost" 
  size="icon" 
  className="h-6 w-6 cursor-pointer hover:bg-accent"  // â¬…ï¸ NOVO
>
  <MoreVertical className="h-4 w-4" />
</Button>
```

### PrÃ³ximos Passos:
1. **Rebuild obrigatÃ³rio**: `./rebuild-web.sh -b`
2. **Hard refresh no browser**: Ctrl+Shift+R
3. **Verificar localizaÃ§Ã£o**: BotÃ£o trÃªs pontinhos (â‹®) ao lado do badge de tipo da sessÃ£o
4. **Se ainda invisÃ­vel**: Considerar usar `variant="outline"` ou adicionar label "AÃ§Ãµes"

---

## ğŸ¨ FEATURE: EDITOR DE SESSÃ•ES SALVAS (21 Outubro 2025)

### Objetivo:
Permitir ediÃ§Ã£o completa do conteÃºdo de arquivos de sessÃ£o salvos, incluindo modificaÃ§Ã£o de valores de HPAs e Node Pools salvos com valores incorretos.

### ImplementaÃ§Ã£o Completa:

**1. Frontend: EditSessionModal.tsx (NOVO - 480 linhas)**

Componente completo de ediÃ§Ã£o de sessÃµes com:

**Features:**
- âœ… **Tabs para HPAs e Node Pools** - OrganizaÃ§Ã£o por tipo de recurso
- âœ… **Lista clicÃ¡vel** - Click para expandir/editar cada item
- âœ… **FormulÃ¡rios completos**:
  - HPAs: Min/Max Replicas, Target CPU/Memory, CPU/Memory Request/Limit
  - Node Pools: Node Count, Autoscaling, Min/Max Node Count
- âœ… **RemoÃ§Ã£o de itens** - BotÃ£o "Remover" para cada HPA/Node Pool
- âœ… **ValidaÃ§Ã£o** - Tipos corretos (nÃºmeros inteiros para counts/replicas)
- âœ… **Alert de aviso** - Mensagem destacando que modifica arquivo diretamente
- âœ… **ScrollArea** - Suporte para muitos itens sem quebrar layout
- âœ… **Deep copy** - EdiÃ§Ã£o nÃ£o afeta sessÃ£o original atÃ© salvar

**Estrutura:**
```typescript
interface EditSessionModalProps {
  session: Session | null;
  open: boolean;
  onOpenChange: (open: boolean) => void;
  onSave: () => void;  // Callback para recarregar lista
}

// Estados principais
const [editedSession, setEditedSession] = useState<Session | null>(null);
const [selectedHPAIndex, setSelectedHPAIndex] = useState<number | null>(null);
const [selectedNodePoolIndex, setSelectedNodePoolIndex] = useState<number | null>(null);

// MÃ©todos de atualizaÃ§Ã£o
updateHPAChange(index, field, value)     // Atualiza campo de HPA
updateNodePoolChange(index, field, value) // Atualiza campo de Node Pool
deleteHPAChange(index)                    // Remove HPA da sessÃ£o
deleteNodePoolChange(index)               // Remove Node Pool da sessÃ£o
```

**UI/UX:**
- Click no card para expandir formulÃ¡rio inline
- Card selecionado fica com borda azul (`border-blue-500 bg-blue-50`)
- Badges mostrando cluster, namespace, resource group
- Contadores nos tabs: `HPAs (3)`, `Node Pools (2)`
- Mensagem quando lista vazia: "Nenhum HPA nesta sessÃ£o"

**2. Backend: UpdateSession Handler (handlers/sessions.go)**

```go
func (h *SessionsHandler) UpdateSession(c *gin.Context) {
    // 1. ValidaÃ§Ãµes (session manager, folder obrigatÃ³rio)
    // 2. Parse do JSON body para models.Session
    // 3. Recalcular metadata (clusters, namespaces, contadores)
    // 4. Salvar com SaveSessionToFolder()
    // 5. Retornar sucesso
}
```

**CaracterÃ­sticas:**
- âœ… **Folder obrigatÃ³rio** - Evita ambiguidade sobre onde salvar
- âœ… **Metadata auto-calculada** - Clusters afetados, contadores atualizados
- âœ… **Reutiliza SaveSessionToFolder()** - Mesma lÃ³gica do TUI
- âœ… **ValidaÃ§Ã£o completa** - Erros detalhados em JSON response

**3. Rota API (server.go)**

```go
api.PUT("/sessions/:name", sessionHandler.UpdateSession)
```

Query parameters:
- `name` (path) - Nome da sessÃ£o a atualizar
- `folder` (query, obrigatÃ³rio) - Pasta onde sessÃ£o estÃ¡ salva

Body: JSON completo da sessÃ£o editada

**4. IntegraÃ§Ã£o LoadSessionModal.tsx**

```typescript
// Estado adicional
const [sessionToEdit, setSessionToEdit] = useState<Session | null>(null);

// Novo item no dropdown menu
<DropdownMenuItem onClick={() => setSessionToEdit(session)}>
  <Edit2 className="h-4 w-4 mr-2" />
  Editar ConteÃºdo
</DropdownMenuItem>

// Modal no final do componente
<EditSessionModal
  session={sessionToEdit}
  open={!!sessionToEdit}
  onOpenChange={(open) => !open && setSessionToEdit(null)}
  onSave={() => {
    loadSessions(); // Recarrega lista
    setSessionToEdit(null);
  }}
/>
```

### Fluxo Completo de Uso:

1. **Abrir Load Session Modal** - UsuÃ¡rio clica em botÃ£o "Load Session"
2. **Selecionar pasta** - Escolhe pasta (HPA-Upscale, Node-Downscale, etc)
3. **Click no menu dropdown (â‹®)** - TrÃªs pontinhos ao lado da sessÃ£o
4. **Selecionar "Editar ConteÃºdo"** - Abre EditSessionModal
5. **Navegar entre tabs** - "HPAs" ou "Node Pools"
6. **Click em um item** - Expande formulÃ¡rio de ediÃ§Ã£o
7. **Modificar valores**:
   - HPAs: Min/Max replicas, targets, resources
   - Node Pools: Node count, autoscaling, min/max
8. **Remover itens** (opcional) - BotÃ£o "Remover HPA/Node Pool"
9. **Salvar alteraÃ§Ãµes** - BotÃ£o "Salvar AlteraÃ§Ãµes"
10. **API atualiza arquivo** - PUT `/api/v1/sessions/:name?folder=...`
11. **Lista recarrega** - SessÃ£o atualizada aparece na lista
12. **Toast de sucesso** - "SessÃ£o atualizada com sucesso"

### Casos de Uso:

**1. Corrigir valores de HPA salvos incorretamente:**
```
Problema: Salvou min_replicas = 10 mas deveria ser 1
SoluÃ§Ã£o: Editar sessÃ£o â†’ Click no HPA â†’ Alterar "Min Replicas" para 1 â†’ Salvar
```

**2. Remover HPAs/Node Pools de uma sessÃ£o:**
```
CenÃ¡rio: SessÃ£o tem 5 HPAs mas sÃ³ quer aplicar 3
SoluÃ§Ã£o: Editar sessÃ£o â†’ Remover os 2 HPAs indesejados â†’ Salvar
```

**3. Ajustar Node Pool counts para novo stress test:**
```
CenÃ¡rio: Reutilizar sessÃ£o mas com node count diferente
SoluÃ§Ã£o: Editar sessÃ£o â†’ Alterar "Node Count" â†’ Salvar como nova referÃªncia
```

**4. Modificar autoscaling settings:**
```
CenÃ¡rio: Node pool estava com autoscaling enabled mas deve ser manual
SoluÃ§Ã£o: Editar sessÃ£o â†’ Desmarcar "Autoscaling Enabled" â†’ Salvar
```

### Arquivos Criados/Modificados:

**Novos:**
- `internal/web/frontend/src/components/EditSessionModal.tsx` (480 linhas)

**Modificados:**
- `internal/web/handlers/sessions.go` - Handler UpdateSession (+100 linhas)
- `internal/web/server.go` - Rota PUT /sessions/:name
- `internal/web/frontend/src/components/LoadSessionModal.tsx` - IntegraÃ§Ã£o EditSessionModal

### ValidaÃ§Ãµes Implementadas:

**Frontend:**
- âœ… Min Replicas >= 0
- âœ… Max Replicas >= 1
- âœ… Target CPU: 1-100 (opcional)
- âœ… Target Memory: 1-100 (opcional)
- âœ… Node Count >= 0
- âœ… Min/Max Node Count se autoscaling habilitado

**Backend:**
- âœ… Folder obrigatÃ³rio (erro se ausente)
- âœ… JSON vÃ¡lido (binding com ShouldBindJSON)
- âœ… Session manager inicializado
- âœ… Metadata recalculada automaticamente

### PrÃ³ximas MelhoraÃ§Ãµes (Futuro):

**Nice to have:**
- [ ] Preview de diff (before/after) antes de salvar
- [ ] ValidaÃ§Ã£o de formato de resources (100m, 256Mi)
- [ ] Duplicar sessÃ£o com valores editados
- [ ] HistÃ³rico de ediÃ§Ãµes (timestamps)
- [ ] Undo/Redo dentro do editor
- [ ] Adicionar novos HPAs/Node Pools (nÃ£o sÃ³ editar existentes)
- [ ] Busca/filtro dentro da lista de HPAs

### Testing Checklist:

- [ ] Editar valores de HPA e salvar
- [ ] Editar valores de Node Pool e salvar
- [ ] Remover HPA de sessÃ£o
- [ ] Remover Node Pool de sessÃ£o
- [ ] Salvar sessÃ£o vazia (todos itens removidos)
- [ ] Cancelar ediÃ§Ã£o (nÃ£o salvar mudanÃ§as)
- [ ] Editar sessÃ£o, salvar, reabrir editor (valores corretos)
- [ ] Hard refresh do browser apÃ³s rebuild
- [ ] Verificar arquivo JSON foi atualizado em `~/.k8s-hpa-manager/sessions/<pasta>/`

---

## ğŸ”„ HISTÃ“RICO DE CORREÃ‡Ã•ES CRÃTICAS (Outubro 2025)

### 1. **Tela Branca no NodePoolEditor** âœ…
**Causa:** MÃ©todos inexistentes no StagingContext
```typescript
// âŒ ANTES
const stagedPool = staging.getNodePool(key);
staging.addNodePool(modifiedNodePool, nodePool, order);

// âœ… DEPOIS
const stagedPool = staging.stagedNodePools.find(/* ... */);
staging.addNodePoolToStaging(nodePool);
```

### 2. **HPAEditor NÃ£o Salvava no Staging** âœ…
**Causa:** MÃ©todo `staging.add()` nÃ£o existia
```typescript
// âŒ ANTES
staging.add(modifiedHPA, hpa);

// âœ… DEPOIS
staging.addHPAToStaging(hpa);
staging.updateHPAInStaging(cluster, namespace, name, updates);
```

### 3. **Cluster Name Mismatch (-admin suffix)** âœ…
**Causa:** Sessions salvavam sem `-admin`, kubeconfig tinha com `-admin`
**SoluÃ§Ã£o:** `StagingContext.loadFromSession()` adiciona `-admin` automaticamente

### 4. **Build Process** âœ…
**Descoberta:** DEVE usar `./rebuild-web.sh -b` - builds manuais nÃ£o funcionam corretamente
