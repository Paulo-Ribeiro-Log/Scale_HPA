# HPA Watchdog - AnÃ¡lise de IntegraÃ§Ã£o com Prometheus

## ğŸ“Š ComparaÃ§Ã£o: K8s API vs Prometheus

### Abordagem Original (K8s API apenas)

```
Watchdog â†’ K8s API â†’ Metrics Server â†’ Pods
         â†“
    HPASnapshot (cada 30s)
```

**Vantagens:**
- âœ… Acesso direto aos dados do cluster
- âœ… Sem dependÃªncia externa (metrics-server jÃ¡ existe)
- âœ… Dados em tempo real

**Desvantagens:**
- âŒ MÃ©tricas limitadas (CPU/Memory bÃ¡sicas)
- âŒ Sem histÃ³rico nativo (precisa armazenar tudo)
- âŒ Performance impacto ao varrer muitos clusters
- âŒ Sem mÃ©tricas customizadas
- âŒ DifÃ­cil fazer anÃ¡lise temporal complexa

---

### Abordagem com Prometheus (RECOMENDADA) âœ…

```
Watchdog â†’ Prometheus API â†’ TSDB (histÃ³rico rico)
         â†“
    HPASnapshot + Extended Metrics
```

**Vantagens:**
- âœ… **HistÃ³rico nativo** - Prometheus jÃ¡ armazena mÃ©tricas (retention configurÃ¡vel)
- âœ… **MÃ©tricas ricas** - CPU, Memory, Network, Latency, Custom Metrics
- âœ… **Performance** - Queries otimizadas (PromQL)
- âœ… **AnÃ¡lise temporal** - Range queries fÃ¡ceis (`[5m]`, `rate()`, `increase()`)
- âœ… **Alerting nativo** - Pode usar regras do Prometheus como base
- âœ… **EscalÃ¡vel** - Prometheus jÃ¡ lida com muitos targets
- âœ… **CorrelaÃ§Ã£o** - Cruzar dados de HPA com outras mÃ©tricas (requests, errors)

**Desvantagens:**
- âš ï¸ DependÃªncia do Prometheus instalado no cluster
- âš ï¸ ConfiguraÃ§Ã£o adicional (endpoints Prometheus)
- âš ï¸ PossÃ­vel lag de scraping (default: 15s)

---

## ğŸ† DecisÃ£o: Arquitetura HÃ­brida

### Melhor dos Dois Mundos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        HPA Watchdog                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  K8s API        â”‚              â”‚  Prometheus API  â”‚         â”‚
â”‚  â”‚  (Config Data)  â”‚              â”‚  (Metrics Data)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                                 â”‚                   â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚           â–¼             â–¼  â–¼               â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚          Unified Collector                      â”‚           â”‚
â”‚  â”‚  - HPA config (K8s)                            â”‚           â”‚
â”‚  â”‚  - Current/Desired replicas (K8s)              â”‚           â”‚
â”‚  â”‚  - Min/Max replicas (K8s)                      â”‚           â”‚
â”‚  â”‚  - CPU/Memory metrics (Prometheus)             â”‚           â”‚
â”‚  â”‚  - Request rate (Prometheus)                   â”‚           â”‚
â”‚  â”‚  - Error rate (Prometheus)                     â”‚           â”‚
â”‚  â”‚  - P95 latency (Prometheus)                    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                    â–¼                                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚         â”‚  Enhanced Analyzer   â”‚                               â”‚
â”‚         â”‚  - Temporal analysis â”‚                               â”‚
â”‚         â”‚  - Correlation       â”‚                               â”‚
â”‚         â”‚  - Prediction        â”‚                               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                    â–¼                                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚              â”‚  Alerts  â”‚                                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### O Que Vem de Onde

#### K8s API (kubernetes client-go)
**Usado para:** Dados de configuraÃ§Ã£o e estado
```
âœ… HPA config (min/max replicas, targets)
âœ… Current/Desired replicas
âœ… Deployment info
âœ… Pod status (Ready/NotReady)
âœ… Events (HPA scaling events)
âœ… Resource requests/limits
```

#### Prometheus API (prometheus/client_golang)
**Usado para:** MÃ©tricas e anÃ¡lise temporal
```
âœ… CPU usage (real-time + histÃ³rico)
âœ… Memory usage (real-time + histÃ³rico)
âœ… Request rate (QPS)
âœ… Error rate (5xx, 4xx)
âœ… Response latency (P50, P95, P99)
âœ… Network I/O
âœ… Custom application metrics
âœ… HistÃ³rico de scaling (kube_hpa_status_current_replicas[5m])
```

---

## ğŸ”§ Queries Prometheus Ãšteis

### 1. CPU Usage (HPA Target)
```promql
# CPU atual dos pods do HPA
sum(rate(container_cpu_usage_seconds_total{
    namespace="api-gateway",
    pod=~"nginx-.*"
}[1m])) /
sum(kube_pod_container_resource_requests{
    namespace="api-gateway",
    pod=~"nginx-.*",
    resource="cpu"
}) * 100

# HistÃ³rico de 5 minutos
sum(rate(container_cpu_usage_seconds_total{...}[5m]))
```

### 2. Memory Usage (HPA Target)
```promql
# Memory atual
sum(container_memory_working_set_bytes{
    namespace="api-gateway",
    pod=~"nginx-.*"
}) /
sum(kube_pod_container_resource_requests{
    namespace="api-gateway",
    pod=~"nginx-.*",
    resource="memory"
}) * 100
```

### 3. Current Replicas (Historical)
```promql
# RÃ©plicas atuais (Ãºltimos 5 min)
kube_horizontalpodautoscaler_status_current_replicas{
    namespace="api-gateway",
    horizontalpodautoscaler="nginx"
}[5m]

# RÃ©plicas desejadas
kube_horizontalpodautoscaler_status_desired_replicas{...}
```

### 4. Request Rate (Contexto)
```promql
# Requests por segundo
sum(rate(http_requests_total{
    namespace="api-gateway",
    service="nginx"
}[1m]))
```

### 5. Error Rate (CorrelaÃ§Ã£o)
```promql
# Taxa de erro (%)
sum(rate(http_requests_total{
    namespace="api-gateway",
    status=~"5.."
}[1m])) /
sum(rate(http_requests_total{
    namespace="api-gateway"
}[1m])) * 100
```

### 6. P95 Latency (CorrelaÃ§Ã£o)
```promql
histogram_quantile(0.95,
    sum(rate(http_request_duration_seconds_bucket{
        namespace="api-gateway"
    }[1m])) by (le)
)
```

---

## ğŸ“Š Enhanced Data Model com Prometheus

### HPASnapshot Estendido

```go
// HPASnapshot com mÃ©tricas Prometheus
type HPASnapshot struct {
    Timestamp       time.Time
    Cluster         string
    Namespace       string
    Name            string

    // === K8s API Data ===
    // HPA Config
    MinReplicas     int32
    MaxReplicas     int32
    CurrentReplicas int32
    DesiredReplicas int32

    // Targets
    CPUTarget       int32  // % (ex: 70)
    MemoryTarget    int32  // % (ex: 80)

    // === Prometheus Metrics ===
    // Current Metrics (real-time)
    CPUCurrent      float64 // % atual (Prometheus)
    MemoryCurrent   float64 // % atual (Prometheus)

    // Historical Metrics (5 min)
    CPUHistory      []float64 // CPU Ãºltimos 5 min (1 ponto/30s)
    MemoryHistory   []float64 // Memory Ãºltimos 5 min

    // Replica History (Prometheus)
    ReplicaHistory  []int32   // RÃ©plicas Ãºltimos 5 min

    // Extended Metrics (Prometheus)
    RequestRate     float64   // Requests/sec
    ErrorRate       float64   // % errors (5xx)
    P95Latency      float64   // P95 latency (ms)
    NetworkRxBytes  float64   // Network RX (bytes/s)
    NetworkTxBytes  float64   // Network TX (bytes/s)

    // Deployment Resources (K8s API)
    CPURequest      string // Ex: "500m"
    CPULimit        string // Ex: "1000m"
    MemoryRequest   string // Ex: "512Mi"
    MemoryLimit     string // Ex: "1Gi"

    // Status (K8s API)
    Ready           bool
    ScalingActive   bool
    LastScaleTime   *time.Time
}
```

---

## ğŸ”Œ ImplementaÃ§Ã£o: Prometheus Client

### DependÃªncias

```go
import (
    "github.com/prometheus/client_golang/api"
    promv1 "github.com/prometheus/client_golang/api/prometheus/v1"
    "github.com/prometheus/common/model"
)
```

### Prometheus Client Wrapper

```go
package prometheus

import (
    "context"
    "fmt"
    "time"

    "github.com/prometheus/client_golang/api"
    promv1 "github.com/prometheus/client_golang/api/prometheus/v1"
    "github.com/prometheus/common/model"
)

type Client struct {
    api    promv1.API
    cluster string
}

func NewClient(prometheusURL, clusterName string) (*Client, error) {
    client, err := api.NewClient(api.Config{
        Address: prometheusURL,
    })
    if err != nil {
        return nil, err
    }

    return &Client{
        api:    promv1.NewAPI(client),
        cluster: clusterName,
    }, nil
}

// GetCPUUsage retorna CPU usage atual e histÃ³rico (5 min)
func (c *Client) GetCPUUsage(ctx context.Context, namespace, hpaName string, podSelector string) (*MetricData, error) {
    // Query CPU atual (1 min rate)
    query := fmt.Sprintf(`
        sum(rate(container_cpu_usage_seconds_total{
            namespace="%s",
            pod=~"%s"
        }[1m])) /
        sum(kube_pod_container_resource_requests{
            namespace="%s",
            pod=~"%s",
            resource="cpu"
        }) * 100
    `, namespace, podSelector, namespace, podSelector)

    result, warnings, err := c.api.Query(ctx, query, time.Now())
    if err != nil {
        return nil, err
    }
    if len(warnings) > 0 {
        // Log warnings
    }

    // Parse result
    cpuCurrent := parseScalarResult(result)

    // Query histÃ³rico (5 min)
    rangeQuery := fmt.Sprintf(`
        sum(rate(container_cpu_usage_seconds_total{
            namespace="%s",
            pod=~"%s"
        }[1m]))
    `, namespace, podSelector)

    rangeResult, _, err := c.api.QueryRange(ctx, rangeQuery, promv1.Range{
        Start: time.Now().Add(-5 * time.Minute),
        End:   time.Now(),
        Step:  30 * time.Second, // 1 ponto a cada 30s = 10 pontos
    })
    if err != nil {
        return nil, err
    }

    cpuHistory := parseRangeResult(rangeResult)

    return &MetricData{
        Current: cpuCurrent,
        History: cpuHistory,
    }, nil
}

// GetReplicaHistory retorna histÃ³rico de rÃ©plicas (5 min)
func (c *Client) GetReplicaHistory(ctx context.Context, namespace, hpaName string) ([]int32, error) {
    query := fmt.Sprintf(`
        kube_horizontalpodautoscaler_status_current_replicas{
            namespace="%s",
            horizontalpodautoscaler="%s"
        }
    `, namespace, hpaName)

    result, _, err := c.api.QueryRange(ctx, query, promv1.Range{
        Start: time.Now().Add(-5 * time.Minute),
        End:   time.Now(),
        Step:  30 * time.Second,
    })
    if err != nil {
        return nil, err
    }

    return parseReplicaHistory(result), nil
}

// GetRequestRate retorna taxa de requests/sec
func (c *Client) GetRequestRate(ctx context.Context, namespace, service string) (float64, error) {
    query := fmt.Sprintf(`
        sum(rate(http_requests_total{
            namespace="%s",
            service="%s"
        }[1m]))
    `, namespace, service)

    result, _, err := c.api.Query(ctx, query, time.Now())
    if err != nil {
        return 0, err
    }

    return parseScalarResult(result), nil
}

// GetErrorRate retorna taxa de erros (%)
func (c *Client) GetErrorRate(ctx context.Context, namespace, service string) (float64, error) {
    query := fmt.Sprintf(`
        sum(rate(http_requests_total{
            namespace="%s",
            service="%s",
            status=~"5.."
        }[1m])) /
        sum(rate(http_requests_total{
            namespace="%s",
            service="%s"
        }[1m])) * 100
    `, namespace, service, namespace, service)

    result, _, err := c.api.Query(ctx, query, time.Now())
    if err != nil {
        return 0, err
    }

    return parseScalarResult(result), nil
}

// Helper functions
func parseScalarResult(result model.Value) float64 {
    if result.Type() == model.ValVector {
        vector := result.(model.Vector)
        if len(vector) > 0 {
            return float64(vector[0].Value)
        }
    }
    return 0
}

func parseRangeResult(result model.Value) []float64 {
    if result.Type() == model.ValMatrix {
        matrix := result.(model.Matrix)
        if len(matrix) > 0 {
            values := make([]float64, len(matrix[0].Values))
            for i, pair := range matrix[0].Values {
                values[i] = float64(pair.Value)
            }
            return values
        }
    }
    return []float64{}
}

func parseReplicaHistory(result model.Value) []int32 {
    floats := parseRangeResult(result)
    replicas := make([]int32, len(floats))
    for i, f := range floats {
        replicas[i] = int32(f)
    }
    return replicas
}
```

---

## âš™ï¸ ConfiguraÃ§Ã£o com Prometheus

### watchdog.yaml (atualizado)

```yaml
# HPA Watchdog Configuration

monitoring:
  scan_interval_seconds: 30
  history_retention_minutes: 5

  # Prometheus Integration (NOVO)
  prometheus:
    enabled: true                     # Habilita integraÃ§Ã£o Prometheus
    auto_discover: true               # Auto-descobre endpoints via K8s
    fallback_to_metrics_server: true # Fallback para metrics-server se Prometheus indisponÃ­vel

    # Endpoints por cluster (opcional - se auto_discover=false)
    endpoints:
      akspriv-prod-east: "http://prometheus.monitoring.svc:9090"
      akspriv-prod-west: "http://prometheus.monitoring.svc:9090"
      akspriv-qa: "http://prometheus-qa.monitoring.svc:9090"

    # Queries customizadas (opcional - permite override)
    custom_queries:
      cpu_usage: |
        sum(rate(container_cpu_usage_seconds_total{
          namespace="{namespace}",
          pod=~"{pod_selector}"
        }[1m])) / sum(kube_pod_container_resource_requests{...}) * 100

    # Timeout para queries
    query_timeout_seconds: 10

clusters:
  config_path: "~/.k8s-hpa-manager/clusters-config.json"
  auto_discover: true

thresholds:
  # Replica changes
  replica_delta_percent: 50.0
  replica_delta_absolute: 5

  # CPU/Memory
  cpu_warning_percent: 85
  cpu_critical_percent: 90
  memory_warning_percent: 85
  memory_critical_percent: 90

  # Extended Thresholds (Prometheus)
  request_rate_spike_percent: 100.0  # Alerta se request rate dobrar
  error_rate_critical_percent: 5.0   # Alerta se >5% errors
  p95_latency_critical_ms: 1000      # Alerta se P95 >1s
```

---

## ğŸ¨ Interface TUI Atualizada

### Dashboard com MÃ©tricas Prometheus

```
â•”â• HPA: api-gateway/nginx â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘  Status: âœ… Healthy  |  Replicas: 8/10  |  CPU: 78%  |  Mem: 65%   â•‘
â•‘                                                                      â•‘
â•‘  â”Œâ”€ CPU Usage (5 min) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€ Memory Usage (5 min) â”€â”€â”€â”€â”€â”â•‘
â•‘  â”‚  100%â”‚                         â”‚  â”‚  100%â”‚                      â”‚â•‘
â•‘  â”‚   90%â”‚         â•­â”€â”€â•®            â”‚  â”‚   80%â”‚                      â”‚â•‘
â•‘  â”‚   80%â”‚      â•­â”€â”€â•¯  â•°â•®           â”‚  â”‚   60%â”‚   â•­â”€â”€â”€â”€â•®            â”‚â•‘
â•‘  â”‚   70%â”‚   â•­â”€â”€â•¯      â•°â”€          â”‚  â”‚   40%â”‚â•­â”€â”€â•¯    â•°â”€â”€â•®         â”‚â•‘
â•‘  â”‚   60%â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”‚   20%â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚â•‘
â•‘  â”‚      14:30   14:32   14:35     â”‚  â”‚      14:30   14:32   14:35 â”‚â•‘
â•‘  â”‚  Target: 70%                   â”‚  â”‚  Target: 80%                â”‚â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â•‘
â•‘                                                                      â•‘
â•‘  â”Œâ”€ Replicas (5 min) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€ Request Rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â•‘
â•‘  â”‚   10â”‚              â–ˆâ–ˆâ–ˆ          â”‚  â”‚  500 rpsâ”‚        â•­â•®        â”‚â•‘
â•‘  â”‚    8â”‚           â–ˆâ–ˆâ–ˆâ•±            â”‚  â”‚  400 rpsâ”‚      â•­â”€â•¯â•°â”€â•®      â”‚â•‘
â•‘  â”‚    6â”‚        â–ˆâ–ˆâ–ˆâ•±               â”‚  â”‚  300 rpsâ”‚   â•­â”€â”€â•¯    â•°â”€â”€â•®   â”‚â•‘
â•‘  â”‚    4â”‚     â–ˆâ–ˆâ–ˆ                   â”‚  â”‚  200 rpsâ”‚â•­â”€â”€â•¯          â•°â”€  â”‚â•‘
â•‘  â”‚    2â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚  â”‚  100 rpsâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â•‘
â•‘  â”‚     14:30   14:32   14:35      â”‚  â”‚         14:30   14:32  14:35â”‚â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â•‘
â•‘                                                                      â•‘
â•‘  ğŸ“Š Extended Metrics (Prometheus)                                   â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚  Request Rate:    425 req/s    (â†‘ 12% vs 5min ago)          â”‚  â•‘
â•‘  â”‚  Error Rate:      0.8%         (âœ… below 5% threshold)       â”‚  â•‘
â•‘  â”‚  P95 Latency:     245ms        (âœ… below 1000ms threshold)   â”‚  â•‘
â•‘  â”‚  Network RX:      12.5 MB/s                                  â”‚  â•‘
â•‘  â”‚  Network TX:      8.3 MB/s                                   â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸš€ Vantagens da Arquitetura HÃ­brida

### 1. DetecÃ§Ã£o de Anomalias Melhorada

**Antes (sÃ³ K8s API):**
```
âŒ CPU spike: 95% (limite: 90%)
```

**Depois (com Prometheus):**
```
ğŸ”´ ANOMALIA DETECTADA
   CPU spike: 95% (limite: 90%)

   ğŸ“Š Contexto (Prometheus):
   - Request rate: 850 req/s (â†‘300% vs baseline 200 req/s)
   - Error rate: 12% (â†‘ de 0.5%)
   - P95 latency: 2.5s (â†‘ de 150ms)

   ğŸ’¡ DiagnÃ³stico: Traffic spike + errors = provÃ¡vel incident
   ğŸš¨ AÃ§Ã£o sugerida: Verificar upstream services
```

### 2. AnÃ¡lise Temporal Rica

```go
// Detectar "scaling oscillation" (rÃ©plicas sobem e descem rapidamente)
func detectOscillation(replicaHistory []int32) bool {
    changes := 0
    for i := 1; i < len(replicaHistory); i++ {
        if replicaHistory[i] != replicaHistory[i-1] {
            changes++
        }
    }
    // Se rÃ©plicas mudaram >5x em 5 min = oscilaÃ§Ã£o
    return changes > 5
}
```

### 3. PrediÃ§Ã£o de Anomalias

```go
// Usar histÃ³rico Prometheus para prever scaling
func predictScaling(cpuHistory []float64, cpuTarget int32) bool {
    // TendÃªncia: CPU crescendo consistentemente
    if len(cpuHistory) < 5 {
        return false
    }

    // Ãšltimos 5 pontos em tendÃªncia de alta
    increasing := true
    for i := 1; i < 5; i++ {
        if cpuHistory[len(cpuHistory)-i] <= cpuHistory[len(cpuHistory)-i-1] {
            increasing = false
            break
        }
    }

    currentCPU := cpuHistory[len(cpuHistory)-1]

    // Se CPU subindo e prÃ³ximo do target = scaling iminente
    if increasing && currentCPU > float64(cpuTarget)*0.9 {
        return true
    }

    return false
}
```

---

## ğŸ¯ DecisÃ£o Final

### âœ… RECOMENDAÃ‡ÃƒO: Usar Prometheus como Fonte Principal

**Motivos:**
1. **HistÃ³rico nativo** - NÃ£o precisa armazenar tudo em memÃ³ria
2. **MÃ©tricas ricas** - Muito alÃ©m de CPU/Memory
3. **AnÃ¡lise avanÃ§ada** - PromQL permite queries complexas
4. **Escalabilidade** - Prometheus jÃ¡ Ã© otimizado para TSDB
5. **Contexto completo** - Correlacionar HPA com trÃ¡fego, errors, latency

**Fallback:**
- Se Prometheus nÃ£o disponÃ­vel â†’ usar metrics-server (K8s API)
- Flag `fallback_to_metrics_server: true` na config

**ConclusÃ£o:**
A arquitetura hÃ­brida (K8s API para config + Prometheus para mÃ©tricas) oferece o melhor resultado para um watchdog robusto e inteligente! ğŸš€
