# HPA Watchdog - Especifica√ß√£o T√©cnica

## üìã Vis√£o Geral

**HPA Watchdog** √© um vigilante aut√¥nomo para monitoramento de Horizontal Pod Autoscalers (HPAs) em clusters Kubernetes, desenvolvido em Go com interface TUI interativa (Bubble Tea + Lipgloss).

**Status**: üü° Planejamento
**Vers√£o Planejada**: v1.0.0
**√öltima Atualiza√ß√£o**: 23 de outubro de 2025

---

## üéØ Objetivos

### Objetivo Principal
Monitorar continuamente m√∫ltiplos clusters Kubernetes, detectando anomalias em HPAs, deployments e recursos (CPU/Memory) com sistema de alertas configur√°vel.

### Objetivos Espec√≠ficos
1. **Monitoramento Multi-Cluster**: Varrer todos os clusters configurados em paralelo
2. **Compara√ß√£o Temporal**: Armazenar hist√≥rico dos √∫ltimos 5 minutos para cada m√©trica
3. **Detec√ß√£o de Anomalias**: Identificar mudan√ßas abruptas baseadas em thresholds configur√°veis
4. **Sistema de Alertas**: Notifica√ß√µes visuais (TUI) e logs estruturados
5. **Configura√ß√£o Interativa**: Interface TUI para ajustar thresholds e par√¢metros
6. **Autonomia Total**: Rodar independentemente do k8s-hpa-manager principal

---

## üèóÔ∏è Arquitetura

### Vis√£o Geral (H√≠brida: K8s API + Prometheus)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        HPA Watchdog                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  K8s API        ‚îÇ              ‚îÇ  Prometheus API  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  (Config Data)  ‚îÇ              ‚îÇ  (Metrics Data)  ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ           ‚îÇ                                 ‚îÇ                   ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                   ‚îÇ
‚îÇ           ‚ñº             ‚ñº  ‚ñº               ‚ñº                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ          Unified Collector                      ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ HPA config (K8s)                            ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Current/Desired replicas (K8s)              ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ CPU/Memory metrics (Prometheus)             ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Request rate, errors, latency (Prometheus)  ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                    ‚ñº                                            ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ         ‚îÇ  Enhanced Analyzer   ‚îÇ                               ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Temporal analysis ‚îÇ                               ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Correlation       ‚îÇ                               ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Prediction        ‚îÇ                               ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                    ‚ñº                                            ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ              ‚îÇ  Alerts  ‚îÇ                                       ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fontes de Dados:**
- **K8s API** (kubernetes/client-go): HPA config, replicas, deployment info, events
- **Prometheus** (prometheus/client_golang): CPU/Memory metrics, request rate, errors, latency, hist√≥rico

### Estrutura do Projeto

```
hpa-watchdog/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ main.go                    # Entry point
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ monitor/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collector.go           # Unified collector (K8s + Prometheus)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.go            # An√°lise de anomalias avan√ßada
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alerter.go             # Sistema de alertas
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/                # ‚≠ê NOVO
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.go              # Prometheus API client wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queries.go             # PromQL queries predefinidas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ discovery.go           # Auto-discovery de endpoints
‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ timeseries.go          # Time-series cache (reduzido - Prometheus tem hist√≥rico)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ persistence.go         # Persist√™ncia opcional (SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.go              # Carrega configura√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thresholds.go          # Gerenciamento de thresholds
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clusters.go            # Descoberta de clusters
‚îÇ   ‚îú‚îÄ‚îÄ tui/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.go                 # Main Bubble Tea app
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ views.go               # Renderiza√ß√£o de views
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handlers.go            # Event handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard.go      # Dashboard principal
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts_panel.go   # Painel de alertas
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics_chart.go  # Gr√°ficos ASCII (dados Prometheus)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_modal.go   # Modal de configura√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cluster_list.go   # Lista de clusters
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles.go              # Estilos Lipgloss
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ types.go               # Estruturas de dados
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ watchdog.yaml              # Configura√ß√£o padr√£o
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ Makefile
```

---

## üìä Modelo de Dados

### Core Data Structures

```go
// HPASnapshot representa o estado de um HPA em um momento espec√≠fico
// ‚≠ê ESTENDIDO COM M√âTRICAS PROMETHEUS
type HPASnapshot struct {
    Timestamp       time.Time
    Cluster         string
    Namespace       string
    Name            string

    // === K8s API Data (Config & State) ===
    // HPA Config
    MinReplicas     int32
    MaxReplicas     int32
    CurrentReplicas int32
    DesiredReplicas int32

    // Targets
    CPUTarget       int32  // % (ex: 70)
    MemoryTarget    int32  // % (ex: 80)

    // Deployment Resources (K8s API)
    CPURequest      string // Ex: "500m"
    CPULimit        string // Ex: "1000m"
    MemoryRequest   string // Ex: "512Mi"
    MemoryLimit     string // Ex: "1Gi"

    // Status
    Ready           bool
    ScalingActive   bool
    LastScaleTime   *time.Time

    // === Prometheus Metrics (Real-time & Historical) ===
    // Current Metrics (Prometheus)
    CPUCurrent      float64 // % atual (mais preciso que K8s API)
    MemoryCurrent   float64 // % atual

    // Historical Metrics (5 min history from Prometheus)
    CPUHistory      []float64 // CPU √∫ltimos 5 min (1 ponto/30s = 10 pontos)
    MemoryHistory   []float64 // Memory √∫ltimos 5 min
    ReplicaHistory  []int32   // R√©plicas √∫ltimos 5 min (de kube_hpa_status_current_replicas)

    // Extended Metrics (Prometheus - Optional)
    RequestRate     float64   // Requests/sec (de http_requests_total)
    ErrorRate       float64   // % errors (5xx de http_requests_total)
    P95Latency      float64   // P95 latency (ms) de http_request_duration_seconds
    NetworkRxBytes  float64   // Network RX (bytes/s)
    NetworkTxBytes  float64   // Network TX (bytes/s)

    // Metadata
    DataSource      DataSource // Indica se veio de Prometheus ou Metrics-Server
}

// DataSource indica a origem das m√©tricas
type DataSource int

const (
    DataSourcePrometheus    DataSource = iota // M√©tricas do Prometheus (preferencial)
    DataSourceMetricsServer                   // Fallback: Kubernetes Metrics-Server
    DataSourceHybrid                          // H√≠brido: alguns Prometheus, alguns K8s
)

// TimeSeriesData armazena hist√≥rico de 5 minutos
type TimeSeriesData struct {
    HPAKey      string // "cluster/namespace/name"
    Snapshots   []HPASnapshot
    MaxDuration time.Duration // 5 minutos

    sync.RWMutex
}

// Anomaly representa uma anomalia detectada
type Anomaly struct {
    ID          string
    Timestamp   time.Time
    Severity    AlertSeverity // Critical, Warning, Info
    Type        AnomalyType
    Cluster     string
    Namespace   string
    HPAName     string

    // Detalhes
    Message     string
    OldValue    interface{}
    NewValue    interface{}
    Delta       float64 // Varia√ß√£o percentual

    // Contexto
    Snapshot    HPASnapshot
    History     []HPASnapshot // √öltimos N snapshots

    Acknowledged bool
    AckedAt      *time.Time
}

// AnomalyType define tipos de anomalias
type AnomalyType int

const (
    AnomalyReplicaSpike      AnomalyType = iota // Aumento abrupto de r√©plicas
    AnomalyReplicaDrop                          // Queda abrupta de r√©plicas
    AnomalyCPUSpike                             // CPU current > threshold
    AnomalyMemorySpike                          // Memory current > threshold
    AnomalyResourceChange                       // Mudan√ßa em requests/limits
    AnomalyHPAConfigChange                      // Mudan√ßa em min/max replicas
    AnomalyScalingStuck                         // HPA n√£o consegue escalar
    AnomalyTargetMiss                           // Current muito acima/abaixo do target
)

// AlertSeverity define n√≠veis de severidade
type AlertSeverity int

const (
    SeverityInfo     AlertSeverity = iota
    SeverityWarning
    SeverityCritical
)

// Thresholds define limites configur√°veis
type Thresholds struct {
    // Replica changes
    ReplicaDeltaPercent     float64 // Ex: 50% = alerta se r√©plicas mudam >50%
    ReplicaDeltaAbsolute    int32   // Ex: 5 = alerta se r√©plicas mudam ¬±5

    // CPU/Memory
    CPUWarningPercent       int32   // Ex: 85% = warning
    CPUCriticalPercent      int32   // Ex: 95% = critical
    MemoryWarningPercent    int32   // Ex: 85%
    MemoryCriticalPercent   int32   // Ex: 90%

    // Target deviation
    TargetDeviationPercent  float64 // Ex: 30% = alerta se current est√° 30% acima/abaixo do target

    // Scaling behavior
    ScalingStuckMinutes     int     // Ex: 10 min sem escalar quando deveria

    // Config changes
    AlertOnConfigChange     bool    // Alertar mudan√ßas em HPA config
    AlertOnResourceChange   bool    // Alertar mudan√ßas em deployment resources

    sync.RWMutex
}

// WatchdogConfig configura√ß√£o geral
type WatchdogConfig struct {
    // Monitoring
    ScanIntervalSeconds     int      // Ex: 30s entre scans
    HistoryRetentionMinutes int      // Ex: 5 min de hist√≥rico

    // Clusters
    ClustersConfigPath      string   // Path para clusters-config.json
    AutoDiscoverClusters    bool     // Auto-descobre clusters do kubeconfig
    ExcludeClusters         []string // Clusters para ignorar

    // Storage
    EnablePersistence       bool     // Salvar hist√≥rico em SQLite
    PersistencePath         string   // Ex: ~/.hpa-watchdog/history.db

    // Alerts
    MaxActiveAlerts         int      // M√°ximo de alertas ativos (ex: 100)
    AutoAckResolvedAlerts   bool     // Auto-acknowledge alertas resolvidos

    // Thresholds
    Thresholds              Thresholds

    // UI
    RefreshIntervalMs       int      // Ex: 500ms para refresh da TUI
    EnableSounds            bool     // Sons de alerta (beep)

    sync.RWMutex
}
```

---

## üîÑ Fluxo de Opera√ß√£o

### 1. Inicializa√ß√£o

```
1. Load config (watchdog.yaml)
2. Discover clusters (kubeconfig ou clusters-config.json)
3. Initialize time-series storage (in-memory)
4. Start Bubble Tea TUI
5. Start monitoring goroutines (1 por cluster)
```

### 2. Ciclo de Monitoramento (por cluster)

```
Loop (a cada ScanIntervalSeconds):
  1. List all namespaces (skip system namespaces)
  2. For each namespace:
     a. List HPAs
     b. For each HPA:
        - Get HPA config (min/max replicas, targets)
        - Get current metrics (replicas, CPU%, Memory%)
        - Get deployment resources (requests/limits)
        - Create HPASnapshot
        - Store in time-series (keep last 5 min)
  3. Analyze snapshots for anomalies
  4. Generate alerts if thresholds exceeded
  5. Send alerts to TUI via channel
  6. Sleep until next scan
```

### 3. An√°lise de Anomalias

```go
// Pseudo-c√≥digo
func AnalyzeSnapshot(current HPASnapshot, history []HPASnapshot) []Anomaly {
    anomalies := []Anomaly{}

    if len(history) < 2 {
        return anomalies // Precisa de hist√≥rico
    }

    previous := history[len(history)-1]

    // 1. Replica spike/drop
    replicaDelta := float64(current.CurrentReplicas - previous.CurrentReplicas)
    replicaDeltaPercent := (replicaDelta / float64(previous.CurrentReplicas)) * 100

    if abs(replicaDeltaPercent) > Thresholds.ReplicaDeltaPercent {
        anomalies = append(anomalies, Anomaly{
            Type: AnomalyReplicaSpike,
            Severity: SeverityWarning,
            Message: fmt.Sprintf("R√©plicas mudaram %d ‚Üí %d (%.1f%%)",
                previous.CurrentReplicas, current.CurrentReplicas, replicaDeltaPercent),
            Delta: replicaDeltaPercent,
        })
    }

    // 2. CPU spike
    if current.CPUCurrent >= Thresholds.CPUCriticalPercent {
        anomalies = append(anomalies, Anomaly{
            Type: AnomalyCPUSpike,
            Severity: SeverityCritical,
            Message: fmt.Sprintf("CPU cr√≠tico: %d%% (limite: %d%%)",
                current.CPUCurrent, Thresholds.CPUCriticalPercent),
        })
    } else if current.CPUCurrent >= Thresholds.CPUWarningPercent {
        anomalies = append(anomalies, Anomaly{
            Type: AnomalyCPUSpike,
            Severity: SeverityWarning,
            Message: fmt.Sprintf("CPU alto: %d%% (aviso: %d%%)",
                current.CPUCurrent, Thresholds.CPUWarningPercent),
        })
    }

    // 3. Target deviation
    cpuDeviation := float64(current.CPUCurrent - current.CPUTarget)
    cpuDeviationPercent := (cpuDeviation / float64(current.CPUTarget)) * 100

    if abs(cpuDeviationPercent) > Thresholds.TargetDeviationPercent {
        anomalies = append(anomalies, Anomaly{
            Type: AnomalyTargetMiss,
            Severity: SeverityWarning,
            Message: fmt.Sprintf("CPU desviou do target: %d%% (target: %d%%, desvio: %.1f%%)",
                current.CPUCurrent, current.CPUTarget, cpuDeviationPercent),
        })
    }

    // 4. Config changes
    if Thresholds.AlertOnConfigChange {
        if current.MinReplicas != previous.MinReplicas ||
           current.MaxReplicas != previous.MaxReplicas {
            anomalies = append(anomalies, Anomaly{
                Type: AnomalyHPAConfigChange,
                Severity: SeverityInfo,
                Message: fmt.Sprintf("HPA config alterado: min %d‚Üí%d, max %d‚Üí%d",
                    previous.MinReplicas, current.MinReplicas,
                    previous.MaxReplicas, current.MaxReplicas),
            })
        }
    }

    // ... outras an√°lises

    return anomalies
}
```

---

## üé® Interface TUI

### Layout Principal

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  HPA Watchdog v1.0.0              Clusters: 5  HPAs: 142  Alerts: 3 üî¥    ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                            ‚ïë
‚ïë  üìä DASHBOARD                                          ‚öôÔ∏è  Config  ?  Help‚ïë
‚ïë                                                                            ‚ïë
‚ïë  ‚îå‚îÄ Clusters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ Recent Alerts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚ïë
‚ïë  ‚îÇ ‚úÖ akspriv-prod-east (34)‚îÇ  ‚îÇ üî¥ CRITICAL | 14:35:22                  ‚îÇ‚ïë
‚ïë  ‚îÇ ‚úÖ akspriv-prod-west (28)‚îÇ  ‚îÇ    akspriv-prod-east/api-gateway/nginx  ‚îÇ‚ïë
‚ïë  ‚îÇ ‚úÖ akspriv-qa (18)       ‚îÇ  ‚îÇ    CPU spike: 95% (limit: 90%)          ‚îÇ‚ïë
‚ïë  ‚îÇ ‚ö†Ô∏è  akspriv-dev (12)     ‚îÇ  ‚îÇ                                          ‚îÇ‚ïë
‚ïë  ‚îÇ ‚ùå akspriv-staging (OFF) ‚îÇ  ‚îÇ ‚ö†Ô∏è  WARNING | 14:34:10                  ‚îÇ‚ïë
‚ïë  ‚îÇ                           ‚îÇ  ‚îÇ    akspriv-qa/payments/api              ‚îÇ‚ïë
‚ïë  ‚îÇ Total: 142 HPAs          ‚îÇ  ‚îÇ    Replica spike: 5 ‚Üí 15 (200%)         ‚îÇ‚ïë
‚ïë  ‚îÇ Scanning: 30s interval   ‚îÇ  ‚îÇ                                          ‚îÇ‚ïë
‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚ÑπÔ∏è  INFO | 14:32:45                     ‚îÇ‚ïë
‚ïë                                  ‚îÇ    akspriv-dev/frontend/web             ‚îÇ‚ïë
‚ïë  ‚îå‚îÄ Top Anomalies (5 min) ‚îÄ‚îÄ‚îê  ‚îÇ    HPA config changed: max 10‚Üí20        ‚îÇ‚ïë
‚ïë  ‚îÇ üî• CPU Spikes:        8   ‚îÇ  ‚îÇ                                          ‚îÇ‚ïë
‚ïë  ‚îÇ üìà Replica Changes:   5   ‚îÇ  ‚îÇ [‚Üë‚Üì] Scroll  [A] Ack  [C] Clear All    ‚îÇ‚ïë
‚ïë  ‚îÇ ‚öôÔ∏è  Config Changes:   2   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ïë
‚ïë  ‚îÇ üéØ Target Misses:     3   ‚îÇ                                             ‚ïë
‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îå‚îÄ Metrics Chart (api-gateway/nginx) ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚ïë
‚ïë                                  ‚îÇ                                          ‚îÇ‚ïë
‚ïë  ‚îå‚îÄ Quick Stats ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  CPU %  100‚îÇ         ‚ï≠‚îÄ‚îÄ‚ïÆ               ‚îÇ‚ïë
‚ïë  ‚îÇ Active Alerts:        3   ‚îÇ  ‚îÇ         90‚îÇ      ‚ï≠‚îÄ‚îÄ‚ïØ  ‚ï∞‚ïÆ              ‚îÇ‚ïë
‚ïë  ‚îÇ Acked Alerts:         12  ‚îÇ  ‚îÇ         80‚îÇ   ‚ï≠‚îÄ‚îÄ‚ïØ      ‚ï∞‚îÄ             ‚îÇ‚ïë
‚ïë  ‚îÇ Last Scan:      14:35:45  ‚îÇ  ‚îÇ         70‚îÇ‚ï≠‚îÄ‚îÄ‚ïØ                        ‚îÇ‚ïë
‚ïë  ‚îÇ Next Scan:            15s ‚îÇ  ‚îÇ         60‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ‚ïë
‚ïë  ‚îÇ Uptime:          2h 34m   ‚îÇ  ‚îÇ            14:30   14:32   14:34  14:35‚îÇ‚ïë
‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                                          ‚îÇ‚ïë
‚ïë                                  ‚îÇ  Replicas  20‚îÇ              ‚ñà‚ñà‚ñà         ‚îÇ‚ïë
‚ïë  [Tab] Views  [C] Config       ‚îÇ         15‚îÇ           ‚ñà‚ñà‚ñà‚ï±             ‚îÇ‚ïë
‚ïë  [F5] Refresh [Q] Quit         ‚îÇ         10‚îÇ        ‚ñà‚ñà‚ñà‚ï±                ‚îÇ‚ïë
‚ïë                                  ‚îÇ          5‚îÇ     ‚ñà‚ñà‚ñà                    ‚îÇ‚ïë
‚ïë                                  ‚îÇ          0‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ‚ïë
‚ïë                                  ‚îÇ            14:30   14:32   14:34  14:35‚îÇ‚ïë
‚ïë                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Views (Tabs)

#### 1. Dashboard (Principal)
- Overview de todos os clusters
- Alertas recentes (painel scrollable)
- Gr√°ficos ASCII de m√©tricas selecionadas
- Quick stats

#### 2. Alerts View
```
‚ïî‚ïê Active Alerts (3) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë üî¥ CRITICAL | 14:35:22 | akspriv-prod-east/api-gateway/nginx               ‚ïë
‚ïë    Type: CPU Spike                                                         ‚ïë
‚ïë    CPU: 95% (Warning: 85%, Critical: 90%)                                  ‚ïë
‚ïë    Current Replicas: 8  Min: 2  Max: 10                                    ‚ïë
‚ïë    [A] Acknowledge  [D] Details  [‚Üí] View History                          ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚ö†Ô∏è  WARNING | 14:34:10 | akspriv-qa/payments/api                           ‚ïë
‚ïë    Type: Replica Spike                                                     ‚ïë
‚ïë    Replicas: 5 ‚Üí 15 (200% increase)                                        ‚ïë
‚ïë    Reason: CPU 85% ‚Üí trigger scaling                                       ‚ïë
‚ïë    [A] Acknowledge  [D] Details                                            ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚ÑπÔ∏è  INFO | 14:32:45 | akspriv-dev/frontend/web                             ‚ïë
‚ïë    Type: HPA Config Change                                                 ‚ïë
‚ïë    Min Replicas: 2 ‚Üí 2  Max Replicas: 10 ‚Üí 20                             ‚ïë
‚ïë    [A] Acknowledge                                                         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
[‚Üë‚Üì] Navigate  [A] Ack Selected  [Shift+A] Ack All  [ESC] Back
```

#### 3. Cluster View
```
‚ïî‚ïê Cluster: akspriv-prod-east ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë Status: ‚úÖ Online  |  HPAs: 34  |  Namespaces: 12  |  Uptime: 47d 3h       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Namespace          HPAs   Alerts   CPU Avg   Mem Avg   Last Scan          ‚ïë
‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë
‚ïë api-gateway          3       1      78%       65%      14:35:22           ‚ïë
‚ïë payments             5       1      62%       58%      14:35:20           ‚ïë
‚ïë frontend             2       0      45%       42%      14:35:18           ‚ïë
‚ïë backend              8       0      58%       61%      14:35:16           ‚ïë
‚ïë ...                                                                        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
[Enter] View Namespace  [H] HPA Details  [ESC] Back
```

#### 4. Config Modal
```
‚ïî‚ïê Configuration ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                            ‚ïë
‚ïë  Monitoring Settings                                                       ‚ïë
‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ïë
‚ïë  ‚îÇ Scan Interval:            [30] seconds                             ‚îÇ  ‚ïë
‚ïë  ‚îÇ History Retention:        [5] minutes                              ‚îÇ  ‚ïë
‚ïë  ‚îÇ Auto-discover Clusters:   [‚úì] Enabled                              ‚îÇ  ‚ïë
‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚ïë
‚ïë                                                                            ‚ïë
‚ïë  Alert Thresholds                                                          ‚ïë
‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚ïë
‚ïë  ‚îÇ Replica Delta (percent):  [50] %                                   ‚îÇ  ‚ïë
‚ïë  ‚îÇ Replica Delta (absolute): [5] replicas                             ‚îÇ  ‚ïë
‚ïë  ‚îÇ                                                                     ‚îÇ  ‚ïë
‚ïë  ‚îÇ CPU Warning:              [85] %                                   ‚îÇ  ‚ïë
‚ïë  ‚îÇ CPU Critical:             [90] %                                   ‚îÇ  ‚ïë
‚ïë  ‚îÇ Memory Warning:           [85] %                                   ‚îÇ  ‚ïë
‚ïë  ‚îÇ Memory Critical:          [90] %                                   ‚îÇ  ‚ïë
‚ïë  ‚îÇ                                                                     ‚îÇ  ‚ïë
‚ïë  ‚îÇ Target Deviation:         [30] %                                   ‚îÇ  ‚ïë
‚ïë  ‚îÇ Scaling Stuck:            [10] minutes                             ‚îÇ  ‚ïë
‚ïë  ‚îÇ                                                                     ‚îÇ  ‚ïë
‚ïë  ‚îÇ Alert on Config Change:   [‚úì] Enabled                              ‚îÇ  ‚ïë
‚ïë  ‚îÇ Alert on Resource Change: [‚úì] Enabled                              ‚îÇ  ‚ïë
‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚ïë
‚ïë                                                                            ‚ïë
‚ïë  [‚Üë‚Üì] Navigate  [Enter] Edit  [S] Save  [R] Reset Defaults  [ESC] Cancel ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Componentes Interativos

#### Gr√°ficos ASCII
```go
// Exemplo de ASCII chart para CPU/Memory/Replicas
// Usa biblioteca github.com/guptarohit/asciigraph

import "github.com/guptarohit/asciigraph"

func renderMetricChart(snapshots []HPASnapshot, metric string) string {
    data := extractMetricData(snapshots, metric)

    graph := asciigraph.Plot(data,
        asciigraph.Height(10),
        asciigraph.Width(50),
        asciigraph.Caption(metric),
    )

    return graph
}
```

### Controles de Teclado

| Tecla | A√ß√£o |
|-------|------|
| `Tab` | Alternar entre views (Dashboard, Alerts, Clusters, Config) |
| `‚Üë‚Üì` ou `j k` | Navegar listas |
| `Space` | Selecionar item |
| `Enter` | Ver detalhes / Editar |
| `A` | Acknowledge alerta selecionado |
| `Shift+A` | Acknowledge todos os alertas |
| `C` | Clear alertas acknowledged |
| `D` | Ver detalhes do alerta |
| `H` | Ver hist√≥rico de snapshots |
| `G` | Ir para gr√°fico de m√©tricas |
| `S` | Salvar configura√ß√£o |
| `R` | Reload configura√ß√£o |
| `F5` | Force refresh |
| `Ctrl+C` ou `Q` | Quit |
| `?` | Help |

---

## ‚öôÔ∏è Configura√ß√£o

### Arquivo `configs/watchdog.yaml`

```yaml
# HPA Watchdog Configuration

monitoring:
  scan_interval_seconds: 30        # Intervalo entre scans (segundos)
  history_retention_minutes: 5     # Quanto hist√≥rico manter em mem√≥ria

clusters:
  config_path: "~/.k8s-hpa-manager/clusters-config.json"
  auto_discover: true              # Auto-descobre clusters do kubeconfig
  exclude:                         # Clusters para ignorar
    - akspriv-test
    - akspriv-sandbox

storage:
  enable_persistence: false        # Salvar hist√≥rico em SQLite
  persistence_path: "~/.hpa-watchdog/history.db"

alerts:
  max_active_alerts: 100           # M√°ximo de alertas ativos
  auto_ack_resolved: true          # Auto-acknowledge alertas resolvidos

thresholds:
  # Replica changes
  replica_delta_percent: 50.0      # Alerta se r√©plicas mudam >50%
  replica_delta_absolute: 5        # Alerta se r√©plicas mudam ¬±5

  # CPU/Memory
  cpu_warning_percent: 85
  cpu_critical_percent: 90
  memory_warning_percent: 85
  memory_critical_percent: 90

  # Target deviation
  target_deviation_percent: 30.0   # Alerta se current est√° 30% acima/abaixo do target

  # Scaling behavior
  scaling_stuck_minutes: 10        # Alerta se n√£o escala quando deveria

  # Config changes
  alert_on_config_change: true     # Alertar mudan√ßas em HPA config
  alert_on_resource_change: true   # Alertar mudan√ßas em deployment resources

ui:
  refresh_interval_ms: 500         # Refresh da TUI
  enable_sounds: false             # Sons de alerta (beep)
  theme: "dark"                    # dark, light, monokai

logging:
  level: "info"                    # debug, info, warn, error
  output: "~/.hpa-watchdog/watchdog.log"
  max_size_mb: 100
  max_backups: 3
```

---

## üîå Integra√ß√£o com k8s-hpa-manager

### Reutiliza√ß√£o de C√≥digo

O watchdog pode reutilizar componentes existentes do k8s-hpa-manager:

```go
// Importar m√≥dulos compartilhados
import (
    "k8s-hpa-manager/internal/kubernetes"  // K8s client wrapper
    "k8s-hpa-manager/internal/config"      // Cluster discovery
    "k8s-hpa-manager/internal/models"      // HPAInfo, Namespace
)

// Exemplo: usar KubeConfigManager para descobrir clusters
func discoverClusters() ([]string, error) {
    manager := config.NewKubeConfigManager()
    return manager.DiscoverClusters()
}
```

### Independ√™ncia

Mesmo compartilhando c√≥digo, o watchdog √© **completamente aut√¥nomo**:
- Bin√°rio separado: `hpa-watchdog`
- Configura√ß√£o separada: `~/.hpa-watchdog/`
- N√£o depende do k8s-hpa-manager estar rodando
- Pode rodar em background como daemon

---

## üöÄ Roadmap de Desenvolvimento

### Fase 1: MVP (v1.0.0) - 2 semanas
- [ ] **Setup Projeto**
  - [ ] Estrutura de diret√≥rios
  - [ ] Go modules
  - [ ] Makefile
- [ ] **Core Monitoring**
  - [ ] Collector de m√©tricas K8s
  - [ ] Time-series storage (in-memory)
  - [ ] An√°lise b√°sica de anomalias
- [ ] **TUI B√°sico**
  - [ ] Dashboard view
  - [ ] Alerts panel
  - [ ] Navega√ß√£o b√°sica
- [ ] **Config System**
  - [ ] YAML parser
  - [ ] Thresholds management

### Fase 2: Features Avan√ßadas (v1.1.0) - 1 semana
- [ ] **Enhanced UI**
  - [ ] Cluster view
  - [ ] Config modal interativo
  - [ ] ASCII charts (asciigraph)
- [ ] **Advanced Analysis**
  - [ ] Scaling stuck detection
  - [ ] Target deviation tracking
  - [ ] Trend prediction (opcional)
- [ ] **Persistence**
  - [ ] SQLite storage
  - [ ] Alert history
  - [ ] Export to JSON/CSV

### Fase 3: Polish & Production (v1.2.0) - 1 semana
- [ ] **Production Ready**
  - [ ] Systemd service file
  - [ ] Docker image
  - [ ] Log rotation
- [ ] **Notifications**
  - [ ] Webhook support (Slack, Discord, Teams)
  - [ ] Email alerts (opcional)
- [ ] **Performance**
  - [ ] Optimize memory usage
  - [ ] Parallel cluster scanning
  - [ ] Benchmark large clusters (100+ HPAs)

---

## üì¶ Depend√™ncias

### Core
- **Go 1.23+**: Linguagem principal
- **client-go v0.31.4**: Cliente Kubernetes oficial
- **Bubble Tea v0.24.2**: TUI framework
- **Lipgloss v1.1.0**: Styling

### Adicionais
- **viper**: Config management (YAML)
- **asciigraph**: Gr√°ficos ASCII
- **go-sqlite3** (opcional): Persist√™ncia
- **zerolog**: Structured logging

```bash
go get github.com/charmbracelet/bubbletea@v0.24.2
go get github.com/charmbracelet/lipgloss@v1.1.0
go get k8s.io/client-go@v0.31.4
go get github.com/spf13/viper
go get github.com/guptarohit/asciigraph
go get github.com/rs/zerolog
go get github.com/mattn/go-sqlite3  # opcional
```

---

## üß™ Testing Strategy

### Unit Tests
```bash
# Testar componentes isolados
go test ./internal/monitor/...
go test ./internal/storage/...
go test ./internal/config/...
```

### Integration Tests
```bash
# Testar integra√ß√£o com K8s (requer cluster)
go test ./tests/integration/...
```

### Manual Testing
```bash
# Build e run
make build
./build/hpa-watchdog

# Com config customizada
./build/hpa-watchdog --config /path/to/config.yaml

# Debug mode
./build/hpa-watchdog --debug
```

---

## üìù Comandos CLI

### Usage

```bash
# Iniciar watchdog com config padr√£o
hpa-watchdog

# Especificar config customizada
hpa-watchdog --config /path/to/watchdog.yaml

# Debug mode (logs verbose)
hpa-watchdog --debug

# Mostrar vers√£o
hpa-watchdog version

# Validar configura√ß√£o
hpa-watchdog validate --config watchdog.yaml

# Export hist√≥rico
hpa-watchdog export --output history.json --format json

# Run como daemon (background)
hpa-watchdog daemon --pidfile /var/run/hpa-watchdog.pid
```

---

## üéØ Casos de Uso

### 1. Monitoramento de Produ√ß√£o
```
Operador SRE quer monitorar 5 clusters de produ√ß√£o 24/7.

Workflow:
1. Configurar thresholds conservadores (CPU: 85%, Replicas: 30%)
2. Habilitar persist√™ncia (SQLite)
3. Rodar como systemd service
4. Configurar webhook para Slack em alertas CRITICAL
5. Dashboard em terminal separado para visualiza√ß√£o
```

### 2. Detec√ß√£o de Anomalias P√≥s-Deploy
```
Dev faz deploy novo e quer monitorar comportamento do HPA.

Workflow:
1. Iniciar watchdog com scan_interval=15s
2. Foco no cluster/namespace espec√≠fico
3. Observar gr√°ficos de CPU/Replicas em tempo real
4. Alertas instant√¢neos se CPU spike > 90%
5. Hist√≥rico de 5 min para an√°lise
```

### 3. Auditoria de Mudan√ßas
```
Compliance quer rastrear todas mudan√ßas em HPAs.

Workflow:
1. Habilitar alert_on_config_change: true
2. Habilitar alert_on_resource_change: true
3. Persist√™ncia habilitada
4. Export peri√≥dico para CSV
5. Alertas INFO para cada mudan√ßa detectada
```

---

## üîê Seguran√ßa

### Autentica√ß√£o K8s
- Usa kubeconfig padr√£o (`~/.kube/config`)
- Suporta m√∫ltiplos contexts
- Respeita RBAC do cluster
- N√£o requer permiss√µes de escrita (read-only)

### Permiss√µes Necess√°rias

```yaml
# ClusterRole necess√°ria para watchdog
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: hpa-watchdog-reader
rules:
- apiGroups: [""]
  resources: ["namespaces", "pods"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "statefulsets", "daemonsets"]
  verbs: ["get", "list"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
```

### Dados Sens√≠veis
- N√£o armazena credenciais
- Logs n√£o cont√™m secrets
- Config YAML pode conter apenas nomes de clusters

---

## üìä M√©tricas e Performance

### Benchmarks Esperados

| M√©trica | Target | Observa√ß√µes |
|---------|--------|-------------|
| **Scan Time** | < 5s por cluster | 50 HPAs, 10 namespaces |
| **Memory Usage** | < 100 MB | 5 clusters, 250 HPAs, 5 min hist√≥rico |
| **CPU Usage** | < 5% idle | Scanning em background |
| **Storage** | < 10 MB/dia | SQLite com 1000 alerts/dia |

### Escalabilidade

| Cen√°rio | HPAs | Clusters | Memory | Scan Time |
|---------|------|----------|--------|-----------|
| Small   | 50   | 2        | ~50 MB | ~3s       |
| Medium  | 250  | 5        | ~100 MB| ~10s      |
| Large   | 500  | 10       | ~200 MB| ~20s      |

---

## üêõ Troubleshooting

### Problemas Comuns

#### 1. Cluster Connection Failed
```
Erro: failed to list namespaces in cluster akspriv-prod: context deadline exceeded

Solu√ß√£o:
- Verificar conectividade VPN
- Testar com kubectl: kubectl cluster-info --context=akspriv-prod
- Aumentar timeout na config: client_timeout_seconds: 30
```

#### 2. High Memory Usage
```
Sintoma: Watchdog usando >500 MB RAM

Solu√ß√£o:
- Reduzir history_retention_minutes (padr√£o: 5 min)
- Limitar max_active_alerts (padr√£o: 100)
- Desabilitar persistence se n√£o necess√°rio
```

#### 3. Missing Metrics
```
Sintoma: M√©tricas de CPU/Memory aparecem como 0%

Solu√ß√£o:
- Verificar se Prometheus est√° acess√≠vel: curl http://prometheus.monitoring.svc:9090/-/healthy
- Testar query PromQL manualmente
- Fallback para metrics-server se Prometheus indispon√≠vel
- Verificar se metrics-server est√° instalado: kubectl top pods
```

#### 4. Prometheus Connection Failed
```
Sintoma: Failed to query Prometheus: connection refused

Solu√ß√£o:
- Verificar endpoint Prometheus na config
- Testar conectividade: kubectl port-forward -n monitoring svc/prometheus 9090:9090
- Verificar auto-discovery: prometheus.auto_discover: true
- Habilitar fallback: prometheus.fallback_to_metrics_server: true
```

---

## üîå Integra√ß√£o Prometheus

### Vis√£o Geral

O HPA Watchdog usa **arquitetura h√≠brida** para coleta de dados:
- **K8s API** (kubernetes/client-go): Configura√ß√£o e estado dos HPAs
- **Prometheus** (prometheus/client_golang): M√©tricas e hist√≥rico temporal

### Vantagens do Prometheus

‚úÖ **Hist√≥rico Nativo** - Prometheus j√° armazena m√©tricas (n√£o precisa cache local)
‚úÖ **M√©tricas Ricas** - CPU, Memory, Network, Request Rate, Errors, Latency
‚úÖ **An√°lise Temporal** - Queries range f√°ceis (`[5m]`, `rate()`, `increase()`)
‚úÖ **Performance** - PromQL otimizado para TSDB
‚úÖ **Correla√ß√£o** - Cruzar HPA com tr√°fego, erros, latency

### Queries Prometheus √öteis

#### 1. CPU Usage (HPA Target)
```promql
# CPU atual dos pods do HPA (%)
sum(rate(container_cpu_usage_seconds_total{
    namespace="api-gateway",
    pod=~"nginx-.*"
}[1m])) /
sum(kube_pod_container_resource_requests{
    namespace="api-gateway",
    pod=~"nginx-.*",
    resource="cpu"
}) * 100
```

#### 2. Memory Usage
```promql
# Memory atual (%)
sum(container_memory_working_set_bytes{
    namespace="api-gateway",
    pod=~"nginx-.*"
}) /
sum(kube_pod_container_resource_requests{
    namespace="api-gateway",
    pod=~"nginx-.*",
    resource="memory"
}) * 100
```

#### 3. Replica History
```promql
# R√©plicas atuais (√∫ltimos 5 min)
kube_horizontalpodautoscaler_status_current_replicas{
    namespace="api-gateway",
    horizontalpodautoscaler="nginx"
}[5m]
```

#### 4. Request Rate
```promql
# Requests por segundo
sum(rate(http_requests_total{
    namespace="api-gateway",
    service="nginx"
}[1m]))
```

#### 5. Error Rate
```promql
# Taxa de erro (%)
sum(rate(http_requests_total{
    namespace="api-gateway",
    status=~"5.."
}[1m])) /
sum(rate(http_requests_total{
    namespace="api-gateway"
}[1m])) * 100
```

#### 6. P95 Latency
```promql
# P95 latency (ms)
histogram_quantile(0.95,
    sum(rate(http_request_duration_seconds_bucket{
        namespace="api-gateway"
    }[1m])) by (le)
) * 1000
```

### Auto-Discovery de Prometheus

```yaml
# watchdog.yaml
monitoring:
  prometheus:
    enabled: true
    auto_discover: true  # Descobre endpoint via K8s Service

    # Padr√µes de busca (padr√£o)
    discovery_patterns:
      - "prometheus.monitoring.svc:9090"
      - "prometheus-server.prometheus.svc:9090"
      - "kube-prometheus-stack-prometheus.monitoring.svc:9090"

    # Ou especificar manualmente
    endpoints:
      akspriv-prod-east: "http://prometheus.monitoring.svc:9090"
```

### Fallback para Metrics-Server

```yaml
monitoring:
  prometheus:
    enabled: true
    fallback_to_metrics_server: true  # Usa K8s metrics-server se Prometheus falhar
```

**Comportamento:**
1. Tenta Prometheus primeiro
2. Se falhar (timeout, connection refused), usa metrics-server
3. M√©tricas b√°sicas (CPU/Memory) via K8s API
4. Sem hist√≥rico rico nem extended metrics

### Exemplo de Alerta Enriquecido

**Antes (s√≥ K8s API):**
```
üî¥ CPU spike: 95% (limite: 90%)
```

**Depois (com Prometheus):**
```
üî¥ ANOMALIA CR√çTICA DETECTADA

üìä HPA: api-gateway/nginx
‚è∞ Timestamp: 14:35:22

üî• CPU Spike: 95% (limite: 90%)
   Hist√≥rico 5min: [72%, 75%, 78%, 85%, 92%, 95%]
   Tend√™ncia: ‚ÜóÔ∏è Alta consistente

üìà Contexto (Prometheus):
   ‚Ä¢ Request rate: 850 req/s (‚Üë300% vs baseline 200 req/s)
   ‚Ä¢ Error rate: 12% (‚Üë de 0.5% normal)
   ‚Ä¢ P95 latency: 2.5s (‚Üë de 150ms normal)
   ‚Ä¢ R√©plicas: 8 ‚Üí 10 (scaling ativo)

üí° Diagn√≥stico Autom√°tico:
   Traffic spike + high errors + latency degradation
   = Prov√°vel incident upstream ou capacidade insuficiente

üö® A√ß√£o Sugerida:
   1. Verificar upstream services (dependencies)
   2. Revisar logs para errors
   3. Considerar aumentar max_replicas se traffic sustentado
```

---

## üìö Documenta√ß√£o Adicional

### README.md do Projeto
- Quick Start
- Installation
- Screenshots
- Contributing

### docs/ARCHITECTURE.md
- Detalhamento t√©cnico da arquitetura
- Diagramas de sequ√™ncia
- Decis√µes de design

### docs/API.md (se houver API REST futura)
- Endpoints
- Request/Response examples
- Authentication

---

## ü§ù Contribui√ß√£o

### Como Contribuir
1. Fork do reposit√≥rio
2. Branch: `git checkout -b feature/nova-feature`
3. Commit: `git commit -m 'feat: adiciona nova feature'`
4. Push: `git push origin feature/nova-feature`
5. Pull Request

### Code Style
- Seguir padr√µes do k8s-hpa-manager
- Usar Bubble Tea commands para async
- Unicode-safe (sempre `[]rune` para texto)
- Error handling adequado

---

## üìÑ Licen√ßa

MIT License - mesmo do k8s-hpa-manager principal

---

## üîó Refer√™ncias

- [Bubble Tea Documentation](https://github.com/charmbracelet/bubbletea)
- [Kubernetes client-go](https://github.com/kubernetes/client-go)
- [HPA Best Practices](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [k8s-hpa-manager](https://github.com/Paulo-Ribeiro-Log/Scale_HPA)

---

**√öltima Atualiza√ß√£o**: 23 de outubro de 2025
**Autor**: Paulo Ribeiro
**Status**: üü° Planejamento - Pronto para desenvolvimento
