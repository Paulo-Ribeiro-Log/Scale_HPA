# Monitor Package

Unified collector that integrates K8s API, Prometheus, Storage, and Analyzer for HPA monitoring.

## ğŸ“‹ Components

### 1. K8sClient (`k8s_client.go`)
Wrapper for Kubernetes client-go with cluster context.

**Features:**
- Multi-cluster support via kubeconfig contexts
- HPA listing and snapshot collection
- Deployment resource extraction
- Namespace filtering
- Connection testing

**Usage:**
```go
cluster := &models.ClusterInfo{
    Name:    "production",
    Context: "prod-cluster",
    Server:  "https://api.prod.k8s.io",
}

k8sClient, err := monitor.NewK8sClient(cluster)
if err != nil {
    log.Fatal(err)
}

// Test connection
ctx := context.Background()
if err := k8sClient.TestConnection(ctx); err != nil {
    log.Fatal(err)
}

// List HPAs
hpas, err := k8sClient.ListHPAs(ctx, "default")
for _, hpa := range hpas {
    snapshot, err := k8sClient.CollectHPASnapshot(ctx, &hpa)
    // ... use snapshot
}
```

### 2. Collector (`collector.go`) âœ…

Unified collector that orchestrates K8s + Prometheus + Analyzer.

**Features:**
- Automatic HPA discovery across namespaces
- Prometheus enrichment (optional)
- Time-series cache integration
- Anomaly detection
- Monitoring loop with configurable interval
- Non-blocking result channel

**Architecture:**
```
Collector
â”œâ”€ K8sClient: Collects HPA state from K8s API
â”œâ”€ PrometheusClient: Enriches with metrics (optional)
â”œâ”€ TimeSeriesCache: Stores snapshots in-memory
â””â”€ Detector: Analyzes and detects anomalies
```

**Usage:**
```go
cluster := &models.ClusterInfo{
    Name:    "production",
    Context: "prod-cluster",
    Server:  "https://api.prod.k8s.io",
}

config := monitor.DefaultCollectorConfig()
config.ScanInterval = 30 * time.Second
config.EnablePrometheus = true
config.ExcludeNamespaces = []string{"monitoring", "logging"}

// Create collector
collector, err := monitor.NewCollector(
    cluster,
    "http://prometheus.monitoring.svc:9090", // Prometheus endpoint
    config,
)
if err != nil {
    log.Fatal(err)
}

// Single scan
ctx := context.Background()
result, err := collector.Scan(ctx)
if err != nil {
    log.Fatal(err)
}

fmt.Printf("Snapshots: %d\n", result.SnapshotsCount)
fmt.Printf("Anomalies: %d\n", len(result.Anomalies))

// Continuous monitoring
resultChan := make(chan *monitor.ScanResult, 10)

ctx, cancel := context.WithCancel(context.Background())
defer cancel()

go collector.StartMonitoring(ctx, resultChan)

for result := range resultChan {
    fmt.Printf("[%s] Snapshots: %d, Anomalies: %d\n",
        result.Timestamp.Format(time.RFC3339),
        result.SnapshotsCount,
        len(result.Anomalies),
    )

    for _, anomaly := range result.Anomalies {
        fmt.Printf("  - %s: %s\n", anomaly.Type, anomaly.Message)
    }
}
```

## ğŸ”„ Monitoring Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Collector.StartMonitoring()                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Every 30s (configurable):                                  â”‚
â”‚                                                              â”‚
â”‚  1. List Namespaces (exclude system namespaces)            â”‚
â”‚     â”œâ”€ kube-system, kube-public, etc skipped               â”‚
â”‚     â””â”€ Custom excludes from config                         â”‚
â”‚                                                              â”‚
â”‚  2. For each namespace:                                     â”‚
â”‚     â”œâ”€ List HPAs via K8s API                               â”‚
â”‚     â”‚                                                        â”‚
â”‚     â””â”€ For each HPA:                                        â”‚
â”‚        â”œâ”€ Collect HPA snapshot from K8s                    â”‚
â”‚        â”‚  â”œâ”€ HPA config (min/max replicas)                 â”‚
â”‚        â”‚  â”œâ”€ Current state (replicas, ready)               â”‚
â”‚        â”‚  â””â”€ Deployment resources (CPU/Memory)             â”‚
â”‚        â”‚                                                     â”‚
â”‚        â”œâ”€ Enrich with Prometheus (if available)            â”‚
â”‚        â”‚  â”œâ”€ CPU/Memory current usage                      â”‚
â”‚        â”‚  â”œâ”€ Historical data (5min)                        â”‚
â”‚        â”‚  â””â”€ Extended metrics (errors, latency)            â”‚
â”‚        â”‚                                                     â”‚
â”‚        â””â”€ Add snapshot to TimeSeriesCache                  â”‚
â”‚                                                              â”‚
â”‚  3. Detect Anomalies                                        â”‚
â”‚     â”œâ”€ Analyzer.Detect() uses cache                        â”‚
â”‚     â”œâ”€ Checks 5 critical anomalies (Phase 1 MVP)          â”‚
â”‚     â””â”€ Returns detected anomalies                          â”‚
â”‚                                                              â”‚
â”‚  4. Send ScanResult to channel                             â”‚
â”‚     â”œâ”€ Snapshots count                                     â”‚
â”‚     â”œâ”€ Anomalies detected                                  â”‚
â”‚     â”œâ”€ Errors encountered                                  â”‚
â”‚     â””â”€ Scan duration                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

### Unit Tests (short mode)
```bash
go test ./internal/monitor/... -short -v
```

### Integration Tests (requires cluster)
```bash
go test ./internal/monitor/... -v
```

---

**Status:** âœ… Phase 2 Complete
**Next:** TUI implementation (Phase 3)
