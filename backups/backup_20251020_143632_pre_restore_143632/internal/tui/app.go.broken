package tui

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"

	tea "github.com/charmbracelet/bubbletea"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	k8sClientSet "k8s.io/client-go/kubernetes"

	"k8s-hpa-manager/internal/azure"
	"k8s-hpa-manager/internal/config"
	"k8s-hpa-manager/internal/kubernetes"
	"k8s-hpa-manager/internal/models"
	"k8s-hpa-manager/internal/session"
	"k8s-hpa-manager/internal/tui/layout"
	"k8s-hpa-manager/internal/ui"
)

// App representa a aplica√ß√£o principal
type App struct {
	// Configura√ß√£o
	kubeconfigPath string
	debug          bool

	// Managers
	kubeManager    *config.KubeConfigManager
	sessionManager *session.Manager
	clients        map[string]*kubernetes.Client

	// Estado da aplica√ß√£o
	model *models.AppModel

	// UI Components
	width  int
	height int

	// Contexto
	ctx    context.Context
	cancel context.CancelFunc

	// Thread safety para rollouts
	rolloutMutex sync.RWMutex
}

// debugLog imprime mensagens apenas quando debug est√° habilitado
func (a *App) debugLog(format string, args ...interface{}) {
	if a.debug {
		fmt.Printf(format+"\n", args...)

		// Tamb√©m escrever para arquivo de debug se poss√≠vel
		if file, err := os.OpenFile("k8s-hpa-debug.log", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644); err == nil {
			fmt.Fprintf(file, format+"\n", args...)
			file.Close()
		}
	}
}

// NewApp cria uma nova inst√¢ncia da aplica√ß√£o
func NewApp(kubeconfigPath string, debug bool) *App {
	ctx, cancel := context.WithCancel(context.Background())

	app := &App{
		kubeconfigPath: kubeconfigPath,
		debug:          debug,
		clients:        make(map[string]*kubernetes.Client),
		ctx:            ctx,
		cancel:         cancel,
		// Inicializar com resolu√ß√£o m√≠nima para garantir visibilidade
		width:          layout.MinTerminalWidth,
		height:         layout.MinTerminalHeight,
		model: &models.AppModel{
			State:               models.StateClusterSelection,
			Loading:             false,
			SelectedIndex:       0,
			ActivePanel:         models.PanelNamespaces,
			SelectedNamespaces:  make([]models.Namespace, 0),
			SelectedHPAs:        make([]models.HPA, 0),
			CurrentNamespaceIdx: 0,
			FormFields:          make(map[string]string),
			// Inicializar Status Panel com Rich-style progress bars
			StatusPanel:         ui.NewStatusPanel("üìä Status e Informa√ß√µes", ui.DefaultStatusPanelConfig()),
		},
	}

	// Log das dimens√µes iniciais
	app.debugLog("üöÄ App iniciada com dimens√µes iniciais: %dx%d", app.width, app.height)

	return app
}

// getStatusPanel retorna o status panel com type safety
func (a *App) getStatusPanel() *ui.StatusPanel {
	if sp, ok := a.model.StatusPanel.(*ui.StatusPanel); ok {
		return sp
	}
	return nil
}

// Init implementa tea.Model interface
func (a *App) Init() tea.Cmd {
	return a.initializeManagers()
}

// initializeManagers inicializa os gerenciadores
func (a *App) initializeManagers() tea.Cmd {
	return func() tea.Msg {
		kubeManager, err := config.NewKubeConfigManager(a.kubeconfigPath)
		if err != nil {
			return initManagersMsg{err: err}
		}

		sessionManager, err := session.NewManager()
		if err != nil {
			return initManagersMsg{kubeManager: kubeManager, err: err}
		}

		return initManagersMsg{
			kubeManager:    kubeManager,
			sessionManager: sessionManager,
			err:            nil,
		}
	}
}

// Update implementa tea.Model interface
func (a *App) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg := msg.(type) {
	case tea.WindowSizeMsg:
		// Aplicar resolu√ß√£o m√≠nima 188x45 para garantir visibilidade completa do layout
		a.debugLog("üñ•Ô∏è  Terminal resize: %dx%d", msg.Width, msg.Height)
		oldWidth, oldHeight := a.width, a.height
		a.width, a.height = a.applyTerminalSizeLimit(msg.Width, msg.Height)
		a.debugLog("üñ•Ô∏è  Applied limits: %dx%d -> %dx%d", oldWidth, oldHeight, a.width, a.height)
		return a, nil

	case clearScreenMsg:
		// For√ßa redesenho completo da tela
		return a, tea.ClearScreen

	case tea.KeyMsg:
		return a.handleKeyPress(msg)

	case tea.MouseMsg:
		return a.handleMouseEvent(msg)

	case initManagersMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to initialize: %v", msg.err)
			return a, nil
		}
		a.kubeManager = msg.kubeManager
		a.sessionManager = msg.sessionManager

		// Descobrir clusters automaticamente
		return a, a.discoverClusters()

	case clustersDiscoveredMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to discover clusters: %v", msg.err)
			return a, nil
		}
		a.model.Clusters = msg.clusters
		a.model.Loading = false
		return a, a.testClusterConnections()

	case clusterConnectionTestMsg:
		a.updateClusterStatus(msg.cluster, msg.status, msg.err)
		return a, nil

	case namespacesLoadedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load namespaces: %v", msg.err)
			a.model.Loading = false
			return a, nil
		}
		// Substituir namespaces (n√£o append) para evitar duplicatas
		a.model.Namespaces = msg.namespaces
		a.model.Loading = false
		// Iniciar contagem de HPAs em background
		return a, tea.Batch(tea.ClearScreen, a.loadHPACounts())

	case hpasLoadedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load HPAs: %v", msg.err)
			return a, nil
		}
		// Substituir HPAs para o namespace atual
		a.model.HPAs = msg.hpas
		return a, tea.ClearScreen

	case hpaDeploymentResourcesEnrichedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load deployment resources: %v", msg.err)
		} else {
			a.model.SuccessMsg = fmt.Sprintf("Deployment resources loaded for %s", msg.hpa.DeploymentName)
		}
		return a, nil

	case sessionHPAsEnrichedMsg:
		// HPAs da sess√£o foram enriquecidos com dados de deployment do cluster
		if msg.enrichedCount > 0 {
			a.debugLog("üìä %d HPAs da sess√£o enriquecidos com dados atuais do cluster\n", msg.enrichedCount)
		}
		return a, nil

	case hpaChangesAppliedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to apply HPA changes: %v", msg.err)
			a.model.SuccessMsg = ""
		} else {
			// Atualizar HPAs aplicados com sucesso - limpar Modified e sincronizar contadores
			for _, appliedHPA := range msg.appliedHPAs {
				for i := range a.model.SelectedHPAs {
					// Encontrar o HPA correspondente e atualizar estado
					if a.model.SelectedHPAs[i].Name == appliedHPA.Name &&
						a.model.SelectedHPAs[i].Namespace == appliedHPA.Namespace &&
						a.model.SelectedHPAs[i].Cluster == appliedHPA.Cluster {
						a.model.SelectedHPAs[i].Modified = false
						// Sincronizar contador de aplica√ß√µes
						a.model.SelectedHPAs[i].AppliedCount = appliedHPA.AppliedCount
						a.model.SelectedHPAs[i].LastAppliedAt = appliedHPA.LastAppliedAt
						break
					}
				}
			}
			// Mostrar mensagem de sucesso
			a.model.SuccessMsg = fmt.Sprintf("Aplicadas mudan√ßas em %d HPA(s)", msg.count)
			a.model.Error = ""
		}
		// Iniciar timer para limpar mensagens ap√≥s 5 segundos
		return a, a.clearStatusMessages()

	case hpaCountUpdatedMsg:
		if msg.err == nil {
			// Atualizar contagem de HPAs no namespace correspondente
			for i := range a.model.Namespaces {
				if a.model.Namespaces[i].Name == msg.namespace {
					a.model.Namespaces[i].HPACount = msg.count
					break
				}
			}
		}
		return a, nil

	case sessionSavedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to save session: %v", msg.err)
			a.model.SuccessMsg = ""
		} else {
			a.model.SuccessMsg = fmt.Sprintf("üíæ Sess√£o '%s' salva com sucesso", msg.sessionName)
			a.model.Error = ""
		}
		return a, a.clearStatusMessages()

	case sessionDeletedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to delete session: %v", msg.err)
			a.model.SuccessMsg = ""
		} else {
			a.model.SuccessMsg = fmt.Sprintf("üóëÔ∏è Sess√£o '%s' deletada com sucesso", msg.sessionName)
			a.model.Error = ""
			// Recarregar lista de sess√µes da pasta atual ap√≥s dele√ß√£o
			if a.model.CurrentFolder != "" {
				return a, a.loadSessionsFromFolder(a.model.CurrentFolder)
			} else {
				return a, a.loadSessions()
			}
		}
		return a, a.clearStatusMessages()

	case sessionRenamedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to rename session: %v", msg.err)
			a.model.SuccessMsg = ""
		} else {
			a.model.SuccessMsg = fmt.Sprintf("‚úèÔ∏è Sess√£o '%s' renomeada para '%s' com sucesso", msg.oldName, msg.newName)
			a.model.Error = ""
			// Recarregar lista de sess√µes da pasta atual ap√≥s renome
			if a.model.CurrentFolder != "" {
				return a, a.loadSessionsFromFolder(a.model.CurrentFolder)
			} else {
				return a, a.loadSessions()
			}
		}
		return a, a.clearStatusMessages()

	case sessionFoldersLoadedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load session folders: %v", msg.err)
		} else {
			a.model.SessionFolders = msg.folders
		}
		return a, nil

	case sessionsLoadedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load sessions: %v", msg.err)
		} else {
			a.model.LoadedSessions = msg.sessions
			a.model.SelectedSessionIdx = 0
		}
		return a, nil

	case sessionStateLoadedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load session state: %v", msg.err)
			a.model.State = models.StateClusterSelection
			return a, nil
		}

		// Encontrar e selecionar o cluster
		for i := range a.model.Clusters {
			if a.model.Clusters[i].Name == msg.clusterName {
				a.model.SelectedCluster = &a.model.Clusters[i]
				break
			}
		}

		if a.model.SelectedCluster == nil {
			a.model.Error = fmt.Sprintf("Cluster '%s' n√£o encontrado", msg.clusterName)
			a.model.State = models.StateClusterSelection
			return a, nil
		}

		// Armazenar nome da sess√£o carregada
		a.model.LoadedSessionName = msg.sessionName
		
		// Verificar se √© sess√£o de node pools ou HPAs
		a.debugLog("üîç Processando sessionStateLoadedMsg: nodePools=%d, hpas=%d\n",
			len(msg.nodePools), len(msg.hpas))

		if len(msg.nodePools) > 0 {
			// √â uma sess√£o de node pools
			a.debugLog("üîß Configurando sess√£o de node pools\n")
			a.model.NodePools = msg.nodePools
			a.model.SelectedNodePools = make([]models.NodePool, 0)

			// Adicionar pools modificados √† lista de selecionados
			for _, pool := range msg.nodePools {
				if pool.Modified {
					a.model.SelectedNodePools = append(a.model.SelectedNodePools, pool)
					a.debugLog("‚úì Pool '%s' adicionado aos selecionados\n", pool.Name)
				}
			}

			// Transicionar para tela de node pools
			a.model.State = models.StateNodeSelection
			a.model.ActivePanel = models.PanelSelectedNodePools
			a.model.SelectedIndex = 0

			a.debugLog("üéØ Estado alterado para StateNodeSelection com %d pools selecionados\n",
				len(a.model.SelectedNodePools))

			a.model.SuccessMsg = fmt.Sprintf("üìö Sess√£o de node pools '%s' carregada com sucesso. %d pool(s) modificado(s).",
				msg.sessionName, len(a.model.SelectedNodePools))
				
		} else {
			// √â uma sess√£o de HPAs (c√≥digo original)
			// Criar cliente Kubernetes para o cluster se n√£o existir
			clusterName := a.model.SelectedCluster.Name
			_, exists := a.clients[clusterName]
			if !exists {
				if a.kubeManager == nil {
					a.model.Error = "Kube manager not initialized"
					a.model.State = models.StateClusterSelection
					return a, nil
				}
				
				clientSet, err := a.kubeManager.GetClient(a.model.SelectedCluster.Context)
				if err != nil {
					a.model.Error = fmt.Sprintf("N√£o foi poss√≠vel conectar ao cluster %s: %v", clusterName, err)
					a.model.State = models.StateClusterSelection
					return a, nil
				}
				
				newClient := kubernetes.NewClient(clientSet, clusterName)
				a.clients[clusterName] = newClient
			}

			// Restaurar namespaces selecionados
			a.model.SelectedNamespaces = msg.namespaces
			a.model.Namespaces = msg.namespaces // Definir namespaces dispon√≠veis
			
			// Restaurar HPAs selecionados com modifica√ß√µes da sess√£o
			a.model.SelectedHPAs = msg.hpas
			a.model.HPAs = msg.hpas // Definir HPAs dispon√≠veis

			// Transicionar para tela de sele√ß√£o de HPAs para permitir edi√ß√£o
			a.model.State = models.StateHPASelection
			a.model.ActivePanel = models.PanelSelectedHPAs
			a.model.SelectedIndex = 0
			a.model.CurrentNamespaceIdx = 0

			a.model.SuccessMsg = fmt.Sprintf("üìö Sess√£o de HPAs '%s' carregada com sucesso", msg.sessionName)

			// Enriquecer HPAs que n√£o possuem dados de deployment
			return a, a.enrichSessionHPAs()
		}
		
		return a, nil

	case azureAuthStartMsg:
		a.model.Loading = true
		a.model.Error = ""
		return a, a.performAzureAuth()

	case azureAuthResultMsg:
		a.model.Loading = false
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Azure CLI authentication failed: %v", msg.err)
			return a, nil
		}
		a.model.SuccessMsg = "‚úÖ Azure CLI authentication successful"
		// Continue with node pool loading after successful authentication
		return a, a.loadNodePools()

	case nodePoolsConfiguratingSubscriptionMsg:
		// Mostrar mensagem "Configurando subscription" e continuar com configura√ß√£o
		a.model.SuccessMsg = fmt.Sprintf("üîÑ Configurando subscription: %s", msg.clusterConfig.Subscription)
		return a, configurateSubscription(msg.clusterConfig)

	case nodePoolsLoadedMsg:
		// Processar log do Azure primeiro, se presente
		var cmd tea.Cmd
		if msg.azureLogMsg != nil {
			statusPanel := a.getStatusPanel()
			switch msg.azureLogMsg.level {
			case "error":
				statusPanel.Error(msg.azureLogMsg.source, msg.azureLogMsg.message)
			case "success":
				statusPanel.Success(msg.azureLogMsg.source, msg.azureLogMsg.message)
			default:
				statusPanel.Info(msg.azureLogMsg.source, msg.azureLogMsg.message)
			}
		}

		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load node pools: %v", msg.err)
			a.model.Loading = false
			return a, cmd
		}

		a.model.NodePools = msg.nodePools
		a.model.Loading = false
		return a, tea.Batch(cmd, tea.ClearScreen)

	case cronJobsLoadedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to load cronjobs: %v", msg.err)
			a.model.Loading = false
			return a, nil
		}
		a.model.CronJobs = msg.cronJobs
		a.model.SelectedCronJobs = make([]models.CronJob, 0)
		a.model.Loading = false
		a.model.SelectedIndex = 0
		return a, tea.ClearScreen

	case cronJobUpdateMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to update cronjobs: %v", msg.err)
			return a, nil
		}
		a.model.SuccessMsg = "‚úÖ CronJobs atualizados com sucesso"
		// Recarregar CronJobs para mostrar estado atual
		return a, a.loadCronJobs()

	case clusterResourcesDiscoveredMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to discover cluster resources: %v", msg.err)
			a.model.Loading = false
			return a, nil
		}
		a.model.ClusterResources = msg.resources
		a.model.SelectedResources = make([]models.ClusterResource, 0)
		a.model.Loading = false
		
		// Ir para estado de sele√ß√£o de recursos
		if a.model.PrometheusStackMode {
			a.model.State = models.StatePrometheusStackManagement
		} else {
			a.model.State = models.StateClusterResourceSelection
		}
		
		return a, tea.ClearScreen

	case nodePoolUpdateMsg:
		// Atualizar node pool modificado
		for i := range a.model.SelectedNodePools {
			if a.model.SelectedNodePools[i].Name == msg.nodePool.Name {
				a.model.SelectedNodePools[i] = msg.nodePool
				break
			}
		}
		return a, nil

	case nodePoolsAppliedMsg:
		if msg.err != nil {
			a.model.Error = fmt.Sprintf("Failed to apply node pool changes: %v", msg.err)
			a.model.SuccessMsg = ""
		} else {
			a.model.SuccessMsg = fmt.Sprintf("‚úÖ Aplicadas mudan√ßas em %d node pool(s)", len(msg.appliedPools))
			a.model.Error = ""

			// Marcar node pools aplicados como n√£o modificados
			for _, appliedPool := range msg.appliedPools {
				for i := range a.model.SelectedNodePools {
					if a.model.SelectedNodePools[i].Name == appliedPool.Name &&
						a.model.SelectedNodePools[i].ClusterName == appliedPool.ClusterName {
						a.model.SelectedNodePools[i].Modified = false

						// Se este node pool est√° em uma sequ√™ncia, marcar como completado
						if a.model.SelectedNodePools[i].SequenceOrder > 0 {
							a.model.SelectedNodePools[i].SequenceStatus = "completed"
							a.debugLog("‚úÖ Node pool %s (ordem %d) marcado como completed",
								a.model.SelectedNodePools[i].Name, a.model.SelectedNodePools[i].SequenceOrder)
						}

						// Atualizar valores originais para refletir o estado atual
						a.model.SelectedNodePools[i].OriginalValues = models.NodePoolValues{
							NodeCount:    appliedPool.NodeCount,
							MinNodeCount: appliedPool.MinNodeCount,
							MaxNodeCount: appliedPool.MaxNodeCount,
							AutoscalingEnabled: appliedPool.AutoscalingEnabled,
						}
						break
					}
				}
			}

			// Verificar se deve iniciar execu√ß√£o sequencial do pr√≥ximo node pool
			cmd := a.checkAndStartSequentialExecution()
			if cmd != nil {
				return a, cmd
			}
		}
		return a, a.clearStatusMessages()

	case resourceChangeAppliedMsg:
		return a.handleResourceChangeApplied(msg)
		
	case resourcesBatchAppliedMsg:
		return a.handleResourcesBatchApplied(msg)
		
	case prometheusStackAppliedMsg:
		return a.handlePrometheusStackApplied(msg)

	case progressUpdateMsg:
		// Atualizar interface durante rollouts
		return a, tea.Tick(500*time.Millisecond, func(t time.Time) tea.Msg {
			return progressUpdateMsg{}
		})

	case cleanupRolloutsMsg:
		// Limpar rollouts conclu√≠dos ap√≥s delay
		a.cleanupCompletedRollouts()
		return a, nil

	case clearStatusMsg:
		// Limpar mensagens de status
		a.model.SuccessMsg = ""
		a.model.Error = ""
		return a, tea.ClearScreen

	case statusLogMsg:
		// Adicionar log ao StatusPanel
		statusPanel := a.getStatusPanel()
		switch msg.level {
		case "info":
			statusPanel.Info(msg.source, msg.message)
		case "error":
			statusPanel.Error(msg.source, msg.message)
		case "success":
			statusPanel.Success(msg.source, msg.message)
		case "warn", "warning":
			statusPanel.Warning(msg.source, msg.message)
		case "debug":
			statusPanel.Debug(msg.source, msg.message)
		default:
			statusPanel.Info(msg.source, msg.message)
		}
		return a, nil
	}

	return a, nil
}

// applyTerminalSizeLimit - Aplica resolu√ß√£o m√≠nima 188x45 para garantir visibilidade completa
func (a *App) applyTerminalSizeLimit(width, height int) (int, int) {
	// Definir resolu√ß√£o m√≠nima para garantir que todos os elementos sejam vis√≠veis
	minWidth := layout.MinTerminalWidth   // Resolu√ß√£o m√≠nima de largura - todos os pain√©is vis√≠veis
	minHeight := layout.MinTerminalHeight // Resolu√ß√£o m√≠nima de altura - layout completo vis√≠vel

	a.debugLog("üîß Checking limits: input=%dx%d, min=%dx%d", width, height, minWidth, minHeight)

	// Se o terminal for menor que o m√≠nimo, ajustar para evitar elementos escondidos
	if width < minWidth {
		a.debugLog("üîß Width too small: %d -> %d", width, minWidth)
		width = minWidth
	}
	if height < minHeight {
		a.debugLog("üîß Height too small: %d -> %d", height, minHeight)
		height = minHeight
	}

	a.debugLog("üîß Final size: %dx%d", width, height)
	return width, height
}

// View implementa tea.Model interface
func (a *App) View() string {
	if a.width == 0 {
		return "Initializing..."
	}

	// Garantir que sempre usamos as dimens√µes m√≠nimas
	if a.width < layout.MinTerminalWidth || a.height < layout.MinTerminalHeight {
		a.debugLog("‚ö†Ô∏è  View() detectou dimens√µes abaixo do m√≠nimo: %dx%d, for√ßando %dx%d",
			a.width, a.height, layout.MinTerminalWidth, layout.MinTerminalHeight)
		a.width = layout.MinTerminalWidth
		a.height = layout.MinTerminalHeight
	}

	// Renderizar conte√∫do baseado no estado atual
	var content string

	switch a.model.State {
	case models.StateClusterSelection:
		content = a.renderClusterSelection()
	case models.StateSessionFolderSelection:
		content = a.renderSessionFolderSelection()
	case models.StateSessionSelection:
		content = a.renderSessionSelection()
	case models.StateNamespaceSelection:
		content = a.renderNamespaceSelection()
	case models.StateHPASelection:
		content = a.renderHPASelection()
	case models.StateHPAEditing:
		content = a.renderHPAEditing()
	case models.StateNodeSelection:
		content = a.renderNodePoolSelection()
	case models.StateNodeEditing:
		content = a.renderNodePoolEditing()
	case models.StateMixedSession:
		content = a.renderMixedSession()
	case models.StateClusterResourceDiscovery:
		content = a.renderClusterResourceDiscovery()
	case models.StateClusterResourceSelection:
		content = a.renderClusterResourceSelection()
	case models.StateClusterResourceEditing:
		content = a.renderClusterResourceEditing()
	case models.StatePrometheusStackManagement:
		content = a.renderPrometheusStackManagement()
	case models.StateCronJobSelection:
		content = a.renderCronJobSelection()
	case models.StateCronJobEditing:
		content = a.renderCronJobEditing()
	case models.StateHelp:
		content = a.renderHelp()
	default:
		content = "Unknown state"
	}

	return content
}

// handleKeyPress processa as teclas pressionadas
func (a *App) handleKeyPress(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	// Se h√° mensagem de erro, ESC limpa o erro
	if a.model.Error != "" {
		if msg.String() == "esc" {
			a.model.Error = ""
			return a, nil
		}
		// Outras teclas n√£o fazem nada na tela de erro (exceto F4 que √© tratado abaixo)
		if msg.String() != "f4" && msg.String() != "ctrl+c" {
			return a, nil
		}
	}
	
	// Se h√° mensagem de sucesso, qualquer tecla limpa
	if a.model.SuccessMsg != "" {
		a.model.SuccessMsg = ""
		return a, nil
	}
	
	switch msg.String() {
	case "ctrl+c", "f4":
		a.cancel()
		// Limpar a tela antes de sair
		fmt.Print("\033[2J\033[H")
		return a, tea.Quit

	case "esc":
		// Voltar ao estado anterior ou fechar modal
		return a.handleEscape()
		
	case "?":
		// Mostrar ajuda
		if a.model.State != models.StateHelp {
			a.model.PreviousState = a.model.State
			a.model.State = models.StateHelp
		}
		return a, nil
		
	case "f7":
		// Gerenciar recursos do cluster (todos)
		return a.handleF7AllResources()
		
	case "f8":
		// Gerenciar recursos Prometheus
		return a.handleF8PrometheusResources()

	case "f9":
		// Gerenciamento de CronJobs
		if a.model.SelectedCluster == nil {
			a.model.Error = "Nenhum cluster selecionado"
			return a, nil
		}
		a.debugLog("üîß F9 pressionado - carregando CronJobs do cluster %s", a.model.SelectedCluster.Name)
		a.model.State = models.StateCronJobSelection
		a.model.Loading = true
		a.model.CronJobs = make([]models.CronJob, 0)
		a.model.SelectedCronJobs = make([]models.CronJob, 0)
		a.model.SelectedIndex = 0
		return a, a.loadCronJobs()
	}

	// Delegar para handler espec√≠fico baseado no estado
	switch a.model.State {
	case models.StateClusterSelection:
		return a.handleClusterSelectionKeys(msg)
	case models.StateSessionFolderSelection:
		return a.handleSessionFolderSelectionKeys(msg)
	case models.StateSessionSelection:
		return a.handleSessionSelectionKeys(msg)
	case models.StateNamespaceSelection:
		return a.handleNamespaceSelectionKeys(msg)
	case models.StateHPASelection:
		return a.handleHPASelectionKeys(msg)
	case models.StateHPAEditing:
		return a.handleHPAEditingKeys(msg)
	case models.StateNodeSelection:
		return a.handleNodePoolSelectionKeys(msg)
	case models.StateNodeEditing:
		return a.handleNodePoolEditingKeys(msg)
	case models.StateMixedSession:
		return a.handleMixedSessionKeys(msg)
	case models.StateClusterResourceDiscovery:
		// Durante descoberta, apenas ESC funciona (j√° tratado acima)
		return a, nil
	case models.StateClusterResourceSelection:
		return a.handleClusterResourceSelectionKeys(msg)
	case models.StateClusterResourceEditing:
		return a.handleClusterResourceEditingKeys(msg)
	case models.StatePrometheusStackManagement:
		return a.handlePrometheusStackKeys(msg)
	case models.StateCronJobSelection:
		return a.handleCronJobSelectionKeys(msg)
	case models.StateCronJobEditing:
		return a.handleCronJobEditingKeys(msg)
	case models.StateHelp:
		return a.handleHelpKeys(msg)
	}

	return a, nil
}

// handleEscape lida com a tecla ESC
func (a *App) handleEscape() (tea.Model, tea.Cmd) {
	// Se est√° editando um campo espec√≠fico, primeiro cancelar a edi√ß√£o
	if a.model.EditingField {
		a.model.EditingField = false
		a.model.EditingValue = ""
		a.model.CursorPosition = 0
		return a, nil
	}

	// Se h√° erro exibido, limpar o erro
	if a.model.Error != "" {
		a.model.Error = ""
		return a, nil
	}

	switch a.model.State {
	case models.StateHelp:
		// Voltar do help para o estado anterior
		a.model.State = a.model.PreviousState
	case models.StateSessionFolderSelection:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
		a.model.CurrentFolder = ""
	case models.StateSessionFolderSave:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
		a.model.CurrentFolder = ""
	case models.StateSessionSelection:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
	case models.StateNamespaceSelection:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
	case models.StateHPASelection:
		a.model.State = models.StateNamespaceSelection
		a.model.SelectedIndex = 0
		a.model.ActivePanel = models.PanelNamespaces
	case models.StateHPAEditing:
		a.model.State = models.StateHPASelection
		a.model.EditingHPA = nil
		a.model.FormFields = make(map[string]string)
		a.model.ActivePanel = models.PanelSelectedHPAs
	case models.StateNodeSelection:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
		a.model.ActivePanel = models.PanelNodePools
	case models.StateNodeEditing:
		a.model.State = models.StateNodeSelection
		a.model.EditingNodePool = nil
		a.model.FormFields = make(map[string]string)
		a.model.ActivePanel = models.PanelSelectedNodePools
	case models.StateMixedSession:
		a.model.State = models.StateClusterSelection
		a.model.CurrentSession = nil
		a.model.SelectedIndex = 0
		a.model.ActivePanel = models.PanelNamespaces
	case models.StateClusterResourceSelection:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
		a.model.ClusterResources = nil
		a.model.SelectedResources = nil
	case models.StateClusterResourceEditing:
		a.model.State = models.StateClusterResourceSelection
		a.model.EditingResource = nil
		a.model.FormFields = make(map[string]string)
	case models.StatePrometheusStackManagement:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
		a.model.PrometheusStackMode = false
		a.model.ClusterResources = nil
		a.model.SelectedResources = nil
	case models.StateCronJobSelection:
		a.model.State = models.StateClusterSelection
		a.model.SelectedIndex = 0
		a.model.CronJobs = nil
		a.model.SelectedCronJobs = nil
		a.model.CronJobScrollOffset = 0
	case models.StateCronJobEditing:
		a.model.State = models.StateCronJobSelection
		a.model.EditingCronJob = nil
		a.model.FormFields = make(map[string]string)
		a.model.SelectedIndex = 0
	}
	return a, nil
}

// updateClusterStatus atualiza o status de um cluster
func (a *App) updateClusterStatus(clusterName string, status models.ConnectionStatus, err error) {
	for i := range a.model.Clusters {
		if a.model.Clusters[i].Name == clusterName {
			a.model.Clusters[i].Status = status
			if err != nil {
				a.model.Clusters[i].Error = err.Error()
			}
			break
		}
	}
}

// renderErrorScreen renderiza a tela de erro
func (a *App) renderErrorScreen() string {
	return fmt.Sprintf("‚ùå Erro: %s\n\nPressione 'ESC' para voltar ou 'F4' para sair.", a.model.Error)
}

// renderSuccessScreen renderiza a tela de sucesso
func (a *App) renderSuccessScreen() string {
	return fmt.Sprintf("%s\n\nPressione qualquer tecla para continuar...", a.model.SuccessMsg)
}

// Fun√ß√µes simplificadas para compatibilidade

// setupClusterAndLoadNamespaces configura o cluster (contexto kubectl + Azure subscription) e carrega namespaces
func (a *App) setupClusterAndLoadNamespaces() tea.Cmd {
	if a.model.SelectedCluster == nil {
		return nil
	}

	return func() tea.Msg {
		clusterName := a.model.SelectedCluster.Name

		// 1. Configurar contexto do kubectl
		if err := a.setKubectlContext(clusterName); err != nil {
			return namespacesLoadedMsg{err: fmt.Errorf("failed to set kubectl context for %s: %w", clusterName, err)}
		}

		// 2. Buscar configura√ß√£o do cluster no clusters-config.json e configurar Azure subscription
		if err := a.setupAzureSubscription(clusterName); err != nil {
			// Azure subscription √© opcional - continuar mesmo se falhar
			a.debugLog("‚ö†Ô∏è Warning: Failed to setup Azure subscription: %v\n", err)
		}

		// 3. Carregar namespaces
		return a.loadNamespacesInternal()
	}
}

func (a *App) loadNamespaces() tea.Cmd {
	return a.setupClusterAndLoadNamespaces()
}

func (a *App) loadNamespacesInternal() tea.Msg {
	// Zerar contadores de aplica√ß√£o para nova sess√£o/cluster
	a.resetHPAApplicationCounters()

	// Obter cliente do cluster selecionado
	clusterName := a.model.SelectedCluster.Name
	client, exists := a.clients[clusterName]
	if !exists {
		// Criar cliente se n√£o existir
		if a.kubeManager == nil {
			return namespacesLoadedMsg{err: fmt.Errorf("kube manager not initialized")}
		}
		
		clientSet, err := a.kubeManager.GetClient(a.model.SelectedCluster.Context)
		if err != nil {
			return namespacesLoadedMsg{err: fmt.Errorf("cluster %s parece estar offline ou inacess√≠vel: %w", clusterName, err)}
		}
		
		newClient := kubernetes.NewClient(clientSet, clusterName)
		a.clients[clusterName] = newClient
		client = newClient
	}
	
	// Carregar namespaces com filtro de sistema baseado na configura√ß√£o
	namespaces, err := client.ListNamespaces(a.ctx, a.model.ShowSystemNamespaces)
	if err != nil {
		return namespacesLoadedMsg{err: err}
	}
	
	// Adicionar cluster name aos namespaces
	for i := range namespaces {
		namespaces[i].Cluster = clusterName
	}
	
	return namespacesLoadedMsg{namespaces: namespaces, err: nil}
}

// setKubectlContext configura o contexto do kubectl para o cluster especificado
func (a *App) setKubectlContext(clusterName string) error {
	a.debugLog("üîÑ Setting kubectl context to: %s\n", clusterName)
	
	cmd := exec.Command("kubectl", "config", "use-context", clusterName)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to set kubectl context '%s': %w - output: %s", clusterName, err, string(output))
	}
	
	a.debugLog("‚úÖ kubectl context set to: %s\n", clusterName)
	return nil
}

// setupAzureSubscription busca o cluster no clusters-config.json e configura a Azure subscription
func (a *App) setupAzureSubscription(clusterName string) error {
	// Buscar configura√ß√£o do cluster no clusters-config.json
	clusterConfig, err := findClusterInConfig(clusterName)
	if err != nil {
		return fmt.Errorf("failed to find cluster in config: %w", err)
	}
	
	a.debugLog("üîÑ Setting Azure subscription to: %s\n", clusterConfig.Subscription)
	
	cmd := exec.Command("az", "account", "set", "--subscription", clusterConfig.Subscription)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return fmt.Errorf("failed to set subscription '%s': %w - output: %s", clusterConfig.Subscription, err, string(output))
	}
	
	a.debugLog("‚úÖ Azure subscription set to: %s\n", clusterConfig.Subscription)
	return nil
}

// loadHPACounts carrega a contagem de HPAs para todos os namespaces em background
func (a *App) loadHPACounts() tea.Cmd {
	if a.model.SelectedCluster == nil || len(a.model.Namespaces) == 0 {
		return nil
	}
	
	var cmds []tea.Cmd
	clusterName := a.model.SelectedCluster.Name
	client, exists := a.clients[clusterName]
	if !exists {
		return nil
	}
	
	// Criar comandos para contar HPAs em cada namespace
	for _, ns := range a.model.Namespaces {
		cmds = append(cmds, a.countHPAsInNamespace(client, ns.Name))
	}
	
	return tea.Batch(cmds...)
}

// countHPAsInNamespace conta HPAs em um namespace espec√≠fico
func (a *App) countHPAsInNamespace(client *kubernetes.Client, namespace string) tea.Cmd {
	return func() tea.Msg {
		count, err := client.CountHPAs(a.ctx, namespace)
		return hpaCountUpdatedMsg{
			namespace: namespace,
			count:     count,
			err:       err,
		}
	}
}

func (a *App) loadHPAs() tea.Cmd {
	if a.model.SelectedCluster == nil || a.model.CurrentNamespaceIdx >= len(a.model.SelectedNamespaces) {
		return nil
	}
	
	return func() tea.Msg {
		// Obter cliente do cluster selecionado
		clusterName := a.model.SelectedCluster.Name
		client, exists := a.clients[clusterName]
		if !exists {
			return hpasLoadedMsg{err: fmt.Errorf("client not found for cluster %s", clusterName)}
		}
		
		// Obter namespace atual
		currentNamespace := a.model.SelectedNamespaces[a.model.CurrentNamespaceIdx]
		
		// Carregar HPAs do namespace
		hpas, err := client.ListHPAs(a.ctx, currentNamespace.Name)
		if err != nil {
			return hpasLoadedMsg{err: err}
		}
		
		// Adicionar informa√ß√µes do cluster aos HPAs
		for i := range hpas {
			hpas[i].Cluster = clusterName
		}
		
		return hpasLoadedMsg{hpas: hpas, err: nil}
	}
}

func (a *App) applyHPAChanges(hpas []models.HPA) tea.Cmd {
	return func() tea.Msg {
		if len(hpas) == 0 {
			return hpaChangesAppliedMsg{count: 0, appliedHPAs: nil, err: nil}
		}

		successCount := 0
		var appliedHPAs []models.HPA
		var lastError error

		// Aplicar mudan√ßas em cada HPA
		for _, hpa := range hpas {
			// Obter cliente do cluster
			client, exists := a.clients[hpa.Cluster]
			if !exists {
				lastError = fmt.Errorf("client not found for cluster %s", hpa.Cluster)
				continue
			}

			// Aplicar mudan√ßas no HPA
			err := client.UpdateHPA(a.ctx, hpa)
			if err != nil {
				lastError = err
				continue
			}

			// Aplicar mudan√ßas nos recursos do deployment se modificados
			if hpa.ResourcesModified {
				err = client.ApplyHPADeploymentResourceChanges(a.ctx, &hpa)
				if err != nil {
					lastError = fmt.Errorf("failed to apply deployment resources: %w", err)
					continue
				}
			}

			// Executar rollout se solicitado
			err = client.TriggerRollout(a.ctx, hpa)
			if err != nil {
				lastError = err
				continue
			}

			// HPA aplicado com sucesso - incrementar contador de aplica√ß√µes
			now := time.Now()
			hpa.AppliedCount++
			hpa.LastAppliedAt = &now

			successCount++
			appliedHPAs = append(appliedHPAs, hpa)
		}

		// Se houve falhas, reportar erro
		if successCount < len(hpas) {
			return hpaChangesAppliedMsg{
				count:       successCount,
				appliedHPAs: appliedHPAs,
				err:         fmt.Errorf("aplicadas %d de %d mudan√ßas. √öltimo erro: %v", successCount, len(hpas), lastError),
			}
		}

		// Sucesso total
		return hpaChangesAppliedMsg{
			count:       successCount,
			appliedHPAs: appliedHPAs,
			err:         nil,
		}
	}
}

// applyHPAChangesAsync - Aplica mudan√ßas em HPAs com rollouts ass√≠ncronos
func (a *App) applyHPAChangesAsync(hpas []models.HPA) tea.Cmd {
	return func() tea.Msg {
		if len(hpas) == 0 {
			return hpaChangesAppliedMsg{count: 0, appliedHPAs: nil, err: nil}
		}

		successCount := 0
		var appliedHPAs []models.HPA
		var lastError error

		// Aplicar mudan√ßas em cada HPA
		for _, hpa := range hpas {
			// Obter cliente do cluster
			client, exists := a.clients[hpa.Cluster]
			if !exists {
				lastError = fmt.Errorf("client not found for cluster %s", hpa.Cluster)
				continue
			}

			// Aplicar mudan√ßas no HPA
			err := client.UpdateHPA(a.ctx, hpa)
			if err != nil {
				lastError = err
				continue
			}

			// Aplicar mudan√ßas nos recursos do deployment se modificados
			if hpa.ResourcesModified {
				err = client.ApplyHPADeploymentResourceChanges(a.ctx, &hpa)
				if err != nil {
					lastError = fmt.Errorf("failed to apply deployment resources: %w", err)
					continue
				}
			}

			// HPA aplicado com sucesso - incrementar contador de aplica√ß√µes
			now := time.Now()
			hpa.AppliedCount++
			hpa.LastAppliedAt = &now

			// Iniciar rollouts ass√≠ncronos se solicitados
			if hpa.PerformRollout || hpa.PerformDaemonSetRollout || hpa.PerformStatefulSetRollout {
				a.startAsyncRollouts(hpa, client)
			}

			successCount++
			appliedHPAs = append(appliedHPAs, hpa)
		}

		// Se houve falhas, reportar erro
		if successCount < len(hpas) {
			return hpaChangesAppliedMsg{
				count:       successCount,
				appliedHPAs: appliedHPAs,
				err:         fmt.Errorf("aplicadas %d de %d mudan√ßas. √öltimo erro: %v", successCount, len(hpas), lastError),
			}
		}

		// Sucesso total
		return hpaChangesAppliedMsg{
			count:       successCount,
			appliedHPAs: appliedHPAs,
			err:         nil,
		}
	}
}

// startAsyncRollouts - Inicia rollouts ass√≠ncronos para um HPA
func (a *App) startAsyncRollouts(hpa models.HPA, client *kubernetes.Client) {
	a.debugLog("üöÄ startAsyncRollouts chamada para HPA: %s/%s", hpa.Namespace, hpa.Name)
	a.debugLog("üîß Rollout flags: Deployment=%t, DaemonSet=%t, StatefulSet=%t",
		hpa.PerformRollout, hpa.PerformDaemonSetRollout, hpa.PerformStatefulSetRollout)

	rolloutTypes := []string{}

	if hpa.PerformRollout {
		rolloutTypes = append(rolloutTypes, "deployment")
		a.debugLog("‚úÖ Adicionando rollout: deployment")
	}
	if hpa.PerformDaemonSetRollout {
		rolloutTypes = append(rolloutTypes, "daemonset")
		a.debugLog("‚úÖ Adicionando rollout: daemonset")
	}
	if hpa.PerformStatefulSetRollout {
		rolloutTypes = append(rolloutTypes, "statefulset")
		a.debugLog("‚úÖ Adicionando rollout: statefulset")
	}

	a.debugLog("üìã Total rollout types: %d", len(rolloutTypes))

	statusPanel := a.getStatusPanel()

	for _, rolloutType := range rolloutTypes {
		// Criar ID √∫nico para o progress bar
		progressID := fmt.Sprintf("rollout_%s_%s_%s", hpa.Name, hpa.Namespace, rolloutType)

		// Adicionar progress bar ao StatusPanel
		statusPanel.AddProgressBar(progressID, fmt.Sprintf("%s/%s %s", hpa.Name, hpa.Namespace, rolloutType), 100)
		statusPanel.UpdateProgress(progressID, 0, "running")

		// Log do in√≠cio do rollout
		statusPanel.HPARollout(hpa.Cluster, hpa.Namespace, hpa.Name, rolloutType)

		// Iniciar goroutine para o rollout
		go a.executeRollout(progressID, hpa, rolloutType, client)
	}
}

// executeRollout - Executa um rollout espec√≠fico e atualiza o progresso
func (a *App) executeRollout(progressID string, hpa models.HPA, rolloutType string, client *kubernetes.Client) {
	statusPanel := a.getStatusPanel()

	// Fun√ß√£o helper para atualizar progresso usando StatusPanel
	updateProgress := func(status models.RolloutStatus, progress int, message, errorMsg string) {
		statusText := "running"
		if status == models.RolloutStatusCompleted {
			statusText = "completed"
		} else if status == models.RolloutStatusFailed {
			statusText = "failed"
		}

		statusPanel.UpdateProgress(progressID, progress, statusText)

		// Adicionar log se houver erro
		if errorMsg != "" {
			statusPanel.Error("rollout", fmt.Sprintf("%s: %s", progressID, errorMsg))
		}
	}

	// Atualizar status para running
	updateProgress(models.RolloutStatusRunning, 10, "Executando rollout...", "")

	// Simular progresso durante o rollout
	progressSteps := []struct {
		progress int
		message  string
		delay    time.Duration
	}{
		{25, "Aplicando mudan√ßas...", 1 * time.Second},
		{50, "Aguardando pods...", 2 * time.Second},
		{75, "Verificando status...", 2 * time.Second},
		{90, "Finalizando...", 1 * time.Second},
	}

	// Executar rollout com kubectl
	cmd := exec.Command("kubectl", "rollout", "restart", rolloutType+"/"+hpa.Name,
		"-n", hpa.Namespace, "--context", hpa.Cluster)

	err := cmd.Start()
	if err != nil {
		updateProgress(models.RolloutStatusFailed, 0, "Falha na execu√ß√£o", err.Error())
		return
	}

	// Atualizar progresso em steps
	for _, step := range progressSteps {
		time.Sleep(step.delay)
		updateProgress(models.RolloutStatusRunning, step.progress, step.message, "")
	}

	// Aguardar conclus√£o do comando
	err = cmd.Wait()

	if err != nil {
		updateProgress(models.RolloutStatusFailed, 0, "Rollout falhou", err.Error())
	} else {
		updateProgress(models.RolloutStatusCompleted, 100, "Rollout conclu√≠do", "")
	}
}

// cleanupCompletedRollouts - Limpa progress bars antigos automaticamente pelo StatusPanel
func (a *App) cleanupCompletedRollouts() {
	// Nota: StatusPanel agora gerencia automaticamente a limpeza de progress bars
	// A limpeza acontece automaticamente ap√≥s 2 minutos de conclus√£o
}

// startProgressTracking - Inicia o sistema de atualiza√ß√£o de progresso
func (a *App) startProgressTracking() tea.Cmd {
	return tea.Tick(500*time.Millisecond, func(t time.Time) tea.Msg {
		return progressUpdateMsg{}
	})
}

// clearStatusMessages - Limpa mensagens de status ap√≥s 5 segundos
func (a *App) clearStatusMessages() tea.Cmd {
	return tea.Tick(5*time.Second, func(t time.Time) tea.Msg {
		return clearStatusMsg{}
	})
}


// startAsyncNodePoolOperation - Inicia tracking de uma opera√ß√£o de node pool
func (a *App) startAsyncNodePoolOperation(pool models.NodePool) {
	// Determinar tipo de opera√ß√£o
	operation := "update"
	if pool.NodeCount != pool.OriginalValues.NodeCount {
		operation = "scale"
	}
	if pool.AutoscalingEnabled != pool.OriginalValues.AutoscalingEnabled {
		if pool.AutoscalingEnabled {
			operation = "autoscale"
		} else {
			operation = "manual"
		}
	}

	// Criar entrada de progresso
// DISABLED: 	progress := models.NodePoolProgress{
		ID:          fmt.Sprintf("%s-%s-%d", pool.ClusterName, pool.Name, time.Now().Unix()),
		PoolName:    pool.Name,
		ClusterName: pool.ClusterName,
		Operation:   operation,
		Status:      models.RolloutStatusRunning,
		Progress:    5,
		Message:     "Preparando opera√ß√£o...",
		StartTime:   time.Now(),

		// Detalhes da opera√ß√£o
		FromNodeCount: pool.OriginalValues.NodeCount,
		ToNodeCount:   pool.NodeCount,
		FromMinNodes:  pool.OriginalValues.MinNodeCount,
		ToMinNodes:    pool.MinNodeCount,
		FromMaxNodes:  pool.OriginalValues.MaxNodeCount,
		ToMaxNodes:    pool.MaxNodeCount,
	}

	// Adicionar ao modelo de forma thread-safe
	a.rolloutMutex.Lock()
	// Primeiro, remover qualquer progresso anterior para este node pool
// DISABLED: 	var filteredProgress []models.NodePoolProgress
// DISABLED: 	for _, p := range a.model.NodePoolProgress {
		if p.PoolName != pool.Name || p.ClusterName != pool.ClusterName {
			filteredProgress = append(filteredProgress, p)
		}
	}
	filteredProgress = append(filteredProgress, progress)
// DISABLED: 	a.model.NodePoolProgress = filteredProgress
	a.rolloutMutex.Unlock()
}

// DISABLED: // updateNodePoolProgress - Atualiza o progresso de uma opera√ß√£o de node pool
// DISABLED: func (a *App) updateNodePoolProgress(poolName string, status models.RolloutStatus, progress int, message, errorMsg string) {
	a.rolloutMutex.Lock()
	defer a.rolloutMutex.Unlock()

// DISABLED: 	for i, p := range a.model.NodePoolProgress {
		if p.PoolName == poolName {
// DISABLED: 			a.model.NodePoolProgress[i].Status = status
// DISABLED: 			a.model.NodePoolProgress[i].Progress = progress
// DISABLED: 			a.model.NodePoolProgress[i].Message = message
			if errorMsg != "" {
// DISABLED: 				a.model.NodePoolProgress[i].Error = errorMsg
			}
			if status == models.RolloutStatusCompleted || status == models.RolloutStatusFailed {
				endTime := time.Now()
// DISABLED: 				a.model.NodePoolProgress[i].EndTime = &endTime
			}
			break
		}
	}
}

func (a *App) applyNodePoolChanges(nodePools []models.NodePool) tea.Cmd {
	return func() tea.Msg {
		if len(nodePools) == 0 {
			return nodePoolsAppliedMsg{appliedPools: nil, err: nil}
		}

		// Iniciar progress tracking para cada node pool
		for _, pool := range nodePools {
			a.startAsyncNodePoolOperation(pool)
		}

		successCount := 0
		var appliedPools []models.NodePool
		var lastError error

		// Aplicar mudan√ßas em cada node pool
		for _, pool := range nodePools {
			// Executar comando Azure CLI para update do node pool
			err := a.updateNodePoolViaAzureCLI(pool)
			if err != nil {
				// Atualizar progress para falha
// DISABLED: 				a.updateNodePoolProgress(pool.Name, models.RolloutStatusFailed, 100, "Falha na aplica√ß√£o", err.Error())
				lastError = err
				continue
			}

			// Node pool aplicado com sucesso
// DISABLED: 			a.updateNodePoolProgress(pool.Name, models.RolloutStatusCompleted, 100, "Opera√ß√£o conclu√≠da", "")

			// Incrementar contador de aplica√ß√µes
			pool.AppliedCount++

			successCount++
			appliedPools = append(appliedPools, pool)
		}

		// Se houve falhas, reportar erro
		if successCount < len(nodePools) {
			return nodePoolsAppliedMsg{
				appliedPools: appliedPools,
				err:          fmt.Errorf("aplicadas %d de %d mudan√ßas. √öltimo erro: %v", successCount, len(nodePools), lastError),
			}
		}

		// Sucesso total
		return nodePoolsAppliedMsg{
			appliedPools: appliedPools,
			err:          nil,
		}
	}
}

func (a *App) saveSession(sessionParam *models.Session) tea.Cmd {
	return func() tea.Msg {
		a.debugLog("üîß saveSession called for '%s', CurrentFolder='%s'", sessionParam.Name, a.model.CurrentFolder)
		if a.sessionManager == nil {
			a.debugLog("‚ùå Session manager not initialized")
			return sessionSavedMsg{
				sessionName: sessionParam.Name,
				err:         fmt.Errorf("session manager not initialized"),
			}
		}

		// Criar sess√£o completa
		fullSession := &models.Session{
			Name:      sessionParam.Name,
			CreatedAt: time.Now(),
			CreatedBy: "k8s-hpa-manager",
			Changes:         make([]models.HPAChange, 0),
			NodePoolChanges: make([]models.NodePoolChange, 0),
		}

		// Identificar clusters afetados
		clustersMap := make(map[string]bool)

		// Adicionar mudan√ßas dos HPAs selecionados
		for _, hpa := range a.model.SelectedHPAs {
			if hpa.Modified {
				clustersMap[hpa.Cluster] = true
				change := models.HPAChange{
					Cluster:   hpa.Cluster,
					Namespace: hpa.Namespace,
					HPAName:   hpa.Name,
					OriginalValues: hpa.OriginalValues,
					NewValues: &models.HPAValues{
						MinReplicas:     hpa.MinReplicas,
						MaxReplicas:     hpa.MaxReplicas,
						TargetCPU:       hpa.TargetCPU,
						TargetMemory:    hpa.TargetMemory,

						// Rollout Options
						PerformRollout:            hpa.PerformRollout,
						PerformDaemonSetRollout:   hpa.PerformDaemonSetRollout,
						PerformStatefulSetRollout: hpa.PerformStatefulSetRollout,

						// Recursos do deployment
						DeploymentName:  hpa.DeploymentName,
						CPURequest:      hpa.TargetCPURequest,
						CPULimit:        hpa.TargetCPULimit,
						MemoryRequest:   hpa.TargetMemoryRequest,
						MemoryLimit:     hpa.TargetMemoryLimit,
					},
					Applied:          false,
					RolloutTriggered: hpa.PerformRollout,
					DaemonSetRolloutTriggered:  hpa.PerformDaemonSetRollout,
					StatefulSetRolloutTriggered: hpa.PerformStatefulSetRollout,
				}
				fullSession.Changes = append(fullSession.Changes, change)
			}
		}

		// Adicionar mudan√ßas dos node pools selecionados
		for _, pool := range a.model.SelectedNodePools {
			if pool.Modified {
				// Identificar cluster do pool (pode estar em pool.ResourceGroup ou SelectedCluster)
				clusterName := ""
				if a.model.SelectedCluster != nil {
					clusterName = a.model.SelectedCluster.Name
				}

				if clusterName != "" {
					clustersMap[clusterName] = true

					// Buscar configura√ß√£o do cluster para obter subscription e resource group
					clusterConfig, err := a.findClusterConfig(clusterName)
					subscription := ""
					resourceGroup := ""
					if err == nil {
						subscription = clusterConfig.Subscription
						resourceGroup = clusterConfig.ResourceGroup
					}

					change := models.NodePoolChange{
						Cluster:       clusterName,
						ResourceGroup: resourceGroup,
						Subscription:  subscription,
						NodePoolName:  pool.Name,
						OriginalValues: pool.OriginalValues, // Usar valores originais salvos no modelo
						NewValues: models.NodePoolValues{
							NodeCount:    pool.NodeCount,
							MinNodeCount: pool.MinNodeCount,
							MaxNodeCount: pool.MaxNodeCount,
							AutoscalingEnabled: pool.AutoscalingEnabled,
						},
						Applied: false,

						// Salvar dados de execu√ß√£o sequencial
						SequenceOrder:  pool.SequenceOrder,
						SequenceStatus: pool.SequenceStatus,
					}
					fullSession.NodePoolChanges = append(fullSession.NodePoolChanges, change)
				}
			}
		}

		// Criar metadados da sess√£o
		clustersAffected := make([]string, 0, len(clustersMap))
		for cluster := range clustersMap {
			clustersAffected = append(clustersAffected, cluster)
		}

		fullSession.Metadata = &models.SessionMetadata{
			ClustersAffected: clustersAffected,
			NamespacesCount:  len(a.model.SelectedNamespaces),
			HPACount:         len(fullSession.Changes),
			NodePoolCount:    len(fullSession.NodePoolChanges),
			ResourceCount:    0, // Para futuro uso
			TotalChanges:     len(fullSession.Changes) + len(fullSession.NodePoolChanges),
		}

		// Salvar sess√£o usando o session manager
		var err error
		a.debugLog("üíæ About to save session. CurrentFolder='%s', HPA count=%d, NodePool count=%d",
			a.model.CurrentFolder, len(fullSession.Changes), len(fullSession.NodePoolChanges))
		if a.model.CurrentFolder != "" {
			// Converter string para SessionFolder
			var folder session.SessionFolder
			switch a.model.CurrentFolder {
			case "HPA-Upscale":
				folder = session.FolderHPAUpscale
				a.debugLog("üìÅ Using folder: HPA-Upscale")
			case "HPA-Downscale":
				folder = session.FolderHPADownscale
				a.debugLog("üìÅ Using folder: HPA-Downscale")
			case "Node-Upscale":
				folder = session.FolderNodeUpscale
				a.debugLog("üìÅ Using folder: Node-Upscale")
			case "Node-Downscale":
				folder = session.FolderNodeDownscale
				a.debugLog("üìÅ Using folder: Node-Downscale")
			default:
				a.debugLog("‚ùå Invalid folder name: %s", a.model.CurrentFolder)
				return sessionSavedMsg{
					sessionName: fullSession.Name,
					err:         fmt.Errorf("invalid folder name: %s", a.model.CurrentFolder),
				}
			}
			a.debugLog("üíæ Calling SaveSessionToFolder...")
			err = a.sessionManager.SaveSessionToFolder(fullSession, folder)
		} else {
			a.debugLog("üíæ Calling SaveSession (root folder)...")
			err = a.sessionManager.SaveSession(fullSession)
		}

		if err != nil {
			a.debugLog("‚ùå Save error: %v", err)
		} else {
			a.debugLog("‚úÖ Session saved successfully")
		}

		return sessionSavedMsg{
			sessionName: fullSession.Name,
			err:         err,
		}
	}
}

// deleteSession remove uma sess√£o salva
func (a *App) deleteSession(sessionName string) tea.Cmd {
	return func() tea.Msg {
		if a.sessionManager == nil {
			return sessionDeletedMsg{
				sessionName: sessionName,
				err:         fmt.Errorf("session manager not initialized"),
			}
		}

		// Deletar sess√£o usando o session manager
		err := a.sessionManager.DeleteSession(sessionName)
		if err != nil {
			return sessionDeletedMsg{
				sessionName: sessionName,
				err:         err,
			}
		}

		return sessionDeletedMsg{
			sessionName: sessionName,
			err:         nil,
		}
	}
}

// deleteSessionFromFolder remove uma sess√£o de uma pasta espec√≠fica
func (a *App) deleteSessionFromFolder(sessionName, folderName string) tea.Cmd {
	return func() tea.Msg {
		if a.sessionManager == nil {
			return sessionDeletedMsg{
				sessionName: sessionName,
				err:         fmt.Errorf("session manager not initialized"),
			}
		}

		// Converter string para SessionFolder
		var folder session.SessionFolder
		if folderName != "" {
			switch folderName {
			case "HPA-Upscale":
				folder = session.FolderHPAUpscale
			case "HPA-Downscale":
				folder = session.FolderHPADownscale
			case "Node-Upscale":
				folder = session.FolderNodeUpscale
			case "Node-Downscale":
				folder = session.FolderNodeDownscale
			default:
				return sessionDeletedMsg{
					sessionName: sessionName,
					err:         fmt.Errorf("invalid folder name: %s", folderName),
				}
			}
		}

		// Deletar sess√£o usando o session manager
		var err error
		if folderName != "" {
			err = a.sessionManager.DeleteSessionFromFolder(sessionName, folder)
		} else {
			err = a.sessionManager.DeleteSession(sessionName)
		}

		return sessionDeletedMsg{
			sessionName: sessionName,
			err:         err,
		}
	}
}

// renameSessionInFolder renomeia uma sess√£o em uma pasta espec√≠fica
func (a *App) renameSessionInFolder(oldName, newName, folderName string) tea.Cmd {
	return func() tea.Msg {
		if a.sessionManager == nil {
			return sessionRenamedMsg{
				oldName: oldName,
				newName: newName,
				err:     fmt.Errorf("session manager not initialized"),
			}
		}

		// Converter string para SessionFolder
		var folder session.SessionFolder
		if folderName != "" {
			switch folderName {
			case "HPA-Upscale":
				folder = session.FolderHPAUpscale
			case "HPA-Downscale":
				folder = session.FolderHPADownscale
			case "Node-Upscale":
				folder = session.FolderNodeUpscale
			case "Node-Downscale":
				folder = session.FolderNodeDownscale
			default:
				return sessionRenamedMsg{
					oldName: oldName,
					newName: newName,
					err:     fmt.Errorf("invalid folder name: %s", folderName),
				}
			}
		}

		// Renomear sess√£o usando o session manager
		var err error
		if folderName != "" {
			err = a.sessionManager.RenameSessionInFolder(oldName, newName, folder)
		} else {
			err = a.sessionManager.RenameSession(oldName, newName)
		}

		return sessionRenamedMsg{
			oldName: oldName,
			newName: newName,
			err:     err,
		}
	}
}

// loadSessionFolders carrega todas as pastas de sess√£o
func (a *App) loadSessionFolders() tea.Cmd {
	return func() tea.Msg {
		if a.sessionManager == nil {
			return sessionFoldersLoadedMsg{err: fmt.Errorf("session manager not initialized")}
		}

		folders := a.sessionManager.ListSessionFolders()
		folderNames := make([]string, len(folders))
		for i, folder := range folders {
			folderNames[i] = string(folder)
		}

		return sessionFoldersLoadedMsg{
			folders: folderNames,
			err:     nil,
		}
	}
}

// loadSessionsFromFolder carrega as sess√µes de uma pasta espec√≠fica
func (a *App) loadSessionsFromFolder(folderName string) tea.Cmd {
	return func() tea.Msg {
		if a.sessionManager == nil {
			return sessionsLoadedMsg{err: fmt.Errorf("session manager not initialized")}
		}

		// Converter string para SessionFolder
		var folder session.SessionFolder
		switch folderName {
		case "HPA-Upscale":
			folder = session.FolderHPAUpscale
		case "HPA-Downscale":
			folder = session.FolderHPADownscale
		case "Node-Upscale":
			folder = session.FolderNodeUpscale
		case "Node-Downscale":
			folder = session.FolderNodeDownscale
		default:
			return sessionsLoadedMsg{err: fmt.Errorf("invalid folder name: %s", folderName)}
		}

		sessions, err := a.sessionManager.ListSessionsInFolder(folder)
		if err != nil {
			return sessionsLoadedMsg{err: err}
		}

		return sessionsLoadedMsg{
			sessions: sessions,
			err:      nil,
		}
	}
}

// loadSessions carrega todas as sess√µes salvas (compatibilidade retroativa)
func (a *App) loadSessions() tea.Cmd {
	return func() tea.Msg {
		if a.sessionManager == nil {
			return sessionsLoadedMsg{err: fmt.Errorf("session manager not initialized")}
		}

		sessions, err := a.sessionManager.ListSessions()
		return sessionsLoadedMsg{sessions: sessions, err: err}
	}
}

// applySessionChanges aplica as mudan√ßas de uma sess√£o carregada
func (a *App) applySessionChanges(session *models.Session) tea.Cmd {
	return func() tea.Msg {
		if len(session.Changes) == 0 {
			return hpaChangesAppliedMsg{count: 0, err: fmt.Errorf("session has no changes to apply")}
		}

		successCount := 0
		var lastError error

		// Aplicar mudan√ßas de cada HPA na sess√£o
		for _, change := range session.Changes {
			// Obter cliente do cluster
			client, exists := a.clients[change.Cluster]
			if !exists {
				// Tentar criar cliente se n√£o existir
				if a.kubeManager == nil {
					lastError = fmt.Errorf("kube manager not initialized")
					continue
				}
				
				clientSet, err := a.kubeManager.GetClient(change.Cluster)
				if err != nil {
					lastError = fmt.Errorf("failed to get client for cluster %s: %w", change.Cluster, err)
					continue
				}
				
				newClient := kubernetes.NewClient(clientSet, change.Cluster)
				a.clients[change.Cluster] = newClient
				client = newClient
			}

			// Criar HPA model a partir do change
			hpa := models.HPA{
				Name:         change.HPAName,
				Namespace:    change.Namespace,
				Cluster:      change.Cluster,
				MinReplicas:  change.NewValues.MinReplicas,
				MaxReplicas:  change.NewValues.MaxReplicas,
				TargetCPU:    change.NewValues.TargetCPU,
				TargetMemory: change.NewValues.TargetMemory,
			}

			// Aplicar mudan√ßas no HPA
			err := client.UpdateHPA(a.ctx, hpa)
			if err != nil {
				lastError = err
				continue
			}

			successCount++
		}

		// Se houve falhas, reportar erro
		if successCount < len(session.Changes) {
			return hpaChangesAppliedMsg{
				count: successCount,
				err:   fmt.Errorf("applied %d of %d changes from session '%s'. Last error: %v", successCount, len(session.Changes), session.Name, lastError),
			}
		}

		// Sucesso total
		return hpaChangesAppliedMsg{
			count: successCount,
			err:   nil,
		}
	}
}

// loadSessionState carrega o estado da aplica√ß√£o baseado numa sess√£o salva
func (a *App) loadSessionState(session *models.Session) tea.Cmd {
	return func() tea.Msg {
		// Zerar contadores de aplica√ß√£o para nova sess√£o
		a.resetHPAApplicationCounters()

		// Verificar tipos de mudan√ßas na sess√£o
		hasHPAChanges := len(session.Changes) > 0
		hasNodePoolChanges := len(session.NodePoolChanges) > 0
		hasResourceChanges := len(session.ResourceChanges) > 0

		a.debugLog("üìä Analisando sess√£o: HPAs=%d, NodePools=%d, Resources=%d\n",
			len(session.Changes), len(session.NodePoolChanges), len(session.ResourceChanges))

		// Verificar se √© sess√£o mista (HPAs + Node Pools)
		if hasHPAChanges && hasNodePoolChanges {
			// Sess√£o mista - carregar ambos HPAs e node pools
			// Por enquanto, vamos carregar os HPAs primeiro e permitir navega√ß√£o entre os pain√©is
			a.debugLog("üîÄ Sess√£o mista detectada - carregando HPAs primeiro\n")
			return a.loadHPASessionState(session)
		} else if hasNodePoolChanges {
			// √â uma sess√£o s√≥ de node pools
			a.debugLog("üîß Sess√£o de node pools detectada\n")
			return a.loadNodePoolSessionState(session)
		} else if hasHPAChanges {
			// √â uma sess√£o s√≥ de HPAs
			a.debugLog("üìä Sess√£o de HPAs detectada\n")
			return a.loadHPASessionState(session)
		} else if hasResourceChanges {
			// √â uma sess√£o de recursos do cluster
			a.debugLog("‚öôÔ∏è Sess√£o de recursos detectada\n")
			return sessionStateLoadedMsg{err: fmt.Errorf("resource sessions not yet supported")}
		} else {
			// Nenhuma mudan√ßa encontrada
			return sessionStateLoadedMsg{err: fmt.Errorf("session '%s' contains no changes to load (empty HPAs, node pools, and resources)", session.Name)}
		}
	}
}

// loadHPASessionState carrega uma sess√£o de HPAs (c√≥digo original)
func (a *App) loadHPASessionState(session *models.Session) tea.Msg {
	// Usar clusters_affected dos metadados da sess√£o (priorit√°rio)
	var targetCluster string
	if session.Metadata != nil && session.Metadata.ClustersAffected != nil && len(session.Metadata.ClustersAffected) > 0 {
		targetCluster = session.Metadata.ClustersAffected[0]
		a.debugLog("üîç Usando cluster dos metadados da sess√£o: %s\n", targetCluster)
	} else {
		// Fallback: identificar clusters √∫nicos nos Changes
		clustersMap := make(map[string]bool)
		for _, change := range session.Changes {
			if change.Cluster != "" {
				clustersMap[change.Cluster] = true
			}
		}

		// Usar o primeiro cluster encontrado
		for cluster := range clustersMap {
			targetCluster = cluster
			break
		}
		if targetCluster != "" {
			a.debugLog("üîç Usando cluster extra√≠do dos changes: %s\n", targetCluster)
		} else {
			a.debugLog("‚ö†Ô∏è Nenhum cluster encontrado nos changes\n")
		}
	}

	if targetCluster == "" {
		return sessionStateLoadedMsg{err: fmt.Errorf("no cluster found in session")}
	}

	// Converter as mudan√ßas da sess√£o em HPAs com modifica√ß√µes
	var sessionHPAs []models.HPA
	namespacesMap := make(map[string]bool)
	
	for _, change := range session.Changes {
		if change.Cluster != targetCluster {
			continue // Por enquanto, s√≥ um cluster
		}
		
		namespacesMap[change.Namespace] = true
		
		hpa := models.HPA{
			Name:            change.HPAName,
			Namespace:       change.Namespace,
			Cluster:         change.Cluster,
			MinReplicas:     change.NewValues.MinReplicas,
			MaxReplicas:     change.NewValues.MaxReplicas,
			TargetCPU:       change.NewValues.TargetCPU,
			TargetMemory:    change.NewValues.TargetMemory,
			PerformRollout:            change.RolloutTriggered,
			PerformDaemonSetRollout:   change.DaemonSetRolloutTriggered,
			PerformStatefulSetRollout: change.StatefulSetRolloutTriggered,
			OriginalValues:            change.OriginalValues,
			Selected:                  true,
			Modified:                  true, // Marcar como modificado

			// Recursos do deployment da sess√£o
			DeploymentName:        change.NewValues.DeploymentName,
			TargetCPURequest:      change.NewValues.CPURequest,
			TargetCPULimit:        change.NewValues.CPULimit,
			TargetMemoryRequest:   change.NewValues.MemoryRequest,
			TargetMemoryLimit:     change.NewValues.MemoryLimit,
			ResourcesModified:     change.NewValues.CPURequest != "" || change.NewValues.CPULimit != "" || change.NewValues.MemoryRequest != "" || change.NewValues.MemoryLimit != "",

			// Valores originais dos recursos (se existirem)
			CurrentCPURequest:     change.OriginalValues.CPURequest,
			CurrentCPULimit:       change.OriginalValues.CPULimit,
			CurrentMemoryRequest:  change.OriginalValues.MemoryRequest,
			CurrentMemoryLimit:    change.OriginalValues.MemoryLimit,
		}

		// Se n√£o h√° dados de recursos na sess√£o, marcar para enriquecer posteriormente
		if hpa.DeploymentName == "" && hpa.CurrentCPURequest == "" && hpa.CurrentCPULimit == "" &&
		   hpa.CurrentMemoryRequest == "" && hpa.CurrentMemoryLimit == "" {
			hpa.NeedsEnrichment = true
		}
		sessionHPAs = append(sessionHPAs, hpa)
	}

	// Criar lista de namespaces da sess√£o
	var sessionNamespaces []models.Namespace
	for ns := range namespacesMap {
		namespace := models.Namespace{
			Name:     ns,
			Cluster:  targetCluster,
			HPACount: 0, // Ser√° contado depois
			Selected: true,
		}
		sessionNamespaces = append(sessionNamespaces, namespace)
	}

	return sessionStateLoadedMsg{
		clusterName: targetCluster,
		namespaces:  sessionNamespaces,
		hpas:        sessionHPAs,
		nodePools:   []models.NodePool{}, // Sess√£o de HPAs n√£o tem node pools
		sessionName: session.Name,
		err:         nil,
	}
}

// loadNodePoolSessionState carrega uma sess√£o de node pools
func (a *App) loadNodePoolSessionState(session *models.Session) tea.Msg {
	// Usar clusters_affected dos metadados da sess√£o (priorit√°rio)
	var targetCluster string

	if session.Metadata != nil && session.Metadata.ClustersAffected != nil && len(session.Metadata.ClustersAffected) > 0 {
		targetCluster = session.Metadata.ClustersAffected[0]
		a.debugLog("üîç Usando cluster dos metadados da sess√£o: %s\n", targetCluster)
	} else {
		// Fallback: identificar clusters √∫nicos nos NodePoolChanges
		clustersMap := make(map[string]bool)
		for _, change := range session.NodePoolChanges {
			if change.Cluster != "" {
				clustersMap[change.Cluster] = true
			}
		}
		// Usar o primeiro cluster encontrado
		for cluster := range clustersMap {
			targetCluster = cluster
			break
		}
		if targetCluster != "" {
			a.debugLog("üîç Usando cluster extra√≠do dos changes: %s\n", targetCluster)
		} else {
			a.debugLog("‚ö†Ô∏è Nenhum cluster encontrado nos changes\n")
		}
	}

	if targetCluster == "" {
		return sessionStateLoadedMsg{err: fmt.Errorf("no cluster found in node pool session")}
	}

	// Buscar configura√ß√£o do cluster no clusters-config.json
	clusterConfig, err := a.findClusterConfig(targetCluster)
	if err != nil {
		return sessionStateLoadedMsg{err: fmt.Errorf("failed to find cluster config for %s: %w", targetCluster, err)}
	}

	// Configurar contexto Azure com a subscription do cluster
	if err := a.setupAzureContext(clusterConfig.Subscription); err != nil {
		return sessionStateLoadedMsg{err: fmt.Errorf("failed to setup Azure context: %w", err)}
	}

	// Carregar node pools atuais do cluster
	a.debugLog("üîÑ Carregando node pools do cluster %s...\n", targetCluster)

	// Normalizar nome do cluster para Azure CLI (remover -admin se existir)
	clusterNameForAzure := targetCluster
	if strings.HasSuffix(clusterNameForAzure, "-admin") {
		clusterNameForAzure = strings.TrimSuffix(clusterNameForAzure, "-admin")
	}

	// Carregar node pools via Azure CLI
	a.debugLog("üìã Carregando node pools: cluster=%s, resourceGroup=%s, subscription=%s\n",
		clusterNameForAzure, clusterConfig.ResourceGroup, clusterConfig.Subscription)

	nodePools, err := loadNodePoolsFromAzure(clusterNameForAzure, clusterConfig.ResourceGroup, clusterConfig.Subscription)
	if err != nil {
		a.debugLog("‚ùå Erro ao carregar node pools: %v\n", err)
		return sessionStateLoadedMsg{err: fmt.Errorf("failed to load node pools: %w", err)}
	}

	a.debugLog("üìä Carregados %d node pools do Azure\n", len(nodePools))

	// Aplicar as modifica√ß√µes da sess√£o aos node pools carregados
	var sessionNodePools []models.NodePool
	poolsInSession := make(map[string]bool)

	// Primeiro, aplicar modifica√ß√µes aos pools existentes no cluster
	for _, pool := range nodePools {
		// Verificar se este pool tem mudan√ßas na sess√£o
		for _, change := range session.NodePoolChanges {
			if change.NodePoolName == pool.Name && change.Cluster == targetCluster {
				// Aplicar mudan√ßas da sess√£o
				pool.NodeCount = change.NewValues.NodeCount
				pool.MinNodeCount = change.NewValues.MinNodeCount
				pool.MaxNodeCount = change.NewValues.MaxNodeCount
				pool.AutoscalingEnabled = change.NewValues.AutoscalingEnabled
				pool.Modified = true
				pool.Selected = true
				poolsInSession[pool.Name] = true

				// Restaurar dados de execu√ß√£o sequencial
				pool.SequenceOrder = change.SequenceOrder
				pool.SequenceStatus = change.SequenceStatus

				a.debugLog("üìù Pool '%s' atualizado com dados da sess√£o (sequ√™ncia: %d, status: %s)\n",
					pool.Name, pool.SequenceOrder, pool.SequenceStatus)
				break
			}
		}
		sessionNodePools = append(sessionNodePools, pool)
	}

	// Adicionar pools que est√£o na sess√£o mas n√£o existem mais no cluster (para hist√≥rico)
	for _, change := range session.NodePoolChanges {
		if change.Cluster == targetCluster && !poolsInSession[change.NodePoolName] {
			// Pool da sess√£o n√£o existe mais no cluster - criar entrada hist√≥rica
			historicalPool := models.NodePool{
				Name:         change.NodePoolName,
				NodeCount:    change.NewValues.NodeCount,
				MinNodeCount: change.NewValues.MinNodeCount,
				MaxNodeCount: change.NewValues.MaxNodeCount,
				AutoscalingEnabled: change.NewValues.AutoscalingEnabled,
				Modified:     true,
				Selected:     true,
				Status:       "Historical", // Marcar como hist√≥rico
			}
			sessionNodePools = append(sessionNodePools, historicalPool)
			a.debugLog("‚ö†Ô∏è Pool '%s' da sess√£o n√£o existe mais no cluster - adicionado como hist√≥rico\n", change.NodePoolName)
		}
	}

	a.debugLog("‚úÖ Carregados %d node pools com modifica√ß√µes aplicadas\n", len(sessionNodePools))

	// Contar quantos pools est√£o marcados como modificados
	modifiedCount := 0
	for _, pool := range sessionNodePools {
		if pool.Modified {
			modifiedCount++
		}
	}
	a.debugLog("üìù %d node pools marcados como modificados\n", modifiedCount)

	// Definir estado da aplica√ß√£o para node pools
	return sessionStateLoadedMsg{
		clusterName: targetCluster,
		namespaces:  []models.Namespace{}, // Node pools n√£o usam namespaces
		hpas:        []models.HPA{},       // N√£o h√° HPAs numa sess√£o de node pools
		nodePools:   sessionNodePools,     // Node pools carregados com modifica√ß√µes aplicadas
		sessionName: session.Name,
		err:         nil,
	}
}

// getCurrentFieldValue retorna o valor atual do campo sendo editado
func (a *App) getCurrentFieldValue(fieldName string) string {
	if a.model.EditingHPA == nil {
		return ""
	}
	
	hpa := a.model.EditingHPA
	switch fieldName {
	case "min_replicas":
		if hpa.MinReplicas != nil {
			return fmt.Sprintf("%d", *hpa.MinReplicas)
		}
		return "1"
	case "max_replicas":
		return fmt.Sprintf("%d", hpa.MaxReplicas)
	case "target_cpu":
		if hpa.TargetCPU != nil {
			return fmt.Sprintf("%d", *hpa.TargetCPU)
		}
		return "80"
	case "target_memory":
		if hpa.TargetMemory != nil {
			return fmt.Sprintf("%d", *hpa.TargetMemory)
		}
		return ""
	case "rollout":
		if hpa.PerformRollout {
			return "true"
		}
		return "false"
	
	// Campos de recursos do deployment
	case "deployment_cpu_request":
		if hpa.TargetCPURequest != "" {
			return hpa.TargetCPURequest
		}
		return hpa.CurrentCPURequest
	case "deployment_cpu_limit":
		if hpa.TargetCPULimit != "" {
			return hpa.TargetCPULimit
		}
		return hpa.CurrentCPULimit
	case "deployment_memory_request":
		if hpa.TargetMemoryRequest != "" {
			return hpa.TargetMemoryRequest
		}
		return hpa.CurrentMemoryRequest
	case "deployment_memory_limit":
		if hpa.TargetMemoryLimit != "" {
			return hpa.TargetMemoryLimit
		}
		return hpa.CurrentMemoryLimit
	
	default:
		return ""
	}
}

// applyFieldValue aplica o valor editado ao campo do HPA
func (a *App) applyFieldValue(fieldName, value string) error {
	if a.model.EditingHPA == nil {
		return fmt.Errorf("no HPA being edited")
	}
	
	hpa := a.model.EditingHPA
	switch fieldName {
	case "min_replicas":
		if value == "" {
			hpa.MinReplicas = nil
		} else {
			val, err := strconv.ParseInt(value, 10, 32)
			if err != nil {
				return err
			}
			minVal := int32(val)
			hpa.MinReplicas = &minVal
		}
	case "max_replicas":
		val, err := strconv.ParseInt(value, 10, 32)
		if err != nil {
			return err
		}
		hpa.MaxReplicas = int32(val)
	case "target_cpu":
		if value == "" {
			hpa.TargetCPU = nil
		} else {
			val, err := strconv.ParseInt(value, 10, 32)
			if err != nil {
				return err
			}
			cpuVal := int32(val)
			hpa.TargetCPU = &cpuVal
		}
	case "target_memory":
		if value == "" {
			hpa.TargetMemory = nil
		} else {
			val, err := strconv.ParseInt(value, 10, 32)
			if err != nil {
				return err
			}
			memVal := int32(val)
			hpa.TargetMemory = &memVal
		}
	case "rollout":
		lowerValue := strings.ToLower(value)
		hpa.PerformRollout = (lowerValue == "true" || lowerValue == "t" || lowerValue == "yes" || lowerValue == "y" || lowerValue == "1")
	
	// Campos de recursos do deployment
	case "deployment_cpu_request":
		hpa.TargetCPURequest = value
		hpa.ResourcesModified = true
	case "deployment_cpu_limit":
		hpa.TargetCPULimit = value
		hpa.ResourcesModified = true
	case "deployment_memory_request":
		hpa.TargetMemoryRequest = value
		hpa.ResourcesModified = true
	case "deployment_memory_limit":
		hpa.TargetMemoryLimit = value
		hpa.ResourcesModified = true
	}
	
	return nil
}

// handleHelpKeys - Navega√ß√£o na tela de ajuda
func (a *App) handleHelpKeys(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "up", "k":
		if a.model.HelpScrollOffset > 0 {
			a.model.HelpScrollOffset--
		}
	case "down", "j":
		// Permitir scroll para baixo (limite ser√° controlado na renderiza√ß√£o)
		a.model.HelpScrollOffset++
	case "pageup":
		a.model.HelpScrollOffset -= 10
		if a.model.HelpScrollOffset < 0 {
			a.model.HelpScrollOffset = 0
		}
	case "pagedown":
		a.model.HelpScrollOffset += 10
	case "home":
		a.model.HelpScrollOffset = 0
	case "end":
		a.model.HelpScrollOffset = 50 // Valor grande para ir ao final
	default:
		// Outras teclas voltam para o estado anterior
		a.model.State = a.model.PreviousState
		a.model.HelpScrollOffset = 0 // Reset scroll
	}
	return a, nil
}

// updateNodePoolViaAzureCLI atualiza um node pool via Azure CLI
func (a *App) updateNodePoolViaAzureCLI(pool models.NodePool) error {
	// Primeiro, verificar se h√° mudan√ßas para aplicar
	if !pool.Modified {
		return nil
	}

	// Etapa 1: Valida√ß√£o inicial (5% -> 15%)
// DISABLED: 	a.updateNodePoolProgress(pool.Name, models.RolloutStatusRunning, 15, "Validando configura√ß√µes...", "")

	// Normalizar nome do cluster para Azure CLI (remover -admin se existir)
	clusterNameForAzure := pool.ClusterName
	if strings.HasSuffix(clusterNameForAzure, "-admin") {
		clusterNameForAzure = strings.TrimSuffix(clusterNameForAzure, "-admin")
	}

	// Etapa 2: Preparando comandos (15% -> 25%)
// DISABLED: 	a.updateNodePoolProgress(pool.Name, models.RolloutStatusRunning, 25, "Preparando comandos Azure CLI...", "")

	// Construir comandos Azure CLI baseados nas mudan√ßas
	var cmds [][]string

	// Update node count se mudou
	if pool.NodeCount != pool.OriginalValues.NodeCount {
		cmd := []string{
			"az", "aks", "nodepool", "scale",
			"--resource-group", pool.ResourceGroup,
			"--cluster-name", clusterNameForAzure,
			"--name", pool.Name,
			"--node-count", fmt.Sprintf("%d", pool.NodeCount),
		}
		// Adicionar subscription se dispon√≠vel (com aspas se houver espa√ßos)
		if pool.Subscription != "" {
			cmd = append(cmd, "--subscription", pool.Subscription)
		}
		cmds = append(cmds, cmd)
	}

	// Update autoscaling enabled/disabled se mudou
	if pool.AutoscalingEnabled != pool.OriginalValues.AutoscalingEnabled {
		if pool.AutoscalingEnabled {
			// Habilitar autoscaling
			cmd := []string{
				"az", "aks", "nodepool", "update",
				"--enable-cluster-autoscaler",
				"--min-count", fmt.Sprintf("%d", pool.MinNodeCount),
				"--max-count", fmt.Sprintf("%d", pool.MaxNodeCount),
				"--resource-group", pool.ResourceGroup,
				"--cluster-name", clusterNameForAzure,
				"--name", pool.Name,
			}
			// Adicionar subscription se dispon√≠vel
			if pool.Subscription != "" {
				cmd = append(cmd, "--subscription", pool.Subscription)
			}
			cmds = append(cmds, cmd)
		} else {
			// Desabilitar autoscaling
			cmd := []string{
				"az", "aks", "nodepool", "update",
				"--disable-cluster-autoscaler",
				"--resource-group", pool.ResourceGroup,
				"--cluster-name", clusterNameForAzure,
				"--name", pool.Name,
			}
			// Adicionar subscription se dispon√≠vel
			if pool.Subscription != "" {
				cmd = append(cmd, "--subscription", pool.Subscription)
			}
			cmds = append(cmds, cmd)
		}
	} else if pool.AutoscalingEnabled && (pool.MinNodeCount != pool.OriginalValues.MinNodeCount || pool.MaxNodeCount != pool.OriginalValues.MaxNodeCount) {
		// Update min/max node count se autoscaling est√° habilitado e mudou
		cmd := []string{
			"az", "aks", "nodepool", "update",
			"--update-cluster-autoscaler",
			"--min-count", fmt.Sprintf("%d", pool.MinNodeCount),
			"--max-count", fmt.Sprintf("%d", pool.MaxNodeCount),
			"--resource-group", pool.ResourceGroup,
			"--cluster-name", clusterNameForAzure,
			"--name", pool.Name,
		}
		// Adicionar subscription se dispon√≠vel
		if pool.Subscription != "" {
			cmd = append(cmd, "--subscription", pool.Subscription)
		}
		cmds = append(cmds, cmd)
	}


	// Se n√£o h√° comandos para executar, n√£o h√° mudan√ßas
	if len(cmds) == 0 {
		return nil
	}

	// Executar todos os comandos com progress tracking mais granular (30% -> 90%)
	totalCmds := len(cmds)
	for i, cmd := range cmds {
		// Progresso inicial para o comando
		startProgress := 30 + (i * 60 / totalCmds)
// DISABLED: 		a.updateNodePoolProgress(pool.Name, models.RolloutStatusRunning, startProgress, fmt.Sprintf("Iniciando comando %d/%d...", i+1, totalCmds), "")

		// Progresso durante execu√ß√£o
		midProgress := 30 + ((i * 60 + 20) / totalCmds)
// DISABLED: 		a.updateNodePoolProgress(pool.Name, models.RolloutStatusRunning, midProgress, fmt.Sprintf("Executando comando %d/%d...", i+1, totalCmds), "")

		err := a.executeAzureCommand(cmd)
		if err != nil {
// DISABLED: 			a.updateNodePoolProgress(pool.Name, models.RolloutStatusFailed, 100, "Falha na execu√ß√£o", err.Error())
			return fmt.Errorf("failed to update node pool %s: %w", pool.Name, err)
		}

		// Progresso final do comando
		endProgress := 30 + ((i + 1) * 60 / totalCmds)
// DISABLED: 		a.updateNodePoolProgress(pool.Name, models.RolloutStatusRunning, endProgress, fmt.Sprintf("Comando %d/%d conclu√≠do", i+1, totalCmds), "")
	}

	// Progresso final antes de completar
// DISABLED: 	a.updateNodePoolProgress(pool.Name, models.RolloutStatusRunning, 95, "Finalizando opera√ß√£o...", "")
	return nil
}

// executeAzureCommand executa um comando Azure CLI
func (a *App) executeAzureCommand(cmdArgs []string) error {
	cmd := exec.Command(cmdArgs[0], cmdArgs[1:]...)
	
	// Log comando completo para debug (apenas em debug mode)
	if a.debug {
		fmt.Printf("üöÄ Running command: %s %s\n", cmdArgs[0], strings.Join(cmdArgs[1:], " "))
	}
	
	output, err := cmd.CombinedOutput()
	if err != nil {
		fmt.Printf("‚ùå Command failed with error: %s\n", err.Error())
		// Mostrar apenas primeiras linhas do erro para n√£o poluir
		errorOutput := string(output)
		lines := strings.Split(errorOutput, "\n")
		if len(lines) > 3 {
			errorOutput = strings.Join(lines[:3], "\n") + "\n... (output truncated)"
		}
		fmt.Printf("üìÑ Error details: %s\n", errorOutput)
		return fmt.Errorf("Azure CLI command failed: %s", err.Error())
	}
	
	// Filtrar output JSON para mostrar apenas informa√ß√µes relevantes
	a.processAzureOutput(string(output))
	return nil
}

// processAzureOutput processa e filtra o output do Azure CLI
func (a *App) processAzureOutput(output string) {
	// Se o output parece ser JSON, tentar extrair informa√ß√µes relevantes
	if strings.TrimSpace(output) != "" && (strings.HasPrefix(strings.TrimSpace(output), "{") || strings.HasPrefix(strings.TrimSpace(output), "[")) {
		// Tentar parsear como JSON para extrair informa√ß√µes √∫teis
		var jsonData map[string]interface{}
		if err := json.Unmarshal([]byte(output), &jsonData); err == nil {
			// Extrair apenas campos relevantes
			if name, ok := jsonData["name"].(string); ok {
				fmt.Printf("   üìã Nome: %s\n", name)
			}
			if count, ok := jsonData["count"].(float64); ok {
				fmt.Printf("   üî¢ Node Count: %.0f\n", count)
			}
			if minCount, ok := jsonData["minCount"].(float64); ok {
				fmt.Printf("   üìâ Min Count: %.0f\n", minCount)
			}
			if maxCount, ok := jsonData["maxCount"].(float64); ok {
				fmt.Printf("   üìà Max Count: %.0f\n", maxCount)
			}
			if status, ok := jsonData["provisioningState"].(string); ok && status != "" {
				fmt.Printf("   üè∑Ô∏è  Status: %s\n", status)
			}
		} else {
			// Se n√£o conseguir parsear JSON, mostrar apenas se n√£o for muito grande
			if len(output) < 200 {
				fmt.Printf("   üìÑ Output: %s\n", strings.TrimSpace(output))
			} else {
				fmt.Printf("   üìÑ Output: ‚úÖ Command executed successfully (output truncated)\n")
			}
		}
	} else if strings.TrimSpace(output) != "" {
		// Para output n√£o-JSON, mostrar apenas se for pequeno
		if len(output) < 200 {
			fmt.Printf("   üìÑ Output: %s\n", strings.TrimSpace(output))
		} else {
			fmt.Printf("   üìÑ Output: ‚úÖ Command executed successfully\n")
		}
	}
}



// renderMixedSession renderiza a interface de sess√£o mista (HPAs + Node Pools)
func (a *App) renderMixedSession() string {
	return "üîÑ Sess√£o Mista (HPAs + Node Pools) - Em Implementa√ß√£o\n\n" +
		"TAB - Alternar entre HPAs e Node Pools\n" +
		"SPACE - Selecionar/Desselecionar\n" +
		"ENTER - Editar\n" +
		"Ctrl+S - Salvar Sess√£o\n" +
		"Ctrl+D/U - Aplicar Mudan√ßas\n" +
		"ESC - Voltar"
}

// applyMixedSession aplica todas as mudan√ßas de uma sess√£o mista
func (a *App) applyMixedSession() tea.Cmd {
	return func() tea.Msg {
		if a.model.CurrentSession == nil {
			return mixedSessionAppliedMsg{err: fmt.Errorf("no session to apply")}
		}

		var errors []string
		successCount := 0

		// Aplicar mudan√ßas de HPAs
		if len(a.model.CurrentSession.Changes) > 0 {
			fmt.Printf("üîÑ Aplicando mudan√ßas em %d HPA(s)...\n", len(a.model.CurrentSession.Changes))
			// Aqui deveria chamar a fun√ß√£o de aplicar HPAs
			// Por simplicidade, simulando sucesso
			successCount += len(a.model.CurrentSession.Changes)
		}

		// Aplicar mudan√ßas de Node Pools
		if len(a.model.CurrentSession.NodePoolChanges) > 0 {
			fmt.Printf("üîÑ Aplicando mudan√ßas em %d Node Pool(s)...\n", len(a.model.CurrentSession.NodePoolChanges))
			// Aqui deveria chamar a fun√ß√£o de aplicar Node Pools
			// Por simplicidade, simulando sucesso
			successCount += len(a.model.CurrentSession.NodePoolChanges)
		}

		if len(errors) > 0 {
			return mixedSessionAppliedMsg{
				err: fmt.Errorf("alguns erros ocorreram: %v", errors),
			}
		}

		return mixedSessionAppliedMsg{
			successCount: successCount,
			err:          nil,
		}
	}
}

// mixedSessionAppliedMsg representa o resultado da aplica√ß√£o de uma sess√£o mista
type mixedSessionAppliedMsg struct {
	successCount int
	err          error
}

// handleF7AllResources inicia o gerenciamento de todos os recursos do cluster
func (a *App) handleF7AllResources() (tea.Model, tea.Cmd) {
	// Verificar se h√° cluster selecionado
	if a.model.SelectedCluster == nil {
		a.model.Error = "Selecione um cluster primeiro para gerenciar recursos"
		return a, nil
	}
	
	// Verificar se est√° em estado v√°lido para F7
	validStates := map[models.AppState]bool{
		models.StateNamespaceSelection: true,
		models.StateHPASelection:      true,
		models.StateNodeSelection:     true,
		models.StateMixedSession:      true,
	}
	
	if !validStates[a.model.State] {
		a.model.Error = "F7 (Recursos) dispon√≠vel apenas ap√≥s sele√ß√£o de cluster"
		return a, nil
	}
	
	// Configurar modo de recursos
	a.model.PrometheusStackMode = false
	a.model.ShowSystemResources = false
	a.model.ResourceFilter = models.ResourceMonitoring // Filtro padr√£o
	
	// Ir para estado de descoberta de recursos
	a.model.State = models.StateClusterResourceDiscovery
	a.model.Loading = true
	
	return a, a.discoverClusterResources(false) // false = todos os recursos
}

// handleF8PrometheusResources inicia o gerenciamento espec√≠fico do Prometheus
func (a *App) handleF8PrometheusResources() (tea.Model, tea.Cmd) {
	// Verificar se h√° cluster selecionado
	if a.model.SelectedCluster == nil {
		a.model.Error = "Selecione um cluster primeiro para gerenciar Prometheus"
		return a, nil
	}
	
	// Verificar se est√° em estado v√°lido para F8
	validStates := map[models.AppState]bool{
		models.StateNamespaceSelection: true,
		models.StateHPASelection:      true,
		models.StateNodeSelection:     true,
		models.StateMixedSession:      true,
	}
	
	if !validStates[a.model.State] {
		a.model.Error = "F8 (Prometheus) dispon√≠vel apenas ap√≥s sele√ß√£o de cluster"
		return a, nil
	}
	
	// Configurar modo Prometheus
	a.model.PrometheusStackMode = true
	a.model.ShowSystemResources = true // Prometheus est√° em namespaces system
	a.model.ResourceFilter = models.ResourceMonitoring
	
	// Ir para estado de descoberta de recursos Prometheus
	a.model.State = models.StateClusterResourceDiscovery
	a.model.Loading = true
	
	return a, a.discoverClusterResources(true) // true = apenas Prometheus
}

// handleMouseEvent - Trata eventos de mouse para scroll
func (a *App) handleMouseEvent(msg tea.MouseMsg) (tea.Model, tea.Cmd) {
	// Verificar se estamos em uma tela que tem painel de status
	validStates := map[models.AppState]bool{
		models.StateHPASelection:  true,
		models.StateNodeSelection: true,
		models.StateMixedSession:  true,
	}

	if !validStates[a.model.State] {
		return a, nil
	}

	switch msg.Type {
	case tea.MouseWheelUp:
		// Scroll up baseado no painel ativo
		if a.model.ActivePanel == models.PanelSelectedHPAs {
			if a.model.HPASelectedScrollOffset > 0 {
				a.model.HPASelectedScrollOffset--
			}
		} else if a.model.ActivePanel == models.PanelSelectedNodePools {
			if a.model.NodePoolSelectedScrollOffset > 0 {
				a.model.NodePoolSelectedScrollOffset--
			}
		} else {
			// Scroll no painel de status por padr√£o
			if a.model.StatusScrollOffset > 0 {
				a.model.StatusScrollOffset--
			}
		}
		return a, nil
	case tea.MouseWheelDown:
		// Scroll down baseado no painel ativo
		if a.model.ActivePanel == models.PanelSelectedHPAs {
			a.model.HPASelectedScrollOffset++
		} else if a.model.ActivePanel == models.PanelSelectedNodePools {
			a.model.NodePoolSelectedScrollOffset++
		} else {
			// Scroll no painel de status por padr√£o
			a.model.StatusScrollOffset++
		}
		return a, nil
	}

	return a, nil
}

// findClusterConfig busca a configura√ß√£o do cluster no clusters-config.json
func (a *App) findClusterConfig(clusterName string) (*models.ClusterConfig, error) {
	return findClusterInConfig(clusterName)
}

// setupAzureContext configura o contexto Azure (login + subscription)
func (a *App) setupAzureContext(subscription string) error {
	// 1. Verificar se estamos logados no Azure
	if err := azure.EnsureAzureLogin(); err != nil {
		return fmt.Errorf("failed to ensure Azure login: %w", err)
	}

	// 2. Configurar a subscription
	if subscription != "" {
		a.debugLog("üîÑ Configurando Azure subscription: %s\n", subscription)
		cmd := exec.Command("az", "account", "set", "--subscription", subscription)
		if err := cmd.Run(); err != nil {
			return fmt.Errorf("failed to set Azure subscription %s: %w", subscription, err)
		}
		a.debugLog("‚úÖ Azure subscription configurada com sucesso\n")
	}

	return nil
}

// enrichSessionHPAs enriquece HPAs da sess√£o que n√£o possuem dados de deployment
func (a *App) enrichSessionHPAs() tea.Cmd {
	return func() tea.Msg {
		if a.model.SelectedCluster == nil {
			return nil // Nada a fazer se n√£o h√° cluster selecionado
		}

		clusterName := a.model.SelectedCluster.Name

		// Obter o client do Kubernetes para este cluster
		clientset, err := a.kubeManager.GetClient(clusterName)
		if err != nil {
			a.debugLog("‚ö†Ô∏è N√£o foi poss√≠vel obter client para enriquecer HPAs: %v\n", err)
			return nil // Falha silenciosa - HPAs ainda funcionar√£o sem dados de deployment
		}

		client := kubernetes.NewClient(clientset, clusterName)
		ctx := context.Background()

		// Enriquecer cada HPA que precisa de dados
		enrichedCount := 0
		for i := range a.model.SelectedHPAs {
			hpa := &a.model.SelectedHPAs[i]
			if hpa.NeedsEnrichment {
				err := client.EnrichHPAWithDeploymentResources(ctx, hpa)
				if err != nil {
					a.debugLog("‚ö†Ô∏è Falha ao enriquecer HPA %s/%s: %v\n", hpa.Namespace, hpa.Name, err)
				} else {
					hpa.NeedsEnrichment = false
					enrichedCount++
					a.debugLog("‚úÖ HPA %s/%s enriquecido com dados de deployment\n", hpa.Namespace, hpa.Name)
				}
			}
		}

		// Tamb√©m enriquecer a lista principal de HPAs
		for i := range a.model.HPAs {
			hpa := &a.model.HPAs[i]
			if hpa.NeedsEnrichment {
				err := client.EnrichHPAWithDeploymentResources(ctx, hpa)
				if err != nil {
					a.debugLog("‚ö†Ô∏è Falha ao enriquecer HPA %s/%s: %v\n", hpa.Namespace, hpa.Name, err)
				} else {
					hpa.NeedsEnrichment = false
				}
			}
		}

		if enrichedCount > 0 {
			a.debugLog("üìä %d HPAs enriquecidos com dados de deployment do cluster\n", enrichedCount)
		}

		return sessionHPAsEnrichedMsg{enrichedCount: enrichedCount}
	}
}

// discoverClusterResources comando para descobrir recursos do cluster
func (a *App) discoverClusterResources(prometheusOnly bool) tea.Cmd {
	return func() tea.Msg {
		clusterName := a.model.SelectedCluster.Name
		
		// Obter o client do Kubernetes para este cluster
		clientset, err := a.kubeManager.GetClient(clusterName)
		if err != nil {
			return clusterResourcesDiscoveredMsg{
				err: fmt.Errorf("failed to get client for cluster %s: %w", clusterName, err),
			}
		}
		
		// Criar client wrapper
		client := kubernetes.NewClient(clientset, clusterName)
		
		// Descobrir recursos
		resources, err := client.DiscoverClusterResources(a.model.ShowSystemResources, prometheusOnly)
		if err != nil {
			return clusterResourcesDiscoveredMsg{
				err: fmt.Errorf("failed to discover cluster resources: %w", err),
			}
		}
		
		return clusterResourcesDiscoveredMsg{
			resources: resources,
			err:       nil,
		}
	}
}

// clusterResourcesDiscoveredMsg mensagem quando recursos s√£o descobertos
type clusterResourcesDiscoveredMsg struct {
	resources []models.ClusterResource
	err       error
}

// ============================================================================
// TEXT EDITING HELPER FUNCTIONS
// ============================================================================

// handleTextEditingKeys processa teclas para edi√ß√£o de texto com navega√ß√£o de cursor (REFATORADO)
func (a *App) handleTextEditingKeys(msg tea.KeyMsg, currentValue string, onSave func(string), onCancel func()) (string, int, bool) {
	// Usar a nova l√≥gica centralizada
	newValue, newCursor, continueEditing := a.handleUnifiedTextInput(msg, currentValue, onSave, onCancel)

	// Atualizar posi√ß√£o do cursor no modelo
	a.model.CursorPosition = newCursor

	return newValue, newCursor, continueEditing
}

// M√©todos centralizados para edi√ß√£o de texto

// handleUnifiedTextInput processa entrada de texto de forma centralizada
func (a *App) handleUnifiedTextInput(msg tea.KeyMsg, currentValue string, onSave func(string), onCancel func()) (string, int, bool) {
	textInput := NewTextInput(currentValue)
	textInput.SetCursorPosition(a.model.CursorPosition)

	newValue, newCursor, continueEditing, _ := textInput.HandleKeyPress(msg, onSave, onCancel)

	return newValue, newCursor, continueEditing
}

// renderTextWithCursor renderiza texto com cursor visual centralizado
func (a *App) renderTextWithCursor(text string, cursorPos int) string {
	textInput := NewTextInput(text)
	textInput.SetCursorPosition(cursorPos)
	return textInput.RenderWithCursor()
}

// insertCursorInText insere o cursor visual na posi√ß√£o correta do texto (m√©todo legado)
func (a *App) insertCursorInText(text string, cursorPos int) string {
	return a.renderTextWithCursor(text, cursorPos)
}

// validateCursorPosition garante que a posi√ß√£o do cursor est√° dentro dos limites v√°lidos
func (a *App) validateCursorPosition(text string) {
	maxPos := len([]rune(text))
	if a.model.CursorPosition < 0 {
		a.model.CursorPosition = 0
	}
	if a.model.CursorPosition > maxPos {
		a.model.CursorPosition = maxPos
	}
}

// resetHPAApplicationCounters zera os contadores de aplica√ß√£o de todos os HPAs (nova sess√£o)
func (a *App) resetHPAApplicationCounters() {
	// Zerar contadores nos HPAs selecionados
	for i := range a.model.SelectedHPAs {
		a.model.SelectedHPAs[i].AppliedCount = 0
		a.model.SelectedHPAs[i].LastAppliedAt = nil
	}

	// Zerar contadores em todos os HPAs carregados
	for i := range a.model.HPAs {
		a.model.HPAs[i].AppliedCount = 0
		a.model.HPAs[i].LastAppliedAt = nil
	}
}

// loadCronJobs carrega os CronJobs do cluster selecionado
func (a *App) loadCronJobs() tea.Cmd {
	return func() tea.Msg {
		if a.model.SelectedCluster == nil {
			return cronJobsLoadedMsg{err: fmt.Errorf("no cluster selected")}
		}

		clusterName := a.model.SelectedCluster.Name
		a.debugLog("üîÑ Carregando CronJobs do cluster: %s", clusterName)

		// Usar cliente Kubernetes para listar CronJobs
		client, err := a.kubeManager.GetClient(clusterName)
		if err != nil {
			return cronJobsLoadedMsg{err: fmt.Errorf("failed to get kubernetes client: %w", err)}
		}

		cronJobs, err := a.loadCronJobsFromKubernetes(client, clusterName)
		if err != nil {
			return cronJobsLoadedMsg{err: fmt.Errorf("failed to load cronjobs: %w", err)}
		}

		a.debugLog("‚úÖ CronJobs carregados: %d encontrados", len(cronJobs))
		return cronJobsLoadedMsg{cronJobs: cronJobs}
	}
}

// loadCronJobsFromKubernetes carrega CronJobs usando a API do Kubernetes
func (a *App) loadCronJobsFromKubernetes(client k8sClientSet.Interface, clusterName string) ([]models.CronJob, error) {
	ctx := context.Background()
	var allCronJobs []models.CronJob

	// Listar todos os namespaces (excluindo system se filtrado)
	namespaces, err := client.CoreV1().Namespaces().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %w", err)
	}

	for _, ns := range namespaces.Items {
		namespaceName := ns.Name

		// Filtrar namespaces de sistema se necess√°rio
		if !a.model.ShowSystemNamespaces && a.isSystemNamespace(namespaceName) {
			continue
		}

		// Listar CronJobs no namespace
		cronJobList, err := client.BatchV1().CronJobs(namespaceName).List(ctx, metav1.ListOptions{})
		if err != nil {
			a.debugLog("‚ö†Ô∏è Erro ao listar CronJobs no namespace %s: %v", namespaceName, err)
			continue
		}

		for _, cronJob := range cronJobList.Items {
			// Converter para nosso modelo
			modelCronJob := models.CronJob{
				Name:      cronJob.Name,
				Namespace: namespaceName,
				Cluster:   clusterName,
				Schedule:  cronJob.Spec.Schedule,
				Suspend:   cronJob.Spec.Suspend,
				OriginalSuspend: cronJob.Spec.Suspend,
			}

			// Adicionar descri√ß√£o leg√≠vel do schedule
			modelCronJob.ScheduleDesc = a.parseCronSchedule(cronJob.Spec.Schedule)

			// Extrair informa√ß√µes de status
			if cronJob.Status.LastScheduleTime != nil {
				modelCronJob.LastScheduleTime = &cronJob.Status.LastScheduleTime.Time
			}

			modelCronJob.ActiveJobs = len(cronJob.Status.Active)
			if cronJob.Status.LastSuccessfulTime != nil && cronJob.Status.LastScheduleTime != nil {
				if cronJob.Status.LastSuccessfulTime.Time.After(cronJob.Status.LastScheduleTime.Time) {
					modelCronJob.LastRunStatus = "Success"
				}
			}

			// Obter informa√ß√µes do job template
			if cronJob.Spec.JobTemplate.Spec.Template.Spec.Containers != nil && len(cronJob.Spec.JobTemplate.Spec.Template.Spec.Containers) > 0 {
				container := cronJob.Spec.JobTemplate.Spec.Template.Spec.Containers[0]

				// Extrair descri√ß√£o funcional dos comandos/argumentos
				functionalDesc := a.extractJobFunction(container.Command, container.Args)

				// Combinar informa√ß√µes de container/image com descri√ß√£o funcional
				containerInfo := fmt.Sprintf("%s (%s)", container.Name, container.Image)
				if functionalDesc != "" {
					modelCronJob.JobTemplate = fmt.Sprintf("%s\n     %s", containerInfo, functionalDesc)
				} else {
					modelCronJob.JobTemplate = containerInfo
				}
			}

			allCronJobs = append(allCronJobs, modelCronJob)
		}
	}

	return allCronJobs, nil
}

// parseCronSchedule converte um schedule cron em descri√ß√£o leg√≠vel
func (a *App) parseCronSchedule(schedule string) string {
	parts := strings.Fields(schedule)
	if len(parts) != 5 {
		return fmt.Sprintf("Schedule: %s", schedule)
	}

	minute := parts[0]
	hour := parts[1]

	// Casos comuns
	if schedule == "0 0 * * *" {
		return "Schedule: 0 0 * * * - executa todo dia √† meia-noite"
	}
	if schedule == "0 2 * * *" {
		return "Schedule: 0 2 * * * - executa todo dia √†s 2:00 AM"
	}
	if schedule == "0 0 * * 1" {
		return "Schedule: 0 0 * * 1 - executa toda segunda-feira √† meia-noite"
	}
	if schedule == "*/5 * * * *" {
		return "Schedule: */5 * * * * - executa a cada 5 minutos"
	}
	if schedule == "0 */2 * * *" {
		return "Schedule: 0 */2 * * * - executa a cada 2 horas"
	}

	// Construir descri√ß√£o baseada nos componentes
	desc := fmt.Sprintf("Schedule: %s", schedule)

	if hour != "*" && minute != "*" {
		if hour[0] != '*' && minute == "0" {
			desc += fmt.Sprintf(" - executa todo dia √†s %s:00", hour)
		} else if hour[0] != '*' && minute != "*" {
			desc += fmt.Sprintf(" - executa todo dia √†s %s:%s", hour, minute)
		}
	}

	return desc
}

// extractJobFunction extrai descri√ß√£o funcional dos comandos e argumentos do container
func (a *App) extractJobFunction(command []string, args []string) string {
	// Combinar comando e argumentos
	var allArgs []string
	allArgs = append(allArgs, command...)
	allArgs = append(allArgs, args...)

	// Juntar todos os argumentos em uma string para an√°lise
	fullCommand := strings.Join(allArgs, " ")

	// Debug: log do comando para an√°lise
	a.debugLog("üîç Analisando comando do CronJob:")
	a.debugLog("   üìã Command: %v", command)
	a.debugLog("   üìã Args: %v", args)
	a.debugLog("   üìã Full: [%s]", fullCommand)

	// Se for um script bash (-c), analisar o conte√∫do do script
	if strings.Contains(fullCommand, "/bin/bash") && strings.Contains(fullCommand, "-c") {
		a.debugLog("   üîß Detectado script bash, analisando conte√∫do")
		return a.extractFromBashScript(fullCommand)
	}

	// Padr√µes comuns para extrair fun√ß√£o
	patterns := []struct {
		pattern string
		description string
	}{
		// kubectl rollout restart deployment X -n Y ou --namespace Y
		{`kubectl.*rollout.*restart.*deployment\s+(\S+).*(?:-n|--namespace)\s+(\S+)`, "Faz rollout restart do deployment %s no namespace %s"},
		// kubectl rollout restart deployment X (sem namespace expl√≠cito)
		{`kubectl.*rollout.*restart.*deployment\s+([a-zA-Z0-9\-_]+)(?:\s|$)`, "Faz rollout restart do deployment %s"},
		// kubectl scale com namespace
		{`kubectl.*scale.*deployment\s+(\S+).*(?:-n|--namespace)\s+(\S+).*replicas[=\s]+(\d+)`, "Escala deployment %s no namespace %s para %s r√©plicas"},
		// kubectl scale sem namespace
		{`kubectl.*scale.*deployment\s+(\S+).*replicas[=\s]+(\d+)`, "Escala deployment %s para %s r√©plicas"},
		{`kubectl.*delete.*pod.*selector\s+(\S+)`, "Remove pods com selector %s"},
		{`kubectl.*apply.*-f\s+(\S+)`, "Aplica configura√ß√£o do arquivo %s"},
		{`curl.*-X\s+POST.*(\S+)`, "Faz requisi√ß√£o POST para %s"},
		{`curl.*-X\s+GET.*(\S+)`, "Faz requisi√ß√£o GET para %s"},
		{`backup.*database.*(\S+)`, "Faz backup do banco de dados %s"},
		{`cleanup.*logs.*(\S+)`, "Limpa logs do servi√ßo %s"},
	}

	// Tentar encontrar padr√µes conhecidos
	for i, p := range patterns {
		re := regexp.MustCompile(`(?i)` + p.pattern)
		matches := re.FindStringSubmatch(fullCommand)
		if len(matches) > 1 {
			a.debugLog("   ‚úÖ Padr√£o %d encontrado: %s", i+1, p.pattern)
			a.debugLog("   üìù Grupos capturados: %v", matches[1:])
			// Aplicar template com os grupos capturados
			switch len(matches) {
			case 2:
				result := fmt.Sprintf(p.description, matches[1])
				a.debugLog("   üéØ Resultado: %s", result)
				return result
			case 3:
				result := fmt.Sprintf(p.description, matches[1], matches[2])
				a.debugLog("   üéØ Resultado: %s", result)
				return result
			case 4:
				result := fmt.Sprintf(p.description, matches[1], matches[2], matches[3])
				a.debugLog("   üéØ Resultado: %s", result)
				return result
			}
		}
	}

	// Se n√£o encontrou padr√£o espec√≠fico, tentar extrair a√ß√£o geral
	a.debugLog("   ‚ö†Ô∏è Nenhum padr√£o espec√≠fico encontrado, tentando padr√µes gerais")

	if strings.Contains(fullCommand, "kubectl") {
		if strings.Contains(fullCommand, "rollout") && strings.Contains(fullCommand, "restart") {
			result := "Executa rollout restart de recursos Kubernetes"
			a.debugLog("   üéØ Resultado geral: %s", result)
			return result
		}
		if strings.Contains(fullCommand, "scale") {
			result := "Escala recursos Kubernetes"
			a.debugLog("   üéØ Resultado geral: %s", result)
			return result
		}
		if strings.Contains(fullCommand, "delete") {
			result := "Remove recursos Kubernetes"
			a.debugLog("   üéØ Resultado geral: %s", result)
			return result
		}
		if strings.Contains(fullCommand, "apply") {
			result := "Aplica configura√ß√µes Kubernetes"
			a.debugLog("   üéØ Resultado geral: %s", result)
			return result
		}
		result := "Executa comando kubectl"
		a.debugLog("   üéØ Resultado geral: %s", result)
		return result
	}

	if strings.Contains(fullCommand, "curl") {
		result := "Executa requisi√ß√£o HTTP"
		a.debugLog("   üéØ Resultado geral: %s", result)
		return result
	}

	if strings.Contains(fullCommand, "backup") {
		result := "Executa backup de dados"
		a.debugLog("   üéØ Resultado geral: %s", result)
		return result
	}

	if strings.Contains(fullCommand, "cleanup") || strings.Contains(fullCommand, "clean") {
		result := "Executa limpeza de recursos"
		a.debugLog("   üéØ Resultado geral: %s", result)
		return result
	}

	// Se n√£o conseguiu identificar, retorna string vazia
	a.debugLog("   ‚ùå Nenhum padr√£o reconhecido, retornando vazio")
	return ""
}

// extractFromBashScript extrai informa√ß√µes de scripts bash executados com -c
func (a *App) extractFromBashScript(fullCommand string) string {
	a.debugLog("   üîç Analisando script bash")

	// Extrair vari√°veis definidas no in√≠cio do script
	var namespace, deployment string

	// Padr√µes para extrair vari√°veis
	namespacePattern := regexp.MustCompile(`(?i)NAMESPACE\s*=\s*([^\s\n]+)`)
	deploymentPattern := regexp.MustCompile(`(?i)DEPLOYMENT\s*=\s*([^\s\n]+)`)

	if matches := namespacePattern.FindStringSubmatch(fullCommand); len(matches) > 1 {
		namespace = matches[1]
		a.debugLog("   üìù Namespace encontrado: %s", namespace)
	}

	if matches := deploymentPattern.FindStringSubmatch(fullCommand); len(matches) > 1 {
		deployment = matches[1]
		a.debugLog("   üìù Deployment encontrado: %s", deployment)
	}

	// Analisar a√ß√µes no script
	if strings.Contains(fullCommand, "kubectl rollout restart") {
		if deployment != "" && namespace != "" {
			result := fmt.Sprintf("Faz rollout restart do deployment %s no namespace %s", deployment, namespace)
			a.debugLog("   üéØ Resultado do script: %s", result)
			return result
		} else if deployment != "" {
			result := fmt.Sprintf("Faz rollout restart do deployment %s", deployment)
			a.debugLog("   üéØ Resultado do script: %s", result)
			return result
		} else {
			result := "Executa rollout restart de deployment"
			a.debugLog("   üéØ Resultado do script: %s", result)
			return result
		}
	}

	if strings.Contains(fullCommand, "kubectl scale") {
		if deployment != "" && namespace != "" {
			result := fmt.Sprintf("Escala deployment %s no namespace %s", deployment, namespace)
			a.debugLog("   üéØ Resultado do script: %s", result)
			return result
		} else {
			result := "Executa scaling de deployment"
			a.debugLog("   üéØ Resultado do script: %s", result)
			return result
		}
	}

	// Outros padr√µes gerais
	if strings.Contains(fullCommand, "kubectl") {
		result := "Executa opera√ß√µes Kubernetes via script"
		a.debugLog("   üéØ Resultado do script: %s", result)
		return result
	}

	a.debugLog("   ‚ùå Script n√£o reconhecido")
	return ""
}

// isSystemNamespace verifica se um namespace √© de sistema
func (a *App) isSystemNamespace(namespace string) bool {
	systemNamespaces := []string{
		"kube-system", "kube-public", "kube-node-lease",
		"istio-system", "istio-injection",
		"cert-manager", "ingress-nginx",
		"monitoring", "prometheus", "grafana",
		"flux-system", "flux", "fluxcd",
		"argocd", "argo", "argo-workflows",
		"tekton-pipelines", "tekton",
		"knative-serving", "knative-eventing",
		"gatekeeper-system", "open-policy-agent",
		"falco", "sysdig",
		"linkerd", "linkerd-viz", "linkerd-jaeger",
		"cilium", "cilium-system",
		"calico-system", "tigera-operator",
		"metallb-system",
		"rook-ceph", "ceph",
		"vault", "vault-system",
		"consul", "consul-system",
		"jaeger", "jaeger-system",
		"elastic-system", "elasticsearch",
		"logging", "fluent", "fluentd", "fluent-bit",
		"datadog", "newrelic",
		"kustomize", "helm",
		"crossplane-system",
		"external-dns",
		"cluster-autoscaler",
		"metrics-server",
		"kubernetes-dashboard",
		"keda", "keda-system",
		"sealed-secrets",
		"velero",
		"backup", "restore",
	}

	for _, sysNs := range systemNamespaces {
		if namespace == sysNs {
			return true
		}
	}
	return false
}

// toggleNodePoolSequenceMarking - Marca/desmarca node pool para execu√ß√£o sequencial (stress test)
func (a *App) toggleNodePoolSequenceMarking(selectedIndex int) {
	if selectedIndex >= len(a.model.SelectedNodePools) {
		return
	}

	currentPool := &a.model.SelectedNodePools[selectedIndex]

	// Se j√° est√° marcado, desmarcar (toggle)
	if currentPool.SequenceOrder > 0 {
		a.debugLog("üîÑ Desmarcando node pool %s (ordem %d)", currentPool.Name, currentPool.SequenceOrder)
		currentPool.SequenceOrder = 0
		currentPool.SequenceStatus = ""
		return
	}

	// Contar quantos j√° est√£o marcados
	markedCount := 0
	for _, pool := range a.model.SelectedNodePools {
		if pool.SequenceOrder > 0 {
			markedCount++
		}
	}

	// Limite de 2 node pools
	if markedCount >= 2 {
		a.debugLog("‚ö†Ô∏è  Limite de 2 node pools j√° atingido para execu√ß√£o sequencial")
		// Poderia adicionar uma mensagem de status aqui
		return
	}

	// Marcar com a pr√≥xima ordem dispon√≠vel
	nextOrder := markedCount + 1
	currentPool.SequenceOrder = nextOrder
	currentPool.SequenceStatus = "pending"

	a.debugLog("‚úÖ Node pool %s marcado para execu√ß√£o sequencial (ordem %d)", currentPool.Name, nextOrder)
}

// checkAndStartSequentialExecution - Verifica se deve iniciar execu√ß√£o do segundo node pool
func (a *App) checkAndStartSequentialExecution() tea.Cmd {
	// Encontrar node pools marcados
	var firstPool *models.NodePool
	var secondPool *models.NodePool

	for i := range a.model.SelectedNodePools {
		pool := &a.model.SelectedNodePools[i]
		if pool.SequenceOrder == 1 {
			firstPool = pool
		} else if pool.SequenceOrder == 2 {
			secondPool = pool
		}
	}

	// Se n√£o h√° sequ√™ncia marcada, nada fazer
	if firstPool == nil || secondPool == nil {
		return nil
	}

	// Se o primeiro node pool foi completado e o segundo ainda est√° pendente
	if firstPool.SequenceStatus == "completed" && secondPool.SequenceStatus == "pending" {
		a.debugLog("‚úÖ Primeiro node pool %s completado, iniciando segundo node pool %s", firstPool.Name, secondPool.Name)

		// Marcar segundo como executando
		secondPool.SequenceStatus = "executing"

		// Executar o segundo node pool automaticamente
		return a.applyNodePoolChanges([]models.NodePool{*secondPool})
	}

	return nil
}

// checkSequenceStatusAndContinue - Verifica status e continua execu√ß√£o sequencial
func (a *App) checkSequenceStatusAndContinue() tea.Cmd {
	// Encontrar node pool atualmente executando
	var executingPool *models.NodePool
	var nextPool *models.NodePool

	for i := range a.model.SelectedNodePools {
		pool := &a.model.SelectedNodePools[i]
		if pool.SequenceStatus == "executing" {
			executingPool = pool
		} else if pool.SequenceStatus == "pending" && executingPool != nil && pool.SequenceOrder == executingPool.SequenceOrder+1 {
			nextPool = pool
		}
	}

	if executingPool == nil {
		return nil // Nenhuma execu√ß√£o em andamento
	}

	// Simular verifica√ß√£o de status (aqui voc√™ implementaria a verifica√ß√£o real via Azure CLI)
	// Por enquanto, vamos marcar como completo ap√≥s um delay
	a.debugLog("‚úÖ Node pool %s completado, iniciando pr√≥ximo", executingPool.Name)
	executingPool.SequenceStatus = "completed"

	// Se h√° pr√≥ximo node pool, execut√°-lo
	if nextPool != nil {
		nextPool.SequenceStatus = "executing"
		a.debugLog("‚ö° Executando pr√≥ximo node pool %s (ordem %d)", nextPool.Name, nextPool.SequenceOrder)
		return a.applyNodePoolChanges([]models.NodePool{*nextPool})
	}

	a.debugLog("üéâ Execu√ß√£o sequencial conclu√≠da!")
	return nil
}

